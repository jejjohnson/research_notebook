

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Sparse Gaussian Processes &#8212; Research Notebook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/notes/gps/sgps';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Algorithms" href="algorithms.html" />
    <link rel="prev" title="Conjugate Gradients" href="cg.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/book_v2.jpeg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../../_static/book_v2.jpeg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../resources/python/overview.html">Python</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../resources/python/ides.html">Integraded Development Environment (IDE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../resources/python/stack.html">Standard Python Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../resources/python/earthsci_stack.html">Earth Science Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../resources/python/dl_stack.html">Deep Learning Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../resources/python/scale_stack.html">Scaling Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../resources/python/good_code.html">Good Code</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/jax_journey/overview.html">My JAX Journey</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/jax_journey/ecosystem.html">Ecosystem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/jax_journey/vmap.html">vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/jax_journey/jit.html">Jit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/jax_journey/classes.html">Classes</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../tutorials/jax_journey/algorithms/overview.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/jax_journey/algorithms/bisection.html">Bisection search</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/jax_journey/algorithms/kernel_derivatives.html">Kernel Derivatives</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/jax_journey/gfs_with_jax.html">Gaussianization Flows</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../tutorials/twelve_steps_ns/overview.html">12 Steps to Navier-Stokes</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/twelve_steps_ns/1.1_linear_advection.html">Linear Convection</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/remote/overview.html">Remote Computing</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/remote/ssh.html">SSH Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/remote/conda.html">Conda 4 Remote Servers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/remote/jlab.html">Jupyter Lab 4 Remote Servers</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../gmt/overview.html">GMT of Learning</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../gmt/hierarchical_rep.html">Hierarchical Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gmt/functa.html">Functa</a></li>








<li class="toctree-l2"><a class="reference internal" href="../gmt/discretize_space.html">Spatial Discretization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gmt/discretize_time.html">Temporal Discretization</a></li>


<li class="toctree-l2"><a class="reference internal" href="../gmt/learning.html">Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/uncertainty.html">Modeling Uncertainty</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../bayesian/overview.html">Bayesian</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../bayesian/intro.html">Language of Uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bayesian/models.html">Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bayesian/inference.html">Inference Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bayesian/inference/variational_inference.html">Variational Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bayesian/inference/cond_vi.html">Conditional Variational Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bayesian/confidence_intervals.html">Confidence Intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bayesian/regression.html">Regression</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../concepts/overview.html">Sleeper Concepts</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../concepts/gaussian.html">Gaussian Distributions</a></li>

<li class="toctree-l2"><a class="reference internal" href="../concepts/change_of_variables.html">Change of Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../concepts/identity_trick.html">Identity Trick</a></li>
<li class="toctree-l2"><a class="reference internal" href="../concepts/inverse_function.html">Inverse Function Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../concepts/jensens.html">Jensens Inequality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../concepts/lin_alg.html">Linear Algebra Tricks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../kernels/overview.html">Kernel Methods</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../kernels/kernel_derivatives.html">Kernel Derivatives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/rv.html">RV Coefficient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/congruence_coeff.html">Congruence Coefficient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/hsic.html">HSIC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernels/mmd.html">Maximum Mean Discrepancy (MMD)</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Gaussian Processes</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="gps.html">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="literature.html">Literature Review</a></li>
<li class="toctree-l2"><a class="reference internal" href="cg.html">Conjugate Gradients</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Sparse Gaussian Processes</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="algorithms.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="gpr_code.html">GP from Scratch</a></li>
<li class="toctree-l3"><a class="reference internal" href="sgp_code.html">Sparse GP From Scratch</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../egps/overview.html">Input Uncertainty in GPs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../info_theory/similarity.html">Similarity</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../info_theory/overview.html">Information Theory</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../info_theory/measures.html">Measures</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../info_theory/information.html">Information Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../info_theory/entropy.html">Entropy &amp; Relative Entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../info_theory/mutual_info.html">Mutual Information and Total Correlation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../info_theory/estimators.html">Information Theory Measures</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../info_theory/classic.html">Classic Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../info_theory/histogram.html">Entropy Estimator - Histogram</a></li>
<li class="toctree-l3"><a class="reference internal" href="../info_theory/experiments/rbig_sample_consistency.html">Experiment - RBIG Sample Consistency</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../normalizing_flows/overview.html">Normalizing Flows</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../normalizing_flows/linear.html">Linear Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normalizing_flows/coupling_layers.html">Coupling Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normalizing_flows/conditional.html">Conditional Normalizing Flows</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normalizing_flows/multiscale.html">Multiscale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normalizing_flows/inverse.html">Minimization Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normalizing_flows/losses.html">Losses</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../normalizing_flows/lecture_1_ig.html">Lecture I - Iterative Gaussianization</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../normalizing_flows/1.0_univariate_gauss.html">1.1 - Univariate Gaussianization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../normalizing_flows/1.1_marginal_gauss.html">1.2 - Marginal Gaussianization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../normalizing_flows/1.2_gaussianization.html">1.2 - Iterative Gaussianization</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../normalizing_flows/lecture_2_gf.html">Lecture II - Gaussianization Flows</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../normalizing_flows/lecture_3_gfs_pt1_mg.html">Parameterized Marginal Gaussianization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../normalizing_flows/lecture_3_gfs_pt2_rot.html">Parameterized Rotations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../normalizing_flows/lecture_3_gfs_pt3_plane.html">Example - 2D Plane</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../nerfs/overview.html">Neural Fields</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../nerfs/formulation.html">Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nerfs/literature_review.html">Literature Review</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../nerfs/pinns.html">Physics-Informed Loss</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../data_assimilation/overview.html">Data Assimilation</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../data_assimilation/dynamical_sys.html">Dynamical Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_assimilation/oi.html">Optimal Interpolation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_assimilation/interp.html">Interpolation Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_assimilation/emu.html">Emulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_assimilation/inv_problems.html">Inverse Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_assimilation/projects.html">Projects</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../data_assimilation/algorithms.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../data_assimilation/markov_models.html">Markov Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_assimilation/gauss_markov.html">Gauss-Markov Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_assimilation/kf.html">Kalman Filter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_assimilation/nkf.html">Normalizing Kalman Filter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_assimilation/enskf.html">Ensemble Kalman Filter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_assimilation/dmm.html">Deep Markov Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_assimilation/4dvarnet.html">4DVarNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data_assimilation/markov_gp.html">Markovian Gaussian Processes</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../data_assimilation/nbs/notebooks.html">Notebooks</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../misc/overview.html">Miscellaneous Notes</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../misc/generative_models.html">Generative Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../misc/diffusion_models.html">Diffusion Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../misc/fixed_point.html">Fixed-Point Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../misc/bilevel_opt.html">Bi-Level Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../misc/diff_operators.html">Differential Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../misc/qg.html">QG Formulations</a></li>


<li class="toctree-l2"><a class="reference internal" href="../misc/elliptical_pde_solver.html">Elliptical PDE Solvers</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Cheat Sheets</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../cheatsheets/bash.html">Bash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cheatsheets/cli.html">Command Line</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cheatsheets/python.html">Python</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/jejjohnson/research_notebook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/jejjohnson/research_notebook/issues/new?title=Issue%20on%20page%20%2Fcontent/notes/gps/sgps.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/content/notes/gps/sgps.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Sparse Gaussian Processes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subset-of-data">Subset of Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-approximations">Kernel Approximations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inducing-points">Inducing Points</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-gps-inducing-points-summary">Sparse GPs - Inducing Points Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function">Objective Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#observations-about-the-sparse-gps">Observations about the Sparse GPs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-compression">Variational Compression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distribution-augmented-space-mathcal-p-f-u">Joint Distribution - Augmented Space <span class="math notranslate nohighlight">\(\mathcal{P}(f,u)\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-distribution-mathcal-p-y-u">Conditional Distribution - <span class="math notranslate nohighlight">\(\mathcal{P}(y|u)\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-bound-on-mathcal-p-y-u">Variational Bound on <span class="math notranslate nohighlight">\(\mathcal P (y|u)\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#titsias-innovation-et-q-f-mathcal-p-f-u">Titsias Innovation: et <span class="math notranslate nohighlight">\(q(f) = \mathcal{P}(f|u)\)</span>.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elbos">ELBOs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lower-bound">Lower Bound</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-bound-on-mathcal-p-y">Variational Bound on <span class="math notranslate nohighlight">\(\mathcal P (y)\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-variational-inference">Stochastic Variational Inference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supplementary-material">Supplementary Material</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-formulas">Important Formulas</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nystrom-approximation">Nystrom Approximation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sherman-morrison-woodbury-formula">Sherman-Morrison-Woodbury Formula</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sylvester-determinant-theorem">Sylvester Determinant Theorem</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resources">Resources</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-walk-throughs">Code Walk-Throughs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#math-walkthroughs">Math Walkthroughs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#papers">Papers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#thesis-explain">Thesis Explain</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#presentations">Presentations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notes">Notes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#blogs">Blogs</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="sparse-gaussian-processes">
<h1>Sparse Gaussian Processes<a class="headerlink" href="#sparse-gaussian-processes" title="Permalink to this heading">#</a></h1>
<hr class="docutils" />
<p>Sparse GPs refer to a family of methods that seek to take a subset of points in order to approximate the full dataset. Typically we can break them down into 5 categories:</p>
<ul class="simple">
<li><p>Subset of Data (Transformation, Random Sampling)</p></li>
<li><p>Data Approximation Methods (Nystrom, Random Fourer Features, Random Kitchen Sinks, Sparse-Spectrum, FastFood, A la Carte)</p></li>
<li><p>Inducing Points (SoR, FITC, DTC, KISS-GP)</p></li>
<li><p>Linear Algebra (Toeplitz, Kronecker, )</p></li>
<li><p>Approximate Inference (Variational Methods)</p></li>
</ul>
<p>Each of these methods ultimately augment the model so that the largest computation goes from <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}(MN^2)\)</span> where <span class="math notranslate nohighlight">\(M&lt;&lt;N\)</span>.</p>
<hr class="docutils" />
<hr class="docutils" />
<section id="subset-of-data">
<h2>Subset of Data<a class="headerlink" href="#subset-of-data" title="Permalink to this heading">#</a></h2>
<p>This is the simplest way to approximate the data. The absolute simplest way is to take a random subsample of your data. However this is often not a good idea because the more data you have the more information you’re more likely to have. It’s an age old rule that says if you want better predictions, it’s often better just to have more data.</p>
<p>A more sophisticated way to get a subsample of your data is to do some sort of pairwise similarity comparison scheme - i.e. Kernel methods. There are a family of methods like the Nystrom approximation or Random Fourier Features (RFF) which takes a subset of the points through pairwise comparisons. These are kernel matrix approximations so we can transform our data from our data space <span class="math notranslate nohighlight">\(\mathcal{X} \in \mathbb{R}^{N \times D}\)</span> to subset data space <span class="math notranslate nohighlight">\(\mathcal{Z} \in \mathbb{R}^{M \times d}\)</span> which is found through an eigen decomposition scheme.</p>
<p>In GPs we calculate a kernel matrix <span class="math notranslate nohighlight">\(\mathbf K \in \mathbb{R}^{N \times N}\)</span>. If <span class="math notranslate nohighlight">\(N\)</span> is large enough, then throughout the marginal likelihood, we need to calculate <span class="math notranslate nohighlight">\(\mathbf K^{-1}\)</span> and <span class="math notranslate nohighlight">\(|\mathbf K|\)</span> which has <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> operations and <span class="math notranslate nohighlight">\(\mathcal{O}(N^2)\)</span> memory costs. So we make an approximate matrix <span class="math notranslate nohighlight">\(\mathbf {\tilde{K}}\)</span> given by the following formula:</p>
<div class="math notranslate nohighlight">
\[\tilde{K}=K_{z}K_{zz}^{-1}K_z^{\top}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(K_{zz}=K(z,z)\in \mathbb{R}^{M\times M}\)</span> - the kernel matrix for the subspace <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(K_z=K(x,z)\in \mathbb{R}^{N\times M}\)</span> - the transformation matrix from the data space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to the subspace <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(K \approx \tilde{K} \in \mathbb{R}^{N \times N}\)</span> - the approximate kernel matrix of the data space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span></p></li>
</ul>
<p>Below is an example of where this would be applicable where we just implement this method where we just transform the day.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.kernel_approximation</span> <span class="kn">import</span> <span class="n">Nystroem</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span> <span class="k">as</span> <span class="n">GPR</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_processes.kernels</span> <span class="kn">import</span> <span class="n">RBF</span>

<span class="c1"># Initialize Nystrom transform</span>
<span class="n">nystrom_map</span> <span class="o">=</span> <span class="n">Nystrom</span><span class="p">(</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_components</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Transform Data</span>
<span class="n">X_transformed</span> <span class="o">=</span> <span class="n">nystrom_map</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># initialize GPR</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPR</span><span class="p">()</span>

<span class="c1"># fit GP model</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="kernel-approximations">
<h2>Kernel Approximations<a class="headerlink" href="#kernel-approximations" title="Permalink to this heading">#</a></h2>
<p>Pivoting off of the method above, we
So now when we calculate the log likelihood term <span class="math notranslate nohighlight">\(\log \mathcal{P}(y|X,\theta)\)</span> we can have an approximation:</p>
<div class="math notranslate nohighlight">
\[\log \mathcal{N}(y | 0, K + \sigma^2I) \approx \log \mathcal{N}(y | 0, \tilde{K} + \sigma^2I)\]</div>
<p>Notice how we haven’t actually changing our formulation because we still have to calculate the inverse of <span class="math notranslate nohighlight">\(\tilde{K}\)</span> which is <span class="math notranslate nohighlight">\(\mathbb{R}^{N \times N}\)</span>. Using the <a class="reference external" href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury matrix identity</a> for the kernel approximation form (<a class="reference external" href="https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula">Sherman-Morrison Formula</a>):</p>
<div class="math notranslate nohighlight">
\[(\tilde{K} + \sigma^2 I)^{-1}=\sigma^{-2}I - \sigma^{-4}K_z(K_{zz}+\sigma^{-2}K_z^{\top}K_z)^{-1}K_z^{\top}\]</div>
<p>Now the matrix that we need to invert is <span class="math notranslate nohighlight">\((K_{zz}+\sigma^{-2}K_z^{\top}K_z)^{-1}\)</span> which is <span class="math notranslate nohighlight">\((M \times M)\)</span> which is considerably smaller if <span class="math notranslate nohighlight">\(M &lt;&lt; N\)</span>. So the overall computational complexity reduces to <span class="math notranslate nohighlight">\(\mathcal{O}(NM^2)\)</span>.</p>
</section>
<hr class="docutils" />
<section id="inducing-points">
<h2>Inducing Points<a class="headerlink" href="#inducing-points" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Deisenroth - GPs for Big Data - <a class="reference external" href="https://www.doc.ic.ac.uk/~mpd37/teaching/tutorials/2015-04-14-mlss.pdf">MLSS2015</a></p></li>
<li><p>Dai - Scalable GPs - <a class="reference external" href="http://zhenwendai.github.io/slides/gpss2018_slides.pdf">MLSS2018</a></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="sparse-gps-inducing-points-summary">
<h2>Sparse GPs - Inducing Points Summary<a class="headerlink" href="#sparse-gps-inducing-points-summary" title="Permalink to this heading">#</a></h2>
<p>One big issue with the above expression is that the inverse of the <span class="math notranslate nohighlight">\(\mathbf{K}_\mathcal{GP}^{-1}\)</span> is <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> which can be very expensive. One can use inducing points which act as a subset of points of size <span class="math notranslate nohighlight">\(M &lt;&lt; N\)</span>. This can be used to reduce the computation to a cost of <span class="math notranslate nohighlight">\(\mathcal{O}(NM^2)\)</span>. There are a number of different methods using this idea including earlier methods like subset of regressors, …, Fully Independent Training t</p>
<p>In this thesis, we focus on a particular implementation called <em>sparse variational free energy</em> (VFE) method <code class="docutils literal notranslate"><span class="pre">cite</span></code>. This performs an approximate inference scheme by introducing a variational parameter <span class="math notranslate nohighlight">\(q(f)\)</span> over the latent function. Then, we can optimize a lower bound on the likelihood (ELBO) to approximate the posterior.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\log p(\mathbf{y}) &amp;\geq \log p(\mathbf{y}) - \text{KL}\left[ q(f)||p(f|\mathbf{y}) \right] \\
&amp;\geq \log \mathcal{N}(\mathbf{y}; 0, \mathbf{Q}_ff + \sigma^2\mathbf{I}) - \frac{1}{2\sigma^2} \text{Tr}\left( \mathbf{K}_{ff} - \mathbf{Q}_{ff} \right) 
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Q}_{ff}=\mathbf{K}_{fu}\mathbf{K}^{-1}_{uu}\mathbf{K}_{uf}\)</span> is the Nystrom approximation of <span class="math notranslate nohighlight">\(\mathbf{K}_{ff}\)</span> and <span class="math notranslate nohighlight">\(u\)</span> is a small subset of <span class="math notranslate nohighlight">\(M&lt;&lt;N\)</span> inducing points at locations <span class="math notranslate nohighlight">\(\{ z_j \}^M_{j=1}\)</span> which makes <span class="math notranslate nohighlight">\([\mathbf{K}_{fu}]_{ij}=k(\mathbf{x}_i, \mathbf{z}_j)\)</span> and <span class="math notranslate nohighlight">\([\mathbf{K}_{uu}]_{ij}=k(\mathbf{z}_i, \mathbf{z}_j)\)</span>. The first term of the ELBO corresponds to a deterministic training conditional (DTC, <code class="docutils literal notranslate"><span class="pre">cite</span></code>) and the added regularization trace term prevents overfitting which has is a problem with the DTC. Since this variational approximation is a Gaussian distribution, <span class="math notranslate nohighlight">\(q(y_*)=\mathcal{N}(y_*; \mu_*, \sigma^2_*)\)</span>, there is a closed-form predictive mean and variance given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mu_\mathcal{SGP}(\mathbf{x}_*) &amp;= k_{u*}^\top \left( \mathbf{K}_{uf}\mathbf{K}_{fu} + \sigma^2 \mathbf{K}_{uu} \right)^{-1} \mathbf{K}_{uf}\mathbf{y} \\
\sigma^2_\mathcal{SGP}(\mathbf{x}_*) &amp;= \sigma^2 + k_{**} - k_{u*}^\top \mathbf{K}_{uu}^\top k_{u*} + k_{u*}^\top \left( \sigma^{-2}\mathbf{K}_{uf}\mathbf{K}_{fu}+ \mathbf{K}_{uu} \right)k_{u*}
\end{aligned}
\end{split}\]</div>
<hr class="docutils" />
<section id="objective-function">
<h3>Objective Function<a class="headerlink" href="#objective-function" title="Permalink to this heading">#</a></h3>
<p>So I think it is important to make note of the similarities between methods; specifically between FITC and VFE which are some staple methods one would use to scale GPs naively. Not only is it helpful for understanding the connection between all of the methods but it also helps with programming and seeing where each method differs algorithmically. Each sparse method is a method of using some set of inducing points or subset of data <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> from the data space <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. We typically have some approximate matrix <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> which approximates the kernel matrix <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Q}_{ff}=\mathbf{K}_{fu}\mathbf{K}_{uu}^{-1}\mathbf{K}_{uf}\]</div>
<p>Then we would use the Sherman-Morrison formula to reduce the computation cost of inverting the matrix <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>. Below is the negative marginal log likelihood cost function that is minimized where we can see the each term broken down:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta)= \frac{N}{2}\log 2\pi + \underbrace{\frac{1}{2} \log\left| \mathbf{Q}_{ff}+\mathbf{G}\right|}_{\text{Complexity Penalty}} + \underbrace{\frac{1}{2}\mathbf{y}^{\top}(\mathbf{Q}_{ff}+\mathbf{G})^{-1}\mathbf{y}}_{\text{Data Fit}} + \underbrace{\frac{1}{2\sigma_n^2}\text{trace}(\mathbf{T})}_{\text{Trace Term}}
\]</div>
<p>The <strong>data fit</strong> term penalizes the data lying outside the covariance ellipse, the <strong>complexity penalty</strong> is the integral of the data fit term over all possible observations <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> which characterizes the volume of possible datasets, the <strong>trace term</strong> ensures the objective function is a true lower bound to the MLE of the full GP. Now, below is a table that shows the differences between each of the methods.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Algorithm</p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(\mathbf{G}\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(\mathbf{T}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>FITC</p></td>
<td class="text-center"><p>diag <span class="math notranslate nohighlight">\((\mathbf{K}_{ff}-\mathbf{Q}_{ff}) + \sigma_n^2\mathbf{I}\)</span></p></td>
<td class="text-center"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>VFE</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\sigma_n^2 \mathbf{I}\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathbf{K}_{ff}-\mathbf{Q}_{ff}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>DTC</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\sigma_n^2 \mathbf{I}\)</span></p></td>
<td class="text-center"><p>0</p></td>
</tr>
</tbody>
</table>
<p>Another thing to keep in mind is that the FITC algorithm approximates the model whereas the VFE algorithm approximates the inference step (the posterior). So here we just a have a difference in philosophy in how one should approach this problem. Many people in the Bayesian community will <a class="reference external" href="https://www.prowler.io/blog/sparse-gps-approximate-the-posterior-not-the-model">argue</a> for approximating the inference but I think it’s important to be pragmatic about these sorts of things.</p>
</section>
<section id="observations-about-the-sparse-gps">
<h3>Observations about the Sparse GPs<a class="headerlink" href="#observations-about-the-sparse-gps" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>VFE</p>
<ul>
<li><p>Overestimates noise variance</p></li>
<li><p>Improves with additional inducing inputs</p></li>
<li><p>Recovers the full GP Posterior</p></li>
<li><p>Hindered by local optima</p></li>
</ul>
</li>
<li><p>FITC</p>
<ul>
<li><p>Can severly underestimate the noise variance</p></li>
<li><p>May ignore additional inducing inputs</p></li>
<li><p>Does not recover the full GP posterior</p></li>
<li><p>Relies on Local Optima</p></li>
</ul>
</li>
</ul>
<p>Some parameter initialization strategies:</p>
<ul class="simple">
<li><p>K-Means</p></li>
<li><p>Initially fixing the hyperparameters</p></li>
<li><p>Random Restarts</p></li>
</ul>
<p>An interesting solution to find good hyperparameters for VFE:</p>
<ol class="arabic simple">
<li><p>Find parameters with FITC solution</p></li>
<li><p>Initialize GP model of VFE with FITC solutions</p></li>
<li><p>Find parameters with VFE.</p></li>
</ol>
<p><strong>Source:</strong></p>
<ul class="simple">
<li><p>Understanding Probabilistic Sparse GP Approximations - Bauer et. al. (2017) - <a class="reference external" href="https://arxiv.org/pdf/1606.04820.pdf">Paper</a></p></li>
<li><p>Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) - <span class="xref myst">Thesis</span></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="variational-compression">
<h3>Variational Compression<a class="headerlink" href="#variational-compression" title="Permalink to this heading">#</a></h3>
<p align="center">
  <img src="pics/variational_compression.png" alt="drawing" width="500"/>
</p>
<p><strong>Figure</strong>: This graphical model shows the relationship between the data <span class="math notranslate nohighlight">\(X\)</span>, the labels <span class="math notranslate nohighlight">\(y\)</span> and the augmented labels <span class="math notranslate nohighlight">\(z\)</span>.</p>
<p>This is a concept I’ve came across that seeks to give a stronger argument for using an augmented space <span class="math notranslate nohighlight">\(\mathcal Z\in \mathbb{R}^{M \times D}\)</span> instead of just the data space <span class="math notranslate nohighlight">\(\mathcal X \in \mathbb{R}^{N \times D}\)</span>. This has allowed us to reduce the computational complexity of all of our most expensive calculations from <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}(NM^2)\)</span> when we are learning the best parameters for our GP models. The term <strong>variational compression</strong> comes from the notion that we want to suppress the function valuse <span class="math notranslate nohighlight">\(f\)</span> with some auxilary variables <span class="math notranslate nohighlight">\(u\)</span>. It’s kind of like reducing the data space <span class="math notranslate nohighlight">\(\mathcal X\)</span> with the auxilary data space <span class="math notranslate nohighlight">\(\mathcal Z\)</span> in a principled way. This approach is very useful as it allows us to use a suite of variational inference techniques which in turn allows us to scale GP methods. In addition, we even have access to advanced optimization strategies such as stochastic variational inference and parallization strategies. You’ll also notice that the GP literature has essentially formulated almost all major GP algorithm families (e.g. GP regression, GP classification and GP latent variable modeling) through this variation compression strategy. Below we will look at a nice argument; presented by Neil Lawrence (MLSS 2019); which really highlights the usefulness and cleverness of this approach and how it relates to many GP algorithms.</p>
<section id="joint-distribution-augmented-space-mathcal-p-f-u">
<h4>Joint Distribution - Augmented Space <span class="math notranslate nohighlight">\(\mathcal{P}(f,u)\)</span><a class="headerlink" href="#joint-distribution-augmented-space-mathcal-p-f-u" title="Permalink to this heading">#</a></h4>
<p>Let’s add an additional set of variables <span class="math notranslate nohighlight">\(u\)</span> that’s jointly Gaussian with our original function <span class="math notranslate nohighlight">\(f\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(f,u)=\mathcal{N}\left( 
    \begin{bmatrix}  
    f \\ u
    \end{bmatrix}; 
    \begin{bmatrix}
    0 \\ 0
    \end{bmatrix},
    \begin{bmatrix}
    K_{ff} &amp; K_{fu} \\
    K_{uf} &amp; K_{uu}
    \end{bmatrix} \right)\end{split}\]</div>
<p>We have a new space where we have introduced some auxilary variables <span class="math notranslate nohighlight">\(u\)</span> to be modeled jointly with <span class="math notranslate nohighlight">\(f\)</span>. Using all of the nice properties of Gaussian distributions, we can easily write down the conditional distribution <span class="math notranslate nohighlight">\(\mathcal{P}(f|u)\)</span> and marginal distribution <span class="math notranslate nohighlight">\(\mathcal{P}(u)\)</span> in terms of the joint distribution <span class="math notranslate nohighlight">\(\mathcal P (f,u)\)</span> using conditional probability rules.</p>
<div class="math notranslate nohighlight">
\[\mathcal P (f,u) = \mathcal{P}(f|u) \cdot \mathcal{P}(u)\]</div>
<p>where:</p>
<ul class="simple">
<li><p>Conditional Dist.: <span class="math notranslate nohighlight">\(\mathcal{P}(\mathbf{f | u}) = \mathcal N (f| \mathbf {\mu_u, \nu^2_{uu}})\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mu_u = \mathbf{K_{fu}K_{uu}^{-1}u}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\nu^2_{uu} = \mathbf{K_{ff} - K_{fu}K_{uu}^{-1}K_{uf}}\)</span></p></li>
</ul>
</li>
<li><p>Augmented space Prior: <span class="math notranslate nohighlight">\(\mathcal P (\mathbf u) = \mathcal N\left( \mathbf u | 0, \mathbf K_{uu} \right)\)</span></p></li>
</ul>
<p>We <strong>could</strong> actually marginalize out <span class="math notranslate nohighlight">\(u\)</span> to get back to the standard GP prior <span class="math notranslate nohighlight">\(\mathcal P (f) = \mathcal{GP} (f | \mathbf{ m, K_{ff}})\)</span>. But keep in mind that the reason why we did the conditional probability is this way is because of the computationally decreased complexity that we gain ,<span class="math notranslate nohighlight">\(\mathcal{O}(N^3) \rightarrow \mathcal{O}(NM^2)\)</span>. We want to ‘compress’ the data space <span class="math notranslate nohighlight">\(\mathcal X\)</span> and subsequently the function space of <span class="math notranslate nohighlight">\(f\)</span>. So now let’s write the complete joint distribution which includes the data likelihood and the augmented latent variable space:</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y,f,u|X,Z)= \underbrace{\mathcal{P}(y|f)}_{\text{Likelihood}} \cdot \underbrace{\mathcal{P} (f|u, X, Z)}_{\text{Conditional Dist.}} \cdot \underbrace{\mathcal{P}(u|Z)}_{\text{Prior}}\]</div>
<p>We have a new term which is the familiar GP likelihood term <span class="math notranslate nohighlight">\(\mathcal P (y|f) = \mathcal{N}(y|f, \sigma_y^2\mathbf I)\)</span>. The rest of the terms we have already defined above. So now you can kind of see how we’re attempting to compress the conditional distribution <span class="math notranslate nohighlight">\(f\)</span>. We no longer need the prior for <span class="math notranslate nohighlight">\(X\)</span> or <span class="math notranslate nohighlight">\(f\)</span> in order to obtain the joint distribution for our model. The prior we have is <span class="math notranslate nohighlight">\(\mathcal P (u)\)</span> which is kind of a made up variable.  From henceforth, I will be omitting the dependency on <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> as they’re not important for the argument that follows. But keep it in the back of your mind that that dependency does exist.</p>
</section>
<section id="conditional-distribution-mathcal-p-y-u">
<h4>Conditional Distribution - <span class="math notranslate nohighlight">\(\mathcal{P}(y|u)\)</span><a class="headerlink" href="#conditional-distribution-mathcal-p-y-u" title="Permalink to this heading">#</a></h4>
<p>The next step would be to try and condition on <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(u\)</span> to obtain the conditional distribution of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(u\)</span>, <span class="math notranslate nohighlight">\(\mathcal{P}(y|u)\)</span>. We can rearrange the terms of the formula above like so:</p>
<div class="math notranslate nohighlight">
\[\frac{\mathcal{P}(y,f,u)}{\mathcal{P}(u)}=  \mathcal{P}(y|f) \cdot\mathcal{P}(f|u)\]</div>
<p>and using the conditional probability rules <span class="math notranslate nohighlight">\(P(A,B)=P(A|B) \cdot P(B) \rightarrow P(A|B)=\frac{P(A,B)}{P(B)}\)</span> we can simplify the formula even further:</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y,f|u)=\mathcal{P}(y|f) \cdot \mathcal{P}(f|u)\]</div>
<p>So, what are we looking at? We are looking at the new joint distribution of <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(f\)</span> given the augmented variable space that we have defined. One step closer to the conditional density. In the nature of GP models and Bayesian inference in general, the next step is to see how we obtain the marginal likelihood where we marginalize out the <span class="math notranslate nohighlight">\(f\)</span>’s. In doing so, we obtain the conditional density that we set off to explore:</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y|u)=\int_f \mathcal{P}(y|f) \cdot \mathcal{P}(f|u) \cdot df\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{P}(y|f)\)</span> - Likelihood</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{P}(f|u)\)</span> - Conditional Distribution</p></li>
</ul>
<p>The last step would be to try and see if we can calculate <span class="math notranslate nohighlight">\(\mathcal{P}(y)\)</span> because if we can get a distribution there, then we can actually train our model using marginal likelihood. Unfortunately we are going to see a problem with this line of thinking when we try to do it directly. If I marginalize out the <span class="math notranslate nohighlight">\(u\)</span>’s I get after grouping the terms:</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y)=\int_u \underbrace{\left[\int_f  \mathcal{P}(y|f) \cdot \mathcal{P}(f|u) \cdot df \right]}_{\mathcal{P}(y|u)}  \cdot \mathcal P(u) \cdot du\]</div>
<p>which reduces to:</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y)=\int_u \mathcal{P}(y|u) \cdot \mathcal P(u) \cdot du\]</div>
<p>This looks very similar to the parameter form of the marginal likelihood. And technically speaking this would allow us to make predictions by conditioning on the trained data <span class="math notranslate nohighlight">\(\mathcal P (y*|y)\)</span>. The two important issues are highlighted in that equation alone:</p>
<ol class="arabic simple">
<li><p>We now have the same bottleneck on our parameter for <span class="math notranslate nohighlight">\(u\)</span> as we do for standard Bayesian parametric modeling.</p></li>
<li><p>The computation of <span class="math notranslate nohighlight">\(\mathcal P (y|u)\)</span> is not trivial calculation and we do not get any computational complexity gains trying to do that integral with the prior <span class="math notranslate nohighlight">\(\mathcal P (u)\)</span>.</p></li>
</ol>
</section>
<section id="variational-bound-on-mathcal-p-y-u">
<h4>Variational Bound on <span class="math notranslate nohighlight">\(\mathcal P (y|u)\)</span><a class="headerlink" href="#variational-bound-on-mathcal-p-y-u" title="Permalink to this heading">#</a></h4>
<p>We’ve shown the difficulties of actually obtaining the probability density function of <span class="math notranslate nohighlight">\(\mathcal{P}(y)\)</span> but in this section we’re just going to show that we can obtain a lower bound for the conditional density function <span class="math notranslate nohighlight">\(\mathcal{P}(y|u)\)</span></p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y|u)=\int_f \mathcal P (y|f) \cdot \mathcal{P}(f|u) \cdot df\]</div>
<p>I’ll do the 4.5 classic steps in order to arrive at a variational lower bound:</p>
<ol class="arabic simple">
<li><p>Given an <strong>integral problem</strong>, take the <span class="math notranslate nohighlight">\(\log\)</span> of both sides of the function.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\log \mathcal P (y|u) = \log \int_f \mathcal P (y|f) \cdot \mathcal{P}(f|u) \cdot df\]</div>
<ol class="arabic simple" start="2">
<li><p>Introduce the variational parameter <span class="math notranslate nohighlight">\(q(f)\)</span> as a <strong>proposal</strong> with the Identity trick.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\log \mathcal P (y|u) = \log \int_f \mathcal P (y|f) \cdot \mathcal{P}(f|u) \cdot \frac{q(f)}{q(f)} \cdot df\]</div>
<ol class="arabic simple" start="3">
<li><p>Use Jensen’s inequality for the log function to rearrange the formula to highlight the <strong>importance weight</strong> and provide a bound for <span class="math notranslate nohighlight">\(\mathcal{F}(q)\)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathcal L () = \log \mathcal P (y|u) \geq  \int_f q(f)  \cdot \log \frac{\mathcal P (y|f) \cdot \mathcal{P}(f|u)}{q(f) } \cdot df = \mathcal F (q)\]</div>
<ol class="arabic simple" start="4">
<li><p>Rearrange to look like an expectation and KL divergence using targeted <span class="math notranslate nohighlight">\(\log\)</span> rules:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathcal F (q) = \int_f q(f) \cdot \log \mathcal P(y|f) \cdot df - \int_f q(f) \cdot \log \frac{\mathcal{P}(f|u)}{q(f)} \cdot df\]</div>
<ol class="arabic simple" start="5">
<li><p>Simplify notation to look like every paper in ML that uses VI to profit and obtain the <strong>variational lower bound</strong>.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathcal F (q) = \mathbb E_{q(f)} \left[ \log \mathcal P(y|f) \right]  - \text{D}_{\text{KL}} \left[ q(f) || \mathcal{P}(f|u)\right]\]</div>
</section>
<section id="titsias-innovation-et-q-f-mathcal-p-f-u">
<h4>Titsias Innovation: et <span class="math notranslate nohighlight">\(q(f) = \mathcal{P}(f|u)\)</span>.<a class="headerlink" href="#titsias-innovation-et-q-f-mathcal-p-f-u" title="Permalink to this heading">#</a></h4>
<p>According to Titsias et al. (2009) he looked at what happens if we let <span class="math notranslate nohighlight">\(q(f)=\mathcal P (f|u)\)</span>. For starters, without our criteria, the KL divergence went to zero and the integral we achieved will have one term less.</p>
<div class="math notranslate nohighlight">
\[\log \mathcal P (y|u) \geq \int_f \mathcal P (f|u) \cdot \log \mathcal P(y|f) \cdot df \]</div>
<p>As a thought experiment though, what would happen if we had thee true posterior of <span class="math notranslate nohighlight">\(\mathcal{P}(f|y,u)\)</span> and an approximating density of <span class="math notranslate nohighlight">\(\mathcal{P}(f|u)\)</span>? Well, we can take the <span class="math notranslate nohighlight">\(KL\)</span> divergence of that quantity and we get the following:</p>
<div class="math notranslate nohighlight">
\[\text{D}_{\text{KL}} \left[ q(f) || \mathcal{P}(f|y, u)\right] = \int_u \mathcal P (f|u) \cdot \log \frac{\mathcal P (f|u)}{\mathcal P (f|y,u)} \cdot du\]</div>
<p>According to Neil Lawrence, maximizing the lower bound minimizes the KL divergence between <span class="math notranslate nohighlight">\(\mathcal{P}(f|u)\)</span> and <span class="math notranslate nohighlight">\(\mathcal{P}(f|u)\)</span>. Maximizing the bound will try to find the optimal compression and looks at the information between <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(u\)</span>. He does not that there is no bound and it is an exact bound when <span class="math notranslate nohighlight">\(u=f\)</span>. I believe that’s related to the GPFlow <a class="reference external" href="https://gpflow.readthedocs.io/en/latest/notebooks/vgp_notes.html">derivation</a> of variational GPs <a class="reference external" href="https://gpflow.readthedocs.io/en/develop/_modules/gpflow/models/vgp.html#VGP">implementation</a> but I don’t have more information on this.</p>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p>Deep Gaussian Processes - <a class="reference external" href="http://inverseprobability.com/talks/notes/deep-gaussian-processes.html">MLSS 2019</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1309.6835.pdf">Gaussian Processes for Big Data</a> - Hensman et. al. (2013)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1412.1370.pdf">Nested Variational Compression in Deep Gaussian Processes</a> - Hensman et. al. (2014)</p></li>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v38/hensman15.pdf">Scalable Variational Gaussian Process Classification</a> - Hensman et. al. (2015)</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="elbos">
<h2>ELBOs<a class="headerlink" href="#elbos" title="Permalink to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathbf \Sigma = K_{ff} - K_{fu}K_{uu}^{-1}K_{fu}^{\top}\)</span></p>
<section id="lower-bound">
<h3>Lower Bound<a class="headerlink" href="#lower-bound" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\mathcal{F}=
\log \mathcal{N} \left(y|0, \tilde{\mathbf K}_{ff} + \sigma_y^2\mathbf I \right) -
\frac{1}{2\sigma_y^2}\text{tr}\left( \mathbf \Sigma\right)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\tilde{\mathbf K}_{ff} = \mathbf{K_{fu}K_{uu}^{-1}K_{fu}^{\top}}\)</span></p>
<ul>
<li><p>Nystrom approximation</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf \Sigma = \mathbf K_{ff} - \tilde{\mathbf K}_{ff}\)</span></p>
<ul>
<li><p>Uncertainty Based Correction</p></li>
</ul>
</li>
</ul>
<section id="variational-bound-on-mathcal-p-y">
<h4>Variational Bound on <span class="math notranslate nohighlight">\(\mathcal P (y)\)</span><a class="headerlink" href="#variational-bound-on-mathcal-p-y" title="Permalink to this heading">#</a></h4>
<p>In this scenario, we marginalize out the remaining <span class="math notranslate nohighlight">\(u\)</span>’s and we can get an error bound on the <span class="math notranslate nohighlight">\(\mathcal P(y)\)</span></p>
<div class="math notranslate nohighlight">
\[\mathcal P (y) = \int_u \mathcal P (y|u) \cdot \mathcal P (u|Z) du\]</div>
<p><strong>Source</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1412.1370.pdf">Nested Variational Compression in Deep Gaussian Processes</a> - Hensman et. al. (2014)</p></li>
<li><p>James Hensman - <a class="reference external" href="http://gpss.cc/gpss15/talks/talk_james.pdf">GPSS 2015</a> | <a class="reference external" href="http://www.approximateinference.org/schedule/Hensman2015.pdf">Aweseome Graphical Models</a></p></li>
</ul>
<p>The explicit form of the lower bound <span class="math notranslate nohighlight">\(\mathcal{P}(y)\)</span> for is gives us:</p>
<div class="math notranslate nohighlight">
\[\log \mathcal P (y) \geq \log \mathcal{N} (y|\mathbf{y|K_{fu}^{-1}m, \sigma_y^2I}) - \frac{1}{2\sigma_y^2} \text{tr}\left(  \right)\]</div>
<p><strong>Source</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1412.1370.pdf">Nested Variational Compression in Deep Gaussian Processes</a> - Hensman et. al. (2014)</p></li>
</ul>
</section>
</section>
<section id="stochastic-variational-inference">
<h3>Stochastic Variational Inference<a class="headerlink" href="#stochastic-variational-inference" title="Permalink to this heading">#</a></h3>
</section>
</section>
<hr class="docutils" />
<section id="supplementary-material">
<h2>Supplementary Material<a class="headerlink" href="#supplementary-material" title="Permalink to this heading">#</a></h2>
<hr class="docutils" />
<section id="important-formulas">
<h3>Important Formulas<a class="headerlink" href="#important-formulas" title="Permalink to this heading">#</a></h3>
<p>These formulas come up when we’re looking for clever ways to deal with sparse matrices in GPs. Typically we will have some matrix <span class="math notranslate nohighlight">\(\mathbf K\in \mathbb R^{N\times N}\)</span> which implies we need to calculate the inverse <span class="math notranslate nohighlight">\(\mathbf K^{-1}\)</span> and the determinant <span class="math notranslate nohighlight">\(|\)</span>det <span class="math notranslate nohighlight">\(\mathbf K|\)</span> which both require <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span>. These formulas below are useful when we want to avoid those computational complexity counts.</p>
<section id="nystrom-approximation">
<h4>Nystrom Approximation<a class="headerlink" href="#nystrom-approximation" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[\mathbf K_{NN} \approx  \mathbf U_{NM} \mathbf \Lambda_{MM} \mathbf U_{NM}^{\top}\]</div>
</section>
<section id="sherman-morrison-woodbury-formula">
<h4>Sherman-Morrison-Woodbury Formula<a class="headerlink" href="#sherman-morrison-woodbury-formula" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[(\mathbf K_{NN} + \sigma_y^2 \mathbf I_N)^{-1} \approx \sigma_y^{-2}\mathbf I_N + \sigma_y^{-2} \mathbf U_{NM}\left( \sigma_y^{-2}\mathbf \Lambda_{MM}^{-1} + \mathbf U_{NM}^{\top} \mathbf U_{NM} \right)^{-1}\mathbf U_{NM}^{\top}\]</div>
</section>
<section id="sylvester-determinant-theorem">
<h4>Sylvester Determinant Theorem<a class="headerlink" href="#sylvester-determinant-theorem" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[\left|\mathbf K_{NN} + \sigma_y^2 \mathbf I_N \right| \approx |\mathbf \Lambda_{MM} | \left|\sigma_y^{2} \mathbf \Lambda_{MM}^{-1} + U_{NM}^{\top} \mathbf U_{NM} \right|\]</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this heading">#</a></h2>
</section>
<section id="code-walk-throughs">
<h2>Code Walk-Throughs<a class="headerlink" href="#code-walk-throughs" title="Permalink to this heading">#</a></h2>
<ul>
<li><p><a class="reference external" href="https://bwengals.github.io/vfe-approximation-for-gaussian-processes-the-gory-details.html">VFE Approximation for GPs, the gory Details</a> | <a class="reference external" href="https://bwengals.github.io/pymc3-fitcvfe-implementation-notes.html">More Notes</a> | <a class="reference external" href="https://bwengals.github.io/inducing-point-methods-to-speed-up-gps.html">Summary of Inducing Point Methods</a></p>
<blockquote>
<div><p>A good walkthrough of the essential equations and how we can implement them from scratch</p>
</div></blockquote>
</li>
<li><p><a class="reference external" href="https://github.com/Alaya-in-Matrix/SparseGP/blob/master/VFE.py">SparseGP - Alaya-in-Matrix</a></p>
<blockquote>
<div><p>Another good walkthrough with everything well defined such that its easy to replicate.</p>
</div></blockquote>
</li>
</ul>
</section>
<section id="math-walkthroughs">
<h2>Math Walkthroughs<a class="headerlink" href="#math-walkthroughs" title="Permalink to this heading">#</a></h2>
<ul>
<li><p><a class="reference external" href="https://www.uv.es/gonmagar/blog/2018/04/19/VariationalFreeEnergy">Gonzalo Blog</a></p>
<blockquote>
<div><p>Step-by-Step Derivations</p>
</div></blockquote>
</li>
</ul>
<section id="papers">
<h3>Papers<a class="headerlink" href="#papers" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>Nystrom Approximation</p>
<ul class="simple">
<li><p><a class="reference external" href="https://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines.pdf">Using Nystrom to Speed Up Kernel Machines</a> - Williams &amp; Seeger (2001)</p></li>
</ul>
</li>
<li><p>Fully Independent Training Conditional (FITC)</p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.gatsby.ucl.ac.uk/~snelson/SPGP_up.pdf">Sparse Gaussian Processes Using Pseudo-Inputs</a> - Snelson and Ghahramani (2006)</p></li>
<li><p><a class="reference external" href="http://www.gatsby.ucl.ac.uk/~snelson/thesis.pdf">Flexible and Efficient GP Models for Machine Learning</a> - Snelson (2007)</p></li>
</ul>
</li>
<li><p>Variational Free Energy (VFE)</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pdfs.semanticscholar.org/9c13/b87b5efb4bb011acc89d90b15f637fa48593.pdf">Variational Learning of Inducing Variables in Sparse GPs</a> - Titsias (2009)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1504.07027.pdf">On Sparse Variational meethods and the KL Divergence between Stochastic Processes</a> - Matthews et. al. (2015)</p></li>
<li><p>Stochastic Variational Inference</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1309.6835.pdf">Gaussian Processes for Big Data</a> - Hensman et al. (2013)</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="http://quinonero.net/Publications/lazaro-gredilla10a.pdf">Sparse Spectrum GPR</a> - Lazaro-Gredilla et al. (2010)</p>
<ul class="simple">
<li><p>SGD, SVI</p>
<ul>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v37/galb15.html">Improving the GP SS Approximation by Representing Uncertainty in Frequency Inputs</a> - Gal et al. (2015)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v70/pan17a/pan17a.pdf">Prediction under Uncertainty in SSGPs w/ Applications to Filtering and Control</a> - Pan et. al. (2017)</p></li>
<li><p><a class="reference external" href="http://www.jmlr.org/papers/volume18/16-579/16-579.pdf">Variational Fourier Features for GPs</a> - Hensman (2018)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1606.04820.pdf">Understanding Probabilistic Sparse GP Approx</a> - Bauer et. al. (2016)</p>
<blockquote>
<div><p>A good paper which highlights some import differences between the FITC, DTC and VFE. It provides a clear notational differences and also mentions how VFE is a special case of DTC.</p>
</div></blockquote>
</li>
<li><p><a class="reference external" href="http://jmlr.org/papers/volume18/16-603/16-603.pdf">A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation</a> - Bui (2017)</p>
<blockquote>
<div><p>A good summary of all of the methods under one unified framework called the Power Expectation Propagation formula.</p>
</div></blockquote>
</li>
</ul>
<section id="thesis-explain">
<h4>Thesis Explain<a class="headerlink" href="#thesis-explain" title="Permalink to this heading">#</a></h4>
<p>Often times the papers that people publish in conferences in Journals don’t have enough information in them. Sometimes it’s really difficult to go through some of the mathematics that people put  in their articles especially with cryptic explanations like “it’s easy to show that…” or “trivially it can be shown that…”. For most of us it’s not easy nor is it trivial. So I’ve included a few thesis that help to explain some of the finer details. I’ve arranged them in order starting from the easiest to the most difficult.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/HildoBijl/GPRT">GPR Techniques</a> - Bijl (2016)</p>
<ul>
<li><p>Chapter V - Noisy Input GPR</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://lib.ugent.be/fulltxt/RUG01/002/367/115/RUG01-002367115_2017_0001_AC.pdf">Non-Stationary Surrogate Modeling with Deep Gaussian Processes</a> - Dutordoir (2016)</p>
<ul>
<li><p>Chapter IV - Finding Uncertain Patterns in GPs</p></li>
</ul>
</li>
<li><p><a class="reference external" href="http://mlg.eng.cam.ac.uk/pub/pdf/Mch14.pdf">Nonlinear Modeling and Control using GPs</a> - McHutchon (2014)</p>
<ul>
<li><p>Chapter II - GP w/ Input Noise (NIGP)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="http://etheses.whiterose.ac.uk/9968/1/Damianou_Thesis.pdf">Deep GPs and Variational Propagation of Uncertainty</a> - Damianou (2015)</p>
<ul>
<li><p>Chapter IV - Uncertain Inputs in Variational GPs</p></li>
<li><p>Chapter II (2.1) - Lit Review</p></li>
</ul>
</li>
<li><p><a class="reference external" href="http://etheses.whiterose.ac.uk/18492/1/MaxZwiesseleThesis.pdf">Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences</a> - Zwießele (2017)</p>
<ul>
<li><p>Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="presentations">
<h3>Presentations<a class="headerlink" href="#presentations" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="http://www2.aueb.gr/users/mtitsias/papers/titsiasNipsVar14.pdf">Variational Inference for Gaussian and Determinantal Point Processes</a> - Titsias (2014)</p></li>
</ul>
</section>
<section id="notes">
<h3>Notes<a class="headerlink" href="#notes" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="http://mlg.eng.cam.ac.uk/thang/docs/talks/rcc_vargp.pdf">On the paper: Variational Learning of Inducing Variables in Sparse Gaussian Processees</a> - Bui and Turner (2014)</p></li>
</ul>
</section>
<section id="blogs">
<h3>Blogs<a class="headerlink" href="#blogs" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://gonzmg88.github.io/blog/2018/04/19/VariationalFreeEnergy">Variational Free Energy for Sparse GPs</a> - Gonzalo</p></li>
<li><p><a class="github reference external" href="https://github.com/Alaya-in-Matrix/SparseGP">Alaya-in-Matrix/SparseGP</a></p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/notes/gps"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="cg.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Conjugate Gradients</p>
      </div>
    </a>
    <a class="right-next"
       href="algorithms.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Algorithms</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subset-of-data">Subset of Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-approximations">Kernel Approximations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inducing-points">Inducing Points</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-gps-inducing-points-summary">Sparse GPs - Inducing Points Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function">Objective Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#observations-about-the-sparse-gps">Observations about the Sparse GPs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-compression">Variational Compression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distribution-augmented-space-mathcal-p-f-u">Joint Distribution - Augmented Space <span class="math notranslate nohighlight">\(\mathcal{P}(f,u)\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-distribution-mathcal-p-y-u">Conditional Distribution - <span class="math notranslate nohighlight">\(\mathcal{P}(y|u)\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-bound-on-mathcal-p-y-u">Variational Bound on <span class="math notranslate nohighlight">\(\mathcal P (y|u)\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#titsias-innovation-et-q-f-mathcal-p-f-u">Titsias Innovation: et <span class="math notranslate nohighlight">\(q(f) = \mathcal{P}(f|u)\)</span>.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#elbos">ELBOs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lower-bound">Lower Bound</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-bound-on-mathcal-p-y">Variational Bound on <span class="math notranslate nohighlight">\(\mathcal P (y)\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-variational-inference">Stochastic Variational Inference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supplementary-material">Supplementary Material</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#important-formulas">Important Formulas</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#nystrom-approximation">Nystrom Approximation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sherman-morrison-woodbury-formula">Sherman-Morrison-Woodbury Formula</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sylvester-determinant-theorem">Sylvester Determinant Theorem</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resources">Resources</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-walk-throughs">Code Walk-Throughs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#math-walkthroughs">Math Walkthroughs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#papers">Papers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#thesis-explain">Thesis Explain</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#presentations">Presentations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notes">Notes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#blogs">Blogs</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By J. Emmanuel Johnson
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>