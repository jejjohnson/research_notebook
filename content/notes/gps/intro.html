
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Gaussian Processes &#8212; Research Notebook</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Basics" href="gps.html" />
    <link rel="prev" title="Maximum Mean Discrepancy (MMD)" href="../kernels/mmd.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/book_v2.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Research Notebook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../resources/python/overview.html">
   Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/ides.html">
     Integraded Development Environment (IDE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/stack.html">
     Standard Python Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/earthsci_stack.html">
     Earth Science Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/dl_stack.html">
     Deep Learning Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/scale_stack.html">
     Scaling Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/good_code.html">
     Good Code
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/jax_journey/overview.html">
   My JAX Journey
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/ecosystem.html">
     Ecosystem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/vmap.html">
     vmap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/jit.html">
     Jit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/classes.html">
     Classes
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/jax_journey/algorithms/overview.html">
     Algorithms
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/algorithms/bisection.html">
       Bisection search
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/algorithms/kernel_derivatives.html">
       Kernel Derivatives
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/gfs_with_jax.html">
       Gaussianization Flows
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/remote/overview.html">
   Remote Computing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/remote/ssh.html">
     SSH Configuration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/remote/conda.html">
     Conda 4 Remote Servers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/remote/jlab.html">
     Jupyter Lab 4 Remote Servers
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../concepts/notation.html">
   Notation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../concepts/quotes.html">
   Quotes
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data/overview.html">
   Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data/representation.html">
     Representation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data/models.html">
     Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../concepts/uncertainty.html">
   Modeling Uncertainty
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../bayesian/overview.html">
   Bayesian
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/intro.html">
     Language of Uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/models.html">
     Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/inference.html">
     Inference Schemes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/inference/variational_inference.html">
     Variational Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/confidence_intervals.html">
     Confidence Intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/regression.html">
     Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../concepts/overview.html">
   Sleeper Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/gaussian.html">
     Gaussian Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/change_of_variables.html">
     Change of Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/identity_trick.html">
     Identity Trick
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/inverse_function.html">
     Inverse Function Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/jensens.html">
     Jensens Inequality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/lin_alg.html">
     Linear Algebra Tricks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../kernels/overview.html">
   Kernel Methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../kernels/kernel_derivatives.html">
     Kernel Derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../kernels/rv.html">
     RV Coefficient
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../kernels/congruence_coeff.html">
     Congruence Coefficient
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../kernels/hsic.html">
     HSIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../kernels/mmd.html">
     Maximum Mean Discrepancy (MMD)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="current reference internal" href="#">
   Gaussian Processes
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="gps.html">
     Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="literature.html">
     Literature Review
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="cg.html">
     Conjugate Gradients
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="sgps.html">
     Sparse Gaussian Processes
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="algorithms.html">
     Algorithms
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
    <label for="toctree-checkbox-10">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="gpr_code.html">
       GP from Scratch
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="sgp_code.html">
       Sparse GP From Scratch
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../egps/overview.html">
     Input Uncertainty in GPs
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../info_theory/similarity.html">
   Similarity
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../info_theory/overview.html">
   Information Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../info_theory/measures.html">
     Measures
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../info_theory/information.html">
       Information Theory
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../info_theory/entropy.html">
       Entropy &amp; Relative Entropy
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../info_theory/mutual_info.html">
       Mutual Information and Total Correlation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../info_theory/estimators.html">
     Information Theory Measures
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../info_theory/classic.html">
       Classic Methods
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../info_theory/histogram.html">
       Entropy Estimator - Histogram
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../info_theory/experiments/rbig_sample_consistency.html">
       Experiment - RBIG Sample Consistency
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../normalizing_flows/overview.html">
   Normalizing Flows
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../normalizing_flows/linear.html">
     Linear Layers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../normalizing_flows/coupling_layers.html">
     Coupling Layers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../normalizing_flows/conditional.html">
     Conditional Normalizing Flows
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../normalizing_flows/multiscale.html">
     Multiscale
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../normalizing_flows/lecture_1_ig.html">
     Lecture I - Iterative Gaussianization
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
    <label for="toctree-checkbox-15">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/1.0_univariate_gauss.html">
       1.1 - Univariate Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/1.1_marginal_gauss.html">
       1.2 - Marginal Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/1.2_gaussianization.html">
       1.2 - Iterative Gaussianization
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../normalizing_flows/lecture_2_gf.html">
     Lecture II - Gaussianization Flows
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
    <label for="toctree-checkbox-16">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/lecture_3_gfs_pt1_mg.html">
       Parameterized Marginal Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/lecture_3_gfs_pt2_rot.html">
       Parameterized Rotations
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/lecture_3_gfs_pt3_plane.html">
       Example - 2D Plane
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../inr/overview.html">
   Implicit Neural Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../inr/formulation.html">
     Formulation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../inr/literature_review.html">
     Literature Review
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data_assimilation/overview.html">
   Data Assimilation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data_assimilation/dynamical_sys.html">
     Dynamical Systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data_assimilation/oi.html">
     Optimal Interpolation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data_assimilation/interp.html">
     Interpolation Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data_assimilation/emu.html">
     Emulation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data_assimilation/inv_problems.html">
     Inverse Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data_assimilation/projects.html">
     Projects
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../data_assimilation/algorithms.html">
     Algorithms
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
    <label for="toctree-checkbox-19">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../data_assimilation/markov_models.html">
       Markov Models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../data_assimilation/gauss_markov.html">
       Gauss-Markov Models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../data_assimilation/kf.html">
       Kalman Filter
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../data_assimilation/nkf.html">
       Normalizing Kalman Filter
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../data_assimilation/enskf.html">
       Ensemble Kalman Filter
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../data_assimilation/dmm.html">
       Deep Markov Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../data_assimilation/4dvarnet.html">
       4DVarNet
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../data_assimilation/markov_gp.html">
       Markovian Gaussian Processes
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../misc/overview.html">
   Miscellaneous Notes
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/generative_models.html">
     Generative Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/diffusion_models.html">
     Diffusion Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/fixed_point.html">
     Fixed-Point Methods
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Cheat Sheets
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../cheatsheets/bash.html">
   Bash
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cheatsheets/cli.html">
   Command Line
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cheatsheets/python.html">
   Python
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/jejjohnson/research_notebook"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jejjohnson/research_notebook/issues/new?title=Issue%20on%20page%20%2Fcontent/notes/gps/intro.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/content/notes/gps/intro.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantities-of-interest">
   Quantities of Interest
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Gaussian Processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-setting">
   Problem Setting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-assumptions">
   Model Assumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exact-gp-inference">
     Exact GP Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimal-solution">
     Optimal Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bottleneck">
   Bottleneck
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predictive-uncertainty">
     Predictive Uncertainty
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-sampling">
     Conditional Sampling
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scaling">
   Scaling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ski-inference">
     SKI Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#derivations">
     Derivations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#predictive-mean">
       Predictive Mean
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression">
   Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussian-process">
     Gaussian Process
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference">
     Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-functions">
   Kernel Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#strengths-and-limitations">
   Strengths and Limitations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limitations">
     Limitations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conjugate-case">
   Conjugate Case
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-conjugate-case">
   Non-Conjugate Case
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Gaussian Processes</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objective">
   Objective
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quantities-of-interest">
   Quantities of Interest
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Gaussian Processes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-setting">
   Problem Setting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-assumptions">
   Model Assumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training">
   Training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exact-gp-inference">
     Exact GP Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimal-solution">
     Optimal Solution
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bottleneck">
   Bottleneck
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predictive-uncertainty">
     Predictive Uncertainty
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-sampling">
     Conditional Sampling
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scaling">
   Scaling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ski-inference">
     SKI Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#derivations">
     Derivations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#predictive-mean">
       Predictive Mean
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression">
   Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussian-process">
     Gaussian Process
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference">
     Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-functions">
   Kernel Functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#strengths-and-limitations">
   Strengths and Limitations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limitations">
     Limitations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conjugate-case">
   Conjugate Case
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#non-conjugate-case">
   Non-Conjugate Case
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="gaussian-processes">
<h1>Gaussian Processes<a class="headerlink" href="#gaussian-processes" title="Permalink to this headline">#</a></h1>
<hr class="docutils" />
<section id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">#</a></h2>
<p>We have a set of sparsely distributed observations from satellite altimetry data. For each observation, there is an associated latitude and longitude spatial coordinate as well as a temporal coordinate. The objective is to interpolate the missing observations for the remaining spatio-temporal coordinates.</p>
</section>
<hr class="docutils" />
<section id="quantities-of-interest">
<h2>Quantities of Interest<a class="headerlink" href="#quantities-of-interest" title="Permalink to this headline">#</a></h2>
<p><strong>Inputs</strong>: We assume that the inputs are a vector of coordinates: latitude, longitude and time, i.e. <span class="math notranslate nohighlight">\(\mathbf{x} = [\text{lat, lon, time}]\)</span>. So a single data point is a 3-dimensional vector, <span class="math notranslate nohighlight">\(\mathbf{x}\in \mathbb{R}^{D_\phi}\)</span>. It is important to note that we are free to do any coordinate transform that we want in order to better represent the data. For example, the temporal coordinates can be converted to spherical coordinates to represent the curvature of the earth. Another example is to convert the temporal coordinates into cyclic coordinates where each hour, day, month, year, etc are converted into a sine and cosine. We will see later that this can be encoded within the kernel function for the Gaussian process which will allow us to capture the assumed dynamics. However, many times physical knowledge of your system can be encoded a priori which can lead to better results.</p>
<p><strong>Outputs</strong>: The outputs are a vector of quantities of interest. For example, we could have a variable which describes the state of the ocean such as sea surface height (SSH) and or sea surface temperature (SST). These variables are then stacked together which gives us a p-dimensional vector, <span class="math notranslate nohighlight">\(\mathbf{y} \in \mathbb{R}^{D_p}\)</span>.</p>
</section>
<hr class="docutils" />
<section id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this headline">#</a></h2>
<p>We assume that we have some set of input and output data points, <span class="math notranslate nohighlight">\(\mathcal{D} = \left\{ \mathbf{x}_n, \mathbf{y}_n \right\}_{n=1}^N\)</span>. This dataset, <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, will be used for training to find the parameters of interest, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
</section>
<hr class="docutils" />
<section id="id1">
<h2>Gaussian Processes<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
</section>
<hr class="docutils" />
<section id="problem-setting">
<h2>Problem Setting<a class="headerlink" href="#problem-setting" title="Permalink to this headline">#</a></h2>
<p>We are interested in the regression problem where we have some quantity of interest, <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, given some inputs, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. We also assume that there exists some function, <span class="math notranslate nohighlight">\(\boldsymbol{f}\)</span>, parameterized by, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, that provides us with a mapping between the inputs, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, and the outputs, <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>. And lastly, we assume that it is corrupted by some identically independently distributed noise, i.e. Gaussian noise. This can be written as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y}_n = \boldsymbol{f}(\mathbf{x}_n; \boldsymbol{\theta}) + \boldsymbol{\epsilon}_n, \hspace{10mm} \boldsymbol{\epsilon} \sim \mathcal{N}(0, \sigma^2)
\]</div>
<p>We are interested in finding a distribution over the functions, <span class="math notranslate nohighlight">\(\boldsymbol{f}\)</span>, that can explain the data, <span class="math notranslate nohighlight">\(\{\mathbf{x},\mathbf{y}\}\)</span>. In other words, we are not interested in the best set of parameters, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> that give us the best fit. Instead we want the best set of parameters, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, which will give us a distribution of functions, <span class="math notranslate nohighlight">\(\boldsymbol{f} \sim P\)</span> that could possibly describe the data.</p>
</section>
<hr class="docutils" />
<section id="model-assumptions">
<h2>Model Assumptions<a class="headerlink" href="#model-assumptions" title="Permalink to this headline">#</a></h2>
<p>We follow the Bayesian formulation but in functional space. Bayes formula can be written as follows:</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{f}(\cdot) | \mathbf{X,Y}) = \frac{p(\mathbf{y}|\boldsymbol{f}(\cdot))\;p(\boldsymbol{f}(\cdot))}{p(\mathbf{Y}|\mathbf{X})}
\]</div>
<p><strong>Prior</strong></p>
<p>The prior, <span class="math notranslate nohighlight">\(p(\boldsymbol{f}(\cdot))\)</span> is a Gaussian process prior which is specified by its mean function, <span class="math notranslate nohighlight">\(\boldsymbol{m}\)</span>, and covariance function, <span class="math notranslate nohighlight">\(\boldsymbol{k}\)</span>. So we have</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{f}(\mathbf{x}) \sim \mathcal{GP}\left(\boldsymbol{f} | \boldsymbol{m}_\psi (\mathbf{x}), \boldsymbol{k}_\phi(\mathbf{x},\mathbf{x}')\right)
\]</div>
<p><strong>Prior Parameters</strong></p>
<p>The mean function, <span class="math notranslate nohighlight">\(\boldsymbol{m}\)</span>, is a mapping from the coordinates, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, to the coordinates of interest, <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>. It represents our prior knowledge of the relationship. This is often assumed to be zero if we don’t have any prior knowledge. However, it can be a parameterized by some hyper-parameters, <span class="math notranslate nohighlight">\(\boldsymbol{\psi}\)</span>, which can also be learned through the Gaussian process regression algorithm. The kernel function, <span class="math notranslate nohighlight">\(\boldsymbol{k} : \mathbb{R}^{D_\phi} \times \mathbb{R}^{D_\phi} \rightarrow \mathbb{R}^{}\)</span> is a mapping representing the correlations between all of the inputs. This is also has some set of parameters, <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span>, which are very important. It represents the correlations is the mean function and <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> is the kernel matrix.</p>
<p><strong>Likelihood</strong></p>
<p>This is the noise model which is assumed to be a Gaussian.</p>
<div class="math notranslate nohighlight">
\[
p(\mathbf{y}|\boldsymbol{f}(\mathbf{X})) = \mathcal{N}(\boldsymbol{f}(\mathbf{X}), \sigma^2 \mathbf{I})
\]</div>
<p>The advantage of this assumption is that it allows us to have keep everything Gaussian because the both the prior and the likelihood are Gaussian distributed. However, many times there is no reason to believe that the likelihood is Gaussian. For example we could have a noise model which is dependent upon the observations or we could have a classification or Log-Cox process scenario whereby we would need a Bernoulli or Poisson distribution respectively. Nevertheless, non-Gaussian likelihoods are out of scope for this work.</p>
<p><strong>Marginal Likelihood</strong></p>
<p>The marginal likelihood is typically the most difficult quantity to calculate.</p>
<div class="math notranslate nohighlight">
\[
p(\mathbf{y}|\mathbf{X},\boldsymbol{\theta}) = \int_{\boldsymbol{f}}p(\mathbf{y}|\boldsymbol{f})p(\boldsymbol{f}|\mathbf{X},\boldsymbol{\theta}) d\boldsymbol{f}
\]</div>
<p>Because the prior, likelihood are Gaussian, we can utilize the conjugacy property which ensures that the marginal likelihood is also Gaussian distributed. So this integral becomes simple because we can calculate this quantity analytically. The formula is given by:</p>
<div class="math notranslate nohighlight">
\[
p(\mathbf{y}|\mathbf{X},\boldsymbol{\theta}) = \mathcal{N}\left(\mathbf{y}|\boldsymbol{m}(\mathbf{X}), \mathbf{K}_{\mathbf{XX}} + \sigma^2\mathbf{I}\right) 
\]</div>
<p><strong>Posterior</strong></p>
<p>The posterior of a Gaussian Process is also a Gaussian process which is normally distributed given predictive mean and predictive covariance.</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{f}(\mathbf{x})|\mathcal{D}) = \mathcal{N}\left(\boldsymbol{\mu}_{\mathcal{GP}}(\mathbf{x}), \boldsymbol{\sigma}^2_\mathcal{GP}(\mathbf{x}, \mathbf{x}')\right)
\]</div>
<p>where we have the analytical formulas for the predictive mean and covariance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{\mu}_{\mathcal{GP}}(\mathbf{x}) &amp;= \boldsymbol{m}(\mathbf{x}) + \boldsymbol{k}_\mathbf{X}(\mathbf{x}) \boldsymbol{\alpha} \\
\boldsymbol{\sigma}^2_\mathcal{GP}(\mathbf{x}, \mathbf{x}') &amp;= \boldsymbol{k}(\mathbf{x}, \mathbf{x}') + \boldsymbol{k}_\mathbf{X}(\mathbf{x})\left( \mathbf{K}_{\mathbf{XX}} + \sigma^2\mathbf{I} \right)^{-1}\boldsymbol{k}_\mathbf{X}(\mathbf{x})^\top
\end{aligned}
\end{split}\]</div>
<hr class="docutils" />
<p><strong>Predictive Density</strong></p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{f}_*|\boldsymbol{f})
\]</div>
<p>Because the prior, likelihood, and the marginal likelihood are all Gaussian, we have a predictive density that is characterized as a multivariate Gaussian distribution.</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{f}(\mathbf{x}_*)|\mathcal{D}) = \mathcal{N}\left(\boldsymbol{\mu}_{\mathcal{GP}}(\mathbf{x}_*), \boldsymbol{\sigma}^2_\mathcal{GP}(\mathbf{x}_*, \mathbf{x}_{*}')\right)
\]</div>
<p>where we have the predictive mean and covariance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{\mu}_{\mathcal{GP}}(\mathbf{x}_*) &amp;= \boldsymbol{m}(\mathbf{x}_*) + \boldsymbol{k}_*(\mathbf{x}_*) \boldsymbol{\alpha} \\
\boldsymbol{\sigma}^2_\mathcal{GP}(\mathbf{x}_*, \mathbf{x}_{*}') &amp;= \boldsymbol{k}(\mathbf{x}_*, \mathbf{x}_{*}') + \boldsymbol{k}_\mathbf{X}(\mathbf{x}_*)\left( \mathbf{K}_{\mathbf{XX}} + \sigma^2\mathbf{I} \right)^{-1}\boldsymbol{k}_\mathbf{X}(\mathbf{x}_*)^\top
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} = \left( \mathbf{K}_{\mathbf{XX}} + \sigma^2\mathbf{I} \right)^{-1}(\mathbf{y} - \boldsymbol{m}(\mathbf{X}))\)</span> is a fixed parameter that can be trained via some inference method, e.g. Maximum Likelihood, Maximum A Posteriori, Variational Inference, MCMC, etc.</p>
</section>
<hr class="docutils" />
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">#</a></h2>
<p>As is very typically in the Bayesian formulation, we can maximize the m
$<span class="math notranslate nohighlight">\(
\boldsymbol{\theta}^* = \argmin_{\boldsymbol{\theta}} - \mathcal{L}(\boldsymbol{\theta})
\)</span>$</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\boldsymbol{\theta}) := \log p(\mathbf{y}|\mathbf{X}, \boldsymbol{\theta})
\]</div>
<section id="exact-gp-inference">
<h3>Exact GP Inference<a class="headerlink" href="#exact-gp-inference" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[
\log p(\mathbf{y}|\mathbf{X}, \boldsymbol{\theta}) = - \frac{N}{2} \log 2 \pi - \frac{1}{2} \log |\mathbf{K}_{\mathbf{XX}} + \sigma^2\mathbf{I}| - \frac{1}{2} (\mathbf{y} - \boldsymbol{m}(\mathbf{X}))^\top\left( \mathbf{K}_{\mathbf{XX}} + \sigma^2\mathbf{I} \right)^{-1}(\mathbf{y} - \boldsymbol{m}(\mathbf{X}))
\]</div>
<p>We introduce some new notation to simplify the equations a little bit.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{K}_{\boldsymbol{\phi}} := \mathbf{K_{XX}} + \sigma^2\mathbf{I}
\]</div>
<div class="math notranslate nohighlight">
\[
\bar{\mathbf{Y}}_{\boldsymbol{\psi}} := \mathbf{Y} - \boldsymbol{m}(\mathbf{X};\boldsymbol{\psi})
\]</div>
<p>We can rewrite the cost function to reflect the new notation.</p>
<div class="math notranslate nohighlight">
\[
\log p(\mathbf{y}|\mathbf{X}, \boldsymbol{\theta}) = - \frac{N}{2} \log 2 \pi - \frac{1}{2} \log |\mathbf{K}_{\boldsymbol{\phi}}| - \frac{1}{2} \bar{\mathbf{Y}}_{\boldsymbol{\psi}}^\top\;\mathbf{K}_{\boldsymbol{\phi}}^{-1}\;\bar{\mathbf{Y}}_{\boldsymbol{\psi}}
\]</div>
<p>where the parameters are the hyper-parameters of the mean function, the noise likelihood and the kernel function respectively. <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = \left\{ \boldsymbol{\psi}, \sigma, \boldsymbol{\phi} \right\}\)</span>.</p>
</section>
<section id="optimal-solution">
<h3>Optimal Solution<a class="headerlink" href="#optimal-solution" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\alpha} := (\mathbf{K_{XX}} + \sigma^2\mathbf{I})^{-1}(\mathbf{Y} - \boldsymbol{m}(\mathbf{X}))
\]</div>
</section>
</section>
<hr class="docutils" />
<section id="bottleneck">
<h2>Bottleneck<a class="headerlink" href="#bottleneck" title="Permalink to this headline">#</a></h2>
<p><strong>Training</strong></p>
<div class="math notranslate nohighlight">
\[
\mathcal{O}(N^3)
\]</div>
<p><strong>Testing</strong></p>
<div class="math notranslate nohighlight">
\[
\mathcal{O}(N^2)
\]</div>
<hr class="docutils" />
<section id="predictive-uncertainty">
<h3>Predictive Uncertainty<a class="headerlink" href="#predictive-uncertainty" title="Permalink to this headline">#</a></h3>
</section>
<hr class="docutils" />
<section id="conditional-sampling">
<h3>Conditional Sampling<a class="headerlink" href="#conditional-sampling" title="Permalink to this headline">#</a></h3>
</section>
</section>
<hr class="docutils" />
<section id="scaling">
<h2>Scaling<a class="headerlink" href="#scaling" title="Permalink to this headline">#</a></h2>
<section id="ski-inference">
<h3>SKI Inference<a class="headerlink" href="#ski-inference" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[
\log p(\mathbf{y}|\mathbf{X},\boldsymbol{\theta}) \approx - \frac{1}{2} \log \left|\det \tilde{\mathbf{K}}_{\mathbf{XX}} + \sigma^2 \mathbf{I}\right| - \frac{1}{2}(\mathbf{y} - \boldsymbol{m}(\mathbf{X}))\left( \tilde{\mathbf{K}}_{\mathbf{XX}} + \sigma^2 \mathbf{I} \right)^{-1} -\frac{N}{2} \log 2\pi
\]</div>
</section>
<section id="derivations">
<h3>Derivations<a class="headerlink" href="#derivations" title="Permalink to this headline">#</a></h3>
<section id="predictive-mean">
<h4>Predictive Mean<a class="headerlink" href="#predictive-mean" title="Permalink to this headline">#</a></h4>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\boldsymbol{\mu}_{\text{KISS-GP}}(\mathbf{x}_*) &amp;= \boldsymbol{w}(\mathbf{x}_*)^\top \mathbf{K_{UU}} \mathbf{}
\end{aligned}
\]</div>
<hr class="docutils" />
<p><strong>Training</strong></p>
<ul class="simple">
<li><p>(Naive) Exact GP - <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span></p></li>
<li><p>Conjugate Gradient - <span class="math notranslate nohighlight">\(\mathcal{O}(N^2)\)</span></p></li>
<li><p>KISS-GP - <span class="math notranslate nohighlight">\(\approx\mathcal{O}(N)\)</span></p></li>
<li><p>Inducing Points - <span class="math notranslate nohighlight">\(\mathcal{O}(NM^3)\)</span></p></li>
<li><p>Variational Stochastic - <span class="math notranslate nohighlight">\(\mathcal{O}(M^3)\)</span></p></li>
</ul>
<p><strong>Predictions</strong></p>
<ul class="simple">
<li><p>(Naive) Exact GP - <span class="math notranslate nohighlight">\(\mathcal{O}(N^2)\)</span></p></li>
<li><p>Conjugate Gradient - <span class="math notranslate nohighlight">\(\mathcal{O}(N)\)</span></p></li>
<li><p>KISS-GP - <span class="math notranslate nohighlight">\(\approx\mathcal{O}(N)\)</span></p></li>
<li><p>Inducing Points - <span class="math notranslate nohighlight">\(\mathcal{O}(NM^2)\)</span></p></li>
<li><p>Variational Stochastic - <span class="math notranslate nohighlight">\(\mathcal{O}(M^2)\)</span></p></li>
</ul>
</section>
</section>
</section>
<section id="regression">
<h2>Regression<a class="headerlink" href="#regression" title="Permalink to this headline">#</a></h2>
<p>Let’s assume we have <span class="math notranslate nohighlight">\(N\)</span> input data points with <span class="math notranslate nohighlight">\(D\)</span> dimensions, <span class="math notranslate nohighlight">\(\mathbf{X} = \{\mathbf{x}_i\}^N_{i=1} \in \mathbb{R}^{N \times D}\)</span> and noisy outputs, <span class="math notranslate nohighlight">\(\mathbf{y} = \{ y_i \}^{N}_{i=1} \in \mathbb{R}^{N}\)</span>. We want to compute the predictive distribution of <span class="math notranslate nohighlight">\(y_*\)</span> at a test location <span class="math notranslate nohighlight">\(\mathbf{x}_*\)</span>. So:</p>
<div class="math notranslate nohighlight">
\[
y_i = f(\mathbf{x}_i) + \epsilon_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is an unknown latent function that is corrupted by Gaussian observation noise <span class="math notranslate nohighlight">\(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span>.</p>
<section id="gaussian-process">
<h3>Gaussian Process<a class="headerlink" href="#gaussian-process" title="Permalink to this headline">#</a></h3>
<p>A Gaussian process is a probability distribution over functions. It places a non-parametric Bayesian model which places a GP prior over a latent function <span class="math notranslate nohighlight">\(f\)</span> as</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}) \sim \mathcal{GP}\left( m(\mathbf{x}), k(\mathbf{x},\mathbf{x}') \right).
\]</div>
<p>where we see it is characterized by a mean function, <span class="math notranslate nohighlight">\(m(\mathbf{x})\)</span> and kernel function <span class="math notranslate nohighlight">\(k(\mathbf{x,x}')\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
m(\mathbf{x}) &amp;= \mathbb{E}[f(\mathbf{x})] \\
k(\mathbf{x,x}') &amp;= \mathbb{E}\left[ (f(\mathbf{x}) - m(\mathbf{x}))(f(\mathbf{x}') - m(\mathbf{x}))  \right]
\end{aligned}
\end{split}\]</div>
<p>The mean function <span class="math notranslate nohighlight">\(m(\mathbf{x})\)</span> is typically zero (for easier computations) and the kernel function typically characterizes the smoothness, scale and … of the GP. Given the training data <span class="math notranslate nohighlight">\(\mathcal{D}=\{\mathbf{X},\mathbf{y}\}\)</span>,</p>
<p>mean function GP prior, <span class="math notranslate nohighlight">\(m_\mathcal{GP}\)</span>, (typically zero) and a covariance function <span class="math notranslate nohighlight">\(k(\mathbf{x}, \mathbf{x}')\)</span> on <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p><strong>Prior</strong></p>
<p><strong>Likelihood</strong> A GP regression model assumes that the outputs can be modeled as</p>
<div class="math notranslate nohighlight">
\[
p(\mathbf{y}|f, \mathbf{X}) \sim \mathcal{N}(y|f, \sigma_y^2\mathbf{I})
\]</div>
<p><strong>Posterior</strong></p>
<hr class="docutils" />
<p>We can obtain a marginal likelihood (model evidence) since any finite set of GPs follows a multivariate Gaussian distribution</p>
<div class="math notranslate nohighlight">
\[
p(\mathbf{y}|\theta) = \int p(\mathbf{y}|f)p(f)df = \mathcal{N}(\mathbf{y}|m_\phi, \mathbf{K}_{GP})
\]</div>
<p>In the simple model (conjugate case due to the Gaussian likelihood), the posterior over <span class="math notranslate nohighlight">\(f, p(f|y)\)</span> can be computed analytically.</p>
<div class="math notranslate nohighlight">
\[
p(\mathbf{y}|\theta) = \mathcal{N}(\mathbf{y}|m_\phi, \mathbf{K}_{GP})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{K}_{GP}=\mathbf{K}_{ff}+ \sigma^2 \mathbf{I}\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span> comprises of the hyperparameters found on the mean function and covariance function. We would then maximize the hyperparameters <span class="math notranslate nohighlight">\(\theta\)</span> via, log marginal-likelihood (see below).</p>
<p>The prediction of <span class="math notranslate nohighlight">\(y_*\)</span> for a test point <span class="math notranslate nohighlight">\(\mathbf{x}_*\)</span> is what we’re really interested in. We can consider the joint distribution of our training data, <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> and test data, <span class="math notranslate nohighlight">\(y_*\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
\mathbf{y} \\
y_*
\end{bmatrix} \sim
\mathcal{N}\left(
\begin{bmatrix} 
    \mathbf{K}_{GP} &amp; k_*^\top\\
    k_* &amp; k_{**}+\sigma^2
\end{bmatrix}   \right)
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(k_{*}=\left[ k(\mathbf{x}_1, \mathbf{x}_*), \ldots, k(\mathbf{x}_N, \mathbf{x}_*)  \right]^\top\)</span> is the projection kernel and <span class="math notranslate nohighlight">\(k_{**}=k(\mathbf{x}_*, \mathbf{x}_*)\)</span> is the test kernel. By way of normally distributed variables, we can obtain the predictive distribution of <span class="math notranslate nohighlight">\(y_*\)</span> conditioned on the training data <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
p(y_*|\mathbf{y}) = \mathcal{N}(\mu_\mathcal{GP}, \sigma^2_\mathcal{GP})
\]</div>
<p>and subsequently closed-form predictive mean and variance equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mu_\mathcal{GP}(\mathbf{x}_*) &amp;= k_{*}^\top \mathbf{K}_{GP}^{-1} \mathbf{y} = k_{*}^\top \alpha  \\
\sigma^2_\mathcal{GP}(\mathbf{x}_*) &amp;= \sigma^2 + k_{**} - k_{*}^\top \mathbf{K}_{GP}^{-1}k_{*}
\end{aligned}
\end{split}\]</div>
</section>
<section id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">#</a></h3>
<p>The covariance function <span class="math notranslate nohighlight">\(k(\mathbf{x}, \mathbf{x}
)\)</span> depends on hyperparameters are usually learned by maximizing the log marginal likelihood.</p>
<div class="math notranslate nohighlight">
\[
\theta^* = \underset{\theta}{\text{argmax}} \log p(\mathbf{y}|\mathbf{X},\theta) = \log \mathcal{N}(\mathbf{y}; 0, \mathbf{K}_\mathcal{GP})
\]</div>
<p>Using the log pdf of a multivariate distribution, we have</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\log \mathcal{N}(\mathbf{y}; 0, \mathbf{K}_\mathcal{GP}) &amp;= - \frac{1}{2} \mathbf{y}^\top \mathbf{K}_\mathcal{GP}^{-1}\mathbf{y} - \frac{1}{2} \log \left| \mathbf{K}_\mathcal{GP} \right| - \frac{N}{2} \log 2 \pi
\end{aligned}
\]</div>
<p>This optimization is done by differentiating the above equation with respect to the hyperparameters. This maximization automatically embodies Occam’s razor as it is a trade-off between the model complexity and overfitting. Typically we do the Cholesky decomposition to efficiently calculate the inversion and determinant measures.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\log p(\mathbf{y}) &amp;= - \frac{1}{2} \mathbf{y}^\top \mathbf{K}_\mathcal{GP}^{-1}\mathbf{y} - \frac{1}{2} \log \left| \mathbf{K}_\mathcal{GP} \right| - \frac{N}{2} \log 2 \pi \\
&amp;= -\frac{1}{2} \||\mathbf{L}^{-1}\mathbf{y}||^2  - \sum_i\log \mathbf{L}_{ii} - \frac{N}{2} \log 2 \pi
\end{aligned}
\end{split}\]</div>
<p>where the <span class="math notranslate nohighlight">\(\mathbf{L}\)</span> is the Cholesky decomposition <span class="math notranslate nohighlight">\(\mathbf{L}= \text{chol}(\mathbf{K}_\mathcal{GP})\)</span>. This gives us a computational complexity of <span class="math notranslate nohighlight">\(\mathcal{O}(N^3 + N^2 + N)\)</span> which is overal <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span>. So this GPR method is really only suited for ~2K-5K problems maximum.</p>
</section>
</section>
<hr class="docutils" />
<section id="kernel-functions">
<h2>Kernel Functions<a class="headerlink" href="#kernel-functions" title="Permalink to this headline">#</a></h2>
<p><strong>Composition Kernels</strong>. Kernels can be combined using sums and products to obtain more expressive formations. Additive kernels (<strong>cite</strong>: Duvenaud)</p>
<p><strong>Input Warping</strong>. This is also the choice for many kernel methods where another model is used instead.</p>
</section>
<hr class="docutils" />
<section id="strengths-and-limitations">
<h2>Strengths and Limitations<a class="headerlink" href="#strengths-and-limitations" title="Permalink to this headline">#</a></h2>
<section id="limitations">
<h3>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline">#</a></h3>
<blockquote class="epigraph">
<div><p>“It is important to keep in mind that Gaussian processes are not appropriate priors for all problems”.</p>
<p class="attribution">—Neal, 1998</p>
</div></blockquote>
<p>It is important to note that although the GP algorithm is one of the most trusted and reliable algorithms, it is not always the best algorithm to use for all problems. Below we mention a few drawbacks that the standard GP algorithm has along with some of the standard approaches to overcoming these drawbacks.</p>
<p><strong>Gaussian Marginals</strong>: GPs have problems modeling heavy-tailed, asymmetric or multi-modal marginal distributions. There are some methods that change the likelihood so that it is heavy tailed~\citep{GPTSTUDENT2011,GPTSTUDENT2014} but this would remove the conjugacy of the likelihood term which would incur difficulties during fitting. Deep GPs and latent covariate models are an improvement to this limitation. A very popular approach is to construct a fully Bayesian model. This entails hyperpriors over the kernel parameters and Monte carlo sampling methods such as Gibbs sampling~\citep{GPGIBBS08}, slice sampling~\citep{GPSLICE2010}, Hamiltonian Monte Carlo~\citep{GPHMC2018}, and Sequential Monte Carlo~\citep{GPSMC15}. These techniques will capture more complex distributions. With the advent of better software~\citep{PYMC16,NUMPYRO2019} and more advanced sampling techniques like a differentiable iterative NUTS implementation~\citep{NUMPYRO2019}, the usefulness of MC schemes is resurfacing.</p>
<p><strong>Limited Number of Moments</strong>. This is related to the previous limitation: the idea that an entire function can be captured in terms of two moments: a mean and a covariance. There are some relationships which are difficult to capture without an adequate description, e.g. discontinuities~\citep{Neal96} and non-stationary processes, and thus is a limitation of the GP priors we choose. The advent of warping the inputs or outputs of a GP has becoming a very popular technique to deal with the limited expressivity of kernels. Input warping is popular in methods such as deep kernel learning whereby a Neural network is used to capture the features and are used as inputs to the kernel function output warping is common in chained~\citep{GPCHAINED2016} and heteroscedastic methods where the function output is warped by another GP to capture the noise model of the data. Deep Gaussian processes~\citep{Damianou2015} can be thought of input and output warping methods due the multi-layer composition of function inputs and outputs.</p>
<p><strong>Linearity of Predictive Mean</strong>. The predictive mean of a GP is linear to the observations, i.e. <span class="math notranslate nohighlight">\(\mu_{GP}=\mathbf{K}\alpha\)</span>. This essentially is a smoother which can be very powerful but also will miss key features. If there is some complex structured embedded within the dataset, then a GP model can never really capture this irregardless of the covariance function found.</p>
<p><strong>Predictive Covariance</strong>. The GP predictive variance is a function of the training inputs and it is independent of the observed inputs. This is important if the input data has some information which could be used to help determine the regions of uncertainty, e.g. the gradient. An example would be data on a spatial grid whereby some regions points would have more certainty than others which could be obtained by knowing the input location and not necessarily the expected output.</p>
</section>
</section>
<section id="conjugate-case">
<h2>Conjugate Case<a class="headerlink" href="#conjugate-case" title="Permalink to this headline">#</a></h2>
<hr class="docutils" />
<section id="id2">
<h3>Inference<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
<p>After specifying the prior distributions, we need to infer the posterior distributions of the parameters <span class="math notranslate nohighlight">\(\theta,\phi,f\)</span>.</p>
<p>We represent this as the</p>
<p>We can optimize this function using the negative log-likelihood of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\Theta) = \frac{1}{2} \left| \mathbf{K}_\mathcal{GP}+ \sigma^2 \mathbf{I} \right| + \frac{1}{2}\mathbf{y}^\top
\]</div>
</section>
</section>
<section id="non-conjugate-case">
<h2>Non-Conjugate Case<a class="headerlink" href="#non-conjugate-case" title="Permalink to this headline">#</a></h2>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/notes/gps"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../kernels/mmd.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Maximum Mean Discrepancy (MMD)</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="gps.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Basics</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By J. Emmanuel Johnson<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>