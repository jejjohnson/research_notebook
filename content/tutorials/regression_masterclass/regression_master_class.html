
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Master Class in Regression and Uncertainty with Tensorflow Probability &#8212; Research Notebook</title>
    
  <link href="../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/book.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Research Notebook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../intro.html">
   Overview
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../resources/python/overview.html">
   Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/ides.html">
     Integraded Development Environment (IDE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/stack.html">
     Standard Python Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/earthsci_stack.html">
     Earth Science Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/dl_stack.html">
     Deep Learning Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/scale_stack.html">
     Scaling Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/good_code.html">
     Good Code
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../jax_journey/overview.html">
   My JAX Journey
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../jax_journey/ecosystem.html">
     Ecosystem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../jax_journey/vmap.html">
     vmap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../jax_journey/jit.html">
     Jit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../jax_journey/classes.html">
     Classes
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../jax_journey/algorithms/overview.html">
     Algorithms
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../jax_journey/algorithms/bisection.html">
       Bisection search
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../jax_journey/algorithms/kernel_derivatives.html">
       Kernel Derivatives
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../jax_journey/algorithms/gpr.html">
       GP from Scratch
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../jax_journey/gfs_with_jax.html">
       Gaussianization Flows
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../remote/overview.html">
   Remote Computing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../remote/conda.html">
     Conda 4 Remote Servers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../remote/vscode_jlab.html">
     JupyterLab + VSCode
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../notes/bayesian/overview.html">
   Bayesian
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/intro.html">
     Bayesian: Language of Uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/gaussian.html">
     Gaussian Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/regression.html">
     Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/inference/inference.html">
     Solving Hard Integral Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/inference/variational_inference.html">
     Variational Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/confidence_intervals.html">
     Confidence Intervals
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../notes/concepts/overview.html">
   Sleeper Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/notation.html">
     Notation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/change_of_variables.html">
     Change of Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/identity_trick.html">
     Identity Trick
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/inverse_function.html">
     Inverse Function Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/jensens.html">
     Jensens Inequality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/lin_alg.html">
     Linear Algebra Tricks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../notes/kernels/overview.html">
   Kernel Methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/kernels/kernel_derivatives.html">
     Kernel Derivatives
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../notes/egps/overview.html">
   Uncertain Gaussian Processes
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/predictions.html">
     Uncertain Predictions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/gauss_approx.html">
     Gaussian Approximation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/mcmc.html">
     Monte Carlo Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/taylor.html">
     Linearization (Taylor Expansions)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/moment_matching.html">
     Moment Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/bgplvm.html">
     Bayesian GPLVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/error_prop.html">
     Error Propagation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/next.html">
     Next Steps
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../notes/egps/experiments.html">
     Notebooks
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../notes/egps/notebooks/gpytorch_egp_taylor.html">
       Gaussian Process Gradients with GPyTorch
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../notes/info_theory/overview.html">
   Information Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/info_theory/histogram.html">
     Entropy Estimator - Histogram
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/info_theory/experiments/rbig_sample_consistency.html">
     Experiment - RBIG Sample Consistency
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../normalizing_flows/overview.html">
   Normalizing Flows
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../normalizing_flows/lecture_1_ig.html">
     Lecture I - Iterative Gaussianization
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/1.0_univariate_gauss.html">
       1.1 - Univariate Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/1.1_marginal_gauss.html">
       1.2 - Marginal Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/1.2_gaussianization.html">
       1.2 - Iterative Gaussianization
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../normalizing_flows/lecture_2_gf.html">
     Lecture II - Gaussianization Flows
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/lecture_3_gfs_pt1_mg.html">
       Parameterized Marginal Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/lecture_3_gfs_pt2_rot.html">
       Parameterized Rotations
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../normalizing_flows/lecture_3_gfs_pt3_plane.html">
       Example - 2D Plane
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/tutorials/regression_masterclass/regression_master_class.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/jejjohnson/research_notebook/main?urlpath=tree/research_notebook/content/tutorials/regression_masterclass/regression_master_class.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resources">
   Resources
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-functions">
     Plot Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression">
     Linear Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-linear-regression-maximum-likelihood">
   Bayesian Linear Regression (Maximum Likelihood)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aleatoric-uncertainty">
     Aleatoric Uncertainty
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-linear-regression-variational">
     Bayesian Linear Regression (Variational)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variational-distribution-poster">
       Variational Distribution Poster
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prior-distribution">
       Prior Distribution
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network">
   Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probabilistic-neural-network">
   Probabilistic Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probabilistic-neural-network-heteroscedasstic">
   Probabilistic Neural Network (Heteroscedasstic)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-neural-network-drop-out">
   Bayesian Neural Network (Drop-Out)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probabilisitc-nn-dropout-heteroscedastic">
   Probabilisitc NN (Dropout - Heteroscedastic)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-neural-network-vanilla">
   Bayesian Neural Network (Vanilla)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#support-vector-regression-rff-approximations">
   Support Vector Regression (RFF Approximations)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#svr-deep">
   SVR (Deep)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-processes-rff-approx">
   Gaussian Processes (RFF Approx.)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#heteroscedastic">
     Heteroscedastic
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-gaussian-processes-rff">
   Deep Gaussian Processes (RFF)
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Master Class in Regression and Uncertainty with Tensorflow Probability</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resources">
   Resources
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-functions">
     Plot Functions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression">
     Linear Regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-linear-regression-maximum-likelihood">
   Bayesian Linear Regression (Maximum Likelihood)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aleatoric-uncertainty">
     Aleatoric Uncertainty
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-linear-regression-variational">
     Bayesian Linear Regression (Variational)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variational-distribution-poster">
       Variational Distribution Poster
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prior-distribution">
       Prior Distribution
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#neural-network">
   Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probabilistic-neural-network">
   Probabilistic Neural Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probabilistic-neural-network-heteroscedasstic">
   Probabilistic Neural Network (Heteroscedasstic)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-neural-network-drop-out">
   Bayesian Neural Network (Drop-Out)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probabilisitc-nn-dropout-heteroscedastic">
   Probabilisitc NN (Dropout - Heteroscedastic)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bayesian-neural-network-vanilla">
   Bayesian Neural Network (Vanilla)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#support-vector-regression-rff-approximations">
   Support Vector Regression (RFF Approximations)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#svr-deep">
   SVR (Deep)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-processes-rff-approx">
   Gaussian Processes (RFF Approx.)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#heteroscedastic">
     Heteroscedastic
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-gaussian-processes-rff">
   Deep Gaussian Processes (RFF)
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <p><a href="https://colab.research.google.com/github/jejjohnson/research_notebook/blob/develop/content/tutorials/regression_masterclass/regression_master_class.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<div class="tex2jax_ignore mathjax_ignore section" id="master-class-in-regression-and-uncertainty-with-tensorflow-probability">
<h1>Master Class in Regression and Uncertainty with Tensorflow Probability<a class="headerlink" href="#master-class-in-regression-and-uncertainty-with-tensorflow-probability" title="Permalink to this headline">¶</a></h1>
<p>In this notebook, I will exploring the new <span class="xref myst">tensorflow 2.0</span>, <span class="xref myst">tensorflow probability</span> and <span class="xref myst">edward2</span>. We have come a long way</p>
<div class="section" id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this headline">¶</a></h2>
<p><strong>BNN</strong></p>
<ul class="simple">
<li><p>A regression masterclass with Aboleth - <a class="reference external" href="https://aboleth.readthedocs.io/en/stable/tutorials/some_regressors.html">docs</a></p></li>
<li><p>Bayesian Regressions with MCMC or Variational Bayes using TensorFlow Probability - <a class="reference external" href="https://brendanhasz.github.io/2018/12/03/tfp-regression">blog</a></p></li>
<li><p>Regression with Probabilistic Layers in TensorFlow Probability - <a class="reference external" href="https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf">blog</a> | <a class="reference external" href="https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_Layers_Regression.ipynb">Notebook</a></p></li>
<li><p>Variational Inference for Bayesian Neural Networks - <a class="reference external" href="http://krasserm.github.io/2019/03/14/bayesian-neural-networks/">Blog</a></p></li>
</ul>
<p><strong>TF2</strong></p>
<ul class="simple">
<li><p>Bayesian Gaussian Mixture Modeling with SVI (06-2019) - <a class="reference external" href="https://brendanhasz.github.io/2019/06/12/tfp-gmm.html">blog</a></p></li>
<li><p>Trip Duration Prediction using Bayesian NN and TF2.0 (06-2019) - <a class="reference external" href="https://brendanhasz.github.io/2019/07/23/bayesian-density-net.html">Blog</a></p></li>
</ul>
<p><strong>Keras</strong></p>
<ul class="simple">
<li><p>A Tutorial on Variational Autoencoders with Concise Keras Implementation - <a class="reference external" href="https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/">blog</a></p></li>
<li><p>Building VAE in TensorFlow - <a class="reference external" href="https://danijar.com/building-variational-auto-encoders-in-tensorflow/">blog</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># Install TensorFlow
!pip install tensorflow-probability edward2 loguru
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.7/dist-packages (0.12.1)
Collecting edward2
?25l  Downloading https://files.pythonhosted.org/packages/54/b2/fc68d9c91acd4b4bc86c2a7fe7737483320affd819378175391cd6f7f0ca/edward2-0.0.2-py2.py3-none-any.whl (163kB)
     |████████████████████████████████| 163kB 7.7MB/s 
?25hCollecting loguru
?25l  Downloading https://files.pythonhosted.org/packages/6d/48/0a7d5847e3de329f1d0134baf707b689700b53bd3066a5a8cfd94b3c9fc8/loguru-0.5.3-py3-none-any.whl (57kB)
     |████████████████████████████████| 61kB 10.1MB/s 
?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (4.4.2)
Requirement already satisfied: cloudpickle&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.3.0)
Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (0.1.6)
Requirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.19.5)
Requirement already satisfied: gast&gt;=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (0.4.0)
Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.15.0)
Installing collected packages: edward2, loguru
Successfully installed edward2-0.0.2 loguru-0.5.3
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Import Packages</span>

<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="c1"># TensorFlow Imports</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># TensorFlow Probability Imports</span>
<span class="kn">import</span> <span class="nn">tensorflow_probability</span> <span class="k">as</span> <span class="nn">tfp</span>
<span class="n">tfd</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span>

<span class="c1"># Keras Imports</span>

<span class="c1"># Edward Imports</span>
<span class="kn">import</span> <span class="nn">edward2</span> <span class="k">as</span> <span class="nn">ed</span>

<span class="c1"># GPFlow Imports</span>

<span class="c1"># NUMPY SETTINGS</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">onp</span>
<span class="n">onp</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># MATPLOTLIB Settings</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s1">&#39;retina&#39;</span>

<span class="c1"># SEABORN SETTINGS</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="s1">&#39;talk&#39;</span><span class="p">,</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="c1"># sns.set(rc={&#39;figure.figsize&#39;: (12, 9.)})</span>
<span class="c1"># sns.set_style(&quot;whitegrid&quot;)</span>

<span class="c1"># PANDAS SETTINGS</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_rows&quot;</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_columns&quot;</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>

<span class="c1"># LOGGING SETTINGS</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">loguru</span>

<span class="o">%</span><span class="n">load_ext</span> <span class="n">autoreload</span>
<span class="o">%</span><span class="n">autoreload</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">tf</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">gpu_device_name</span><span class="p">()</span> <span class="o">!=</span> <span class="s1">&#39;/device:GPU:0&#39;</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;WARNING: GPU device not found.&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;SUCCESS: Found GPU: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">gpu_device_name</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SUCCESS: Found GPU: /device:GPU:0
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span> <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span> <span class="o">+</span> <span class="n">noise</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">ntrain</span><span class="p">,</span> <span class="n">ntest</span><span class="p">,</span> <span class="n">ntrue</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1_000</span>

<span class="n">xtrain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">ntrain</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">ytrain</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xtrain</span><span class="p">)</span>

<span class="n">xtest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="n">ntest</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">ytest</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">xtrue</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="n">ntrue</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">ytrue</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xtrue</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Data&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">ytest</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True Function&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_9_0.png" src="../../../_images/regression_master_class_9_0.png" />
</div>
</div>
<div class="section" id="plot-functions">
<h3>Plot Functions<a class="headerlink" href="#plot-functions" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_results</span><span class="p">(</span><span class="n">ypred</span><span class="p">,</span> <span class="n">stddevs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Points&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">ypred</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Points&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtrue</span><span class="p">,</span> <span class="n">ytrue</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Latent Function&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stddevs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
            <span class="n">xtest</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> 
            <span class="p">(</span><span class="n">ypred</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">stddevs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> 
            <span class="p">(</span><span class="n">ypred</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">stddevs</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span>
            <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Confidence Interval&#39;</span><span class="p">,</span> 
        <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    
    <span class="k">return</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
    
    <span class="c1"># plot training history</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="linear-regression">
<h3>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h3>
<p>This first case, we assume that there is no uncertainty in our model. We are just trying to function that maps our data from <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>.</p>
<div class="math notranslate nohighlight">
\[f=wx+b\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w\)</span> - regression weights</p></li>
<li><p><span class="math notranslate nohighlight">\(b\)</span> - bias</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\underset{w,b}{\text{min}} \frac{1}{2N}\sum_{i=1}^{N}
|| wx_i + b - y_i||_2^2 + 
\frac{\lambda}{2} \left( ||w||_2^2 + ||b||_2^2 \right)
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> - regularization coefficient to penalize large valued weights</p></li>
<li><p><span class="math notranslate nohighlight">\(l_2\)</span> - reconstruction error (MSE)</p></li>
<li><p><span class="math notranslate nohighlight">\(l_2\)</span> - regularization for the weights and bias</p></li>
<li><p><span class="math notranslate nohighlight">\(l_1\)</span> - kernel regularization</p></li>
</ul>
<p>In terms of the tensorflow 2.0 library, we have everything that we need to satisfy this objective function with the <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dense</span></code> layer. It has all of the options to add the additional regularizations that we need for our objective functions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
    <span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span>
    <span class="n">kernel_regularization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

<span class="c1"># single layer parameters</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span>              <span class="c1"># activation function</span>
<span class="n">kernel_regularizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># l2 regularization</span>
<span class="n">bias_regularizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>   <span class="c1"># l2 regularizer</span>

<span class="c1"># initialize model</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="c1"># configure model for mean-squared error regression</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;mean_squared_error&#39;</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mean_absolute_error&#39;</span><span class="p">]</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">es</span><span class="p">])</span>

<span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/100
8/8 [==============================] - 0s 3ms/step - loss: 2.5008 - mean_absolute_error: 1.4235
Epoch 2/100
8/8 [==============================] - 0s 3ms/step - loss: 1.3174 - mean_absolute_error: 1.0436
Epoch 3/100
8/8 [==============================] - 0s 3ms/step - loss: 0.6008 - mean_absolute_error: 0.7038
Epoch 4/100
8/8 [==============================] - 0s 3ms/step - loss: 0.2455 - mean_absolute_error: 0.4325
Epoch 5/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1389 - mean_absolute_error: 0.2955
Epoch 6/100
8/8 [==============================] - 0s 4ms/step - loss: 0.1293 - mean_absolute_error: 0.2781
Epoch 7/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1359 - mean_absolute_error: 0.2921
Epoch 8/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1350 - mean_absolute_error: 0.2909
Epoch 9/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1302 - mean_absolute_error: 0.2838
Epoch 10/100
8/8 [==============================] - 0s 4ms/step - loss: 0.1277 - mean_absolute_error: 0.2792
Epoch 11/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1271 - mean_absolute_error: 0.2766
Epoch 12/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1273 - mean_absolute_error: 0.2769
Epoch 13/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1272 - mean_absolute_error: 0.2770
Epoch 14/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1271 - mean_absolute_error: 0.2771
Epoch 15/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1270 - mean_absolute_error: 0.2763
Epoch 16/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1270 - mean_absolute_error: 0.2749
Epoch 17/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1271 - mean_absolute_error: 0.2735
Epoch 18/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1270 - mean_absolute_error: 0.2729
Epoch 19/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1270 - mean_absolute_error: 0.2732
Epoch 20/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1270 - mean_absolute_error: 0.2755
Epoch 21/100
8/8 [==============================] - 0s 4ms/step - loss: 0.1271 - mean_absolute_error: 0.2769
Epoch 22/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1270 - mean_absolute_error: 0.2764
Epoch 23/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1270 - mean_absolute_error: 0.2770
Epoch 24/100
8/8 [==============================] - 0s 4ms/step - loss: 0.1270 - mean_absolute_error: 0.2767
Epoch 25/100
8/8 [==============================] - 0s 4ms/step - loss: 0.1270 - mean_absolute_error: 0.2760
Epoch 26/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1270 - mean_absolute_error: 0.2750
Epoch 27/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1271 - mean_absolute_error: 0.2742
Epoch 28/100
8/8 [==============================] - 0s 4ms/step - loss: 0.1271 - mean_absolute_error: 0.2751
Epoch 29/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1270 - mean_absolute_error: 0.2755
Epoch 30/100
8/8 [==============================] - 0s 4ms/step - loss: 0.1271 - mean_absolute_error: 0.2767
Epoch 31/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1270 - mean_absolute_error: 0.2763
Epoch 32/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1271 - mean_absolute_error: 0.2751
Epoch 33/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1273 - mean_absolute_error: 0.2761
Epoch 34/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1270 - mean_absolute_error: 0.2750
Epoch 35/100
8/8 [==============================] - 0s 3ms/step - loss: 0.1271 - mean_absolute_error: 0.2751
</pre></div>
</div>
<img alt="../../../_images/regression_master_class_13_1.png" src="../../../_images/regression_master_class_13_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_14_0.png" src="../../../_images/regression_master_class_14_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="bayesian-linear-regression-maximum-likelihood">
<h2>Bayesian Linear Regression (Maximum Likelihood)<a class="headerlink" href="#bayesian-linear-regression-maximum-likelihood" title="Permalink to this headline">¶</a></h2>
<p>This will be very similar to the above linear regression formulation.</p>
<div class="math notranslate nohighlight">
\[y_i \sim \mathcal{N}(wx_i, \sigma^2)\]</div>
<div class="math notranslate nohighlight">
\[w \sim \mathcal{N}(0, \lambda^{-1})\]</div>
<div class="math notranslate nohighlight">
\[w \sim \mathcal{N}(m, v)\]</div>
<p>First we need a log likelihood function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">neg_log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">p_y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Negative log likelihood loss function&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">p_y</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we want to define a normal distribution as the last layer of our neural network. We can use this special keras layer called the <code class="docutils literal notranslate"><span class="pre">DistributionLambda</span></code> which allows us to define a distribution that is conditioned on the layer inputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">noise</span> <span class="o">=</span> <span class="mf">0.01</span>     <span class="c1"># fixed noise parameter</span>
<span class="n">layer</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noise</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Note</strong>: we have to actually set the noise here. It would probably be much better if we actually learned the noise parameter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lambda_</span> <span class="o">=</span> <span class="mf">100.</span>
<span class="n">noise</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lambda_</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># weight std dev. prior</span>

<span class="c1"># build model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">noise</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">neg_log_likelihood</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">es</span><span class="p">])</span>

<span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/100
8/8 [==============================] - 0s 3ms/step - loss: 1317.3788
Epoch 2/100
8/8 [==============================] - 0s 2ms/step - loss: 435.8055
Epoch 3/100
8/8 [==============================] - 0s 3ms/step - loss: 69.9018
Epoch 4/100
8/8 [==============================] - 0s 3ms/step - loss: 9.2511
Epoch 5/100
8/8 [==============================] - 0s 2ms/step - loss: 27.1877
Epoch 6/100
8/8 [==============================] - 0s 2ms/step - loss: 22.8518
Epoch 7/100
8/8 [==============================] - 0s 3ms/step - loss: 8.9779
Epoch 8/100
8/8 [==============================] - 0s 2ms/step - loss: 5.0881
Epoch 9/100
8/8 [==============================] - 0s 2ms/step - loss: 6.0635
Epoch 10/100
8/8 [==============================] - 0s 3ms/step - loss: 5.6853
Epoch 11/100
8/8 [==============================] - 0s 2ms/step - loss: 5.0791
Epoch 12/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9924
Epoch 13/100
8/8 [==============================] - 0s 2ms/step - loss: 5.0030
Epoch 14/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9634
Epoch 15/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9737
Epoch 16/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9714
Epoch 17/100
8/8 [==============================] - 0s 3ms/step - loss: 5.0022
Epoch 18/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9703
Epoch 19/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9605
Epoch 20/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9669
Epoch 21/100
8/8 [==============================] - 0s 4ms/step - loss: 4.9647
Epoch 22/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9787
Epoch 23/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9849
Epoch 24/100
8/8 [==============================] - 0s 5ms/step - loss: 4.9666
Epoch 25/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9576
Epoch 26/100
8/8 [==============================] - 0s 3ms/step - loss: 5.0174
Epoch 27/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9612
Epoch 28/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9770
Epoch 29/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9622
Epoch 30/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9679
Epoch 31/100
8/8 [==============================] - 0s 3ms/step - loss: 5.0044
Epoch 32/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9533
Epoch 33/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9675
Epoch 34/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9747
Epoch 35/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9799
Epoch 36/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9611
Epoch 37/100
8/8 [==============================] - 0s 2ms/step - loss: 5.0147
Epoch 38/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9811
Epoch 39/100
8/8 [==============================] - 0s 2ms/step - loss: 5.0000
Epoch 40/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9723
Epoch 41/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9714
Epoch 42/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9571
Epoch 43/100
8/8 [==============================] - 0s 3ms/step - loss: 5.0059
Epoch 44/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9524
Epoch 45/100
8/8 [==============================] - 0s 3ms/step - loss: 5.0194
Epoch 46/100
8/8 [==============================] - 0s 2ms/step - loss: 5.0106
Epoch 47/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9858
Epoch 48/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9679
Epoch 49/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9622
Epoch 50/100
8/8 [==============================] - 0s 4ms/step - loss: 4.9665
Epoch 51/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9660
Epoch 52/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9943
Epoch 53/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9911
Epoch 54/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9619
Epoch 55/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9868
Epoch 56/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9721
Epoch 57/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9826
Epoch 58/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9713
Epoch 59/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9662
Epoch 60/100
8/8 [==============================] - 0s 3ms/step - loss: 5.0265
Epoch 61/100
8/8 [==============================] - 0s 3ms/step - loss: 5.0157
Epoch 62/100
8/8 [==============================] - 0s 3ms/step - loss: 5.0192
Epoch 63/100
8/8 [==============================] - 0s 2ms/step - loss: 4.9599
Epoch 64/100
8/8 [==============================] - 0s 3ms/step - loss: 4.9718
</pre></div>
</div>
<img alt="../../../_images/regression_master_class_18_1.png" src="../../../_images/regression_master_class_18_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">yhat</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_19_0.png" src="../../../_images/regression_master_class_19_0.png" />
</div>
</div>
<div class="section" id="aleatoric-uncertainty">
<h3>Aleatoric Uncertainty<a class="headerlink" href="#aleatoric-uncertainty" title="Permalink to this headline">¶</a></h3>
<p>In this demonstration, I make an assumption that we can have uncertain w.r.t. to the inputs. This pivots off the note I mentioned earlier about being able to fit the noise as a function of the inputs.</p>
<p>If we wanted to actually predict the noise coefficient of the normal distribution we could do something like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">noise</span> <span class="o">=</span> <span class="mf">0.05</span>     <span class="c1"># noise</span>
<span class="n">jitter</span> <span class="o">=</span> <span class="mf">1e-3</span>    <span class="c1"># for stability reasons</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> 
        <span class="n">scale</span><span class="o">=</span><span class="mf">1e-3</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">noise</span> <span class="o">*</span> <span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]))</span>
<span class="p">)</span>
</pre></div>
</div>
<p>But at the very least, we now have a neural network that gives us some uncertain estimates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lambda_</span> <span class="o">=</span> <span class="mf">100.</span>
<span class="n">noise</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lambda_</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># weight std dev. prior</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lambda_</span> <span class="o">=</span> <span class="mf">100.</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">.5</span> <span class="c1">#( 1 / lambda_) ** (1/2)  # weight std dev. prior</span>
<span class="n">jitter</span> <span class="o">=</span> <span class="mf">1e-3</span>    <span class="c1"># for stability reasons</span>

<span class="c1"># build model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">],</span> 
            <span class="n">scale</span><span class="o">=</span><span class="n">jitter</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">noise</span> <span class="o">*</span> <span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]))</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">neg_log_likelihood</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">es</span><span class="p">])</span>

<span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/100
8/8 [==============================] - 1s 3ms/step - loss: 73298.3281
Epoch 2/100
8/8 [==============================] - 0s 3ms/step - loss: 4533.5747
Epoch 3/100
8/8 [==============================] - 0s 3ms/step - loss: 861.9164
Epoch 4/100
8/8 [==============================] - 0s 3ms/step - loss: 361.4844
Epoch 5/100
8/8 [==============================] - 0s 3ms/step - loss: 257.9195
Epoch 6/100
8/8 [==============================] - 0s 3ms/step - loss: 208.2983
Epoch 7/100
8/8 [==============================] - 0s 3ms/step - loss: 186.9795
Epoch 8/100
8/8 [==============================] - 0s 3ms/step - loss: 174.5722
Epoch 9/100
8/8 [==============================] - 0s 3ms/step - loss: 165.7518
Epoch 10/100
8/8 [==============================] - 0s 3ms/step - loss: 158.7435
Epoch 11/100
8/8 [==============================] - 0s 3ms/step - loss: 153.3272
Epoch 12/100
8/8 [==============================] - 0s 3ms/step - loss: 146.9155
Epoch 13/100
8/8 [==============================] - 0s 3ms/step - loss: 142.4492
Epoch 14/100
8/8 [==============================] - 0s 3ms/step - loss: 137.2356
Epoch 15/100
8/8 [==============================] - 0s 3ms/step - loss: 132.4207
Epoch 16/100
8/8 [==============================] - 0s 3ms/step - loss: 127.4735
Epoch 17/100
8/8 [==============================] - 0s 3ms/step - loss: 123.4007
Epoch 18/100
8/8 [==============================] - 0s 3ms/step - loss: 118.7350
Epoch 19/100
8/8 [==============================] - 0s 3ms/step - loss: 114.4710
Epoch 20/100
8/8 [==============================] - 0s 3ms/step - loss: 110.5165
Epoch 21/100
8/8 [==============================] - 0s 3ms/step - loss: 106.3463
Epoch 22/100
8/8 [==============================] - 0s 3ms/step - loss: 102.4903
Epoch 23/100
8/8 [==============================] - 0s 3ms/step - loss: 99.2937
Epoch 24/100
8/8 [==============================] - 0s 3ms/step - loss: 95.5626
Epoch 25/100
8/8 [==============================] - 0s 3ms/step - loss: 92.1285
Epoch 26/100
8/8 [==============================] - 0s 3ms/step - loss: 89.0106
Epoch 27/100
8/8 [==============================] - 0s 3ms/step - loss: 86.0473
Epoch 28/100
8/8 [==============================] - 0s 3ms/step - loss: 82.9365
Epoch 29/100
8/8 [==============================] - 0s 3ms/step - loss: 80.0623
Epoch 30/100
8/8 [==============================] - 0s 3ms/step - loss: 77.3325
Epoch 31/100
8/8 [==============================] - 0s 3ms/step - loss: 75.0416
Epoch 32/100
8/8 [==============================] - 0s 3ms/step - loss: 72.2651
Epoch 33/100
8/8 [==============================] - 0s 4ms/step - loss: 69.9082
Epoch 34/100
8/8 [==============================] - 0s 3ms/step - loss: 67.4570
Epoch 35/100
8/8 [==============================] - 0s 3ms/step - loss: 65.4502
Epoch 36/100
8/8 [==============================] - 0s 3ms/step - loss: 63.2964
Epoch 37/100
8/8 [==============================] - 0s 3ms/step - loss: 61.1707
Epoch 38/100
8/8 [==============================] - 0s 3ms/step - loss: 59.3757
Epoch 39/100
8/8 [==============================] - 0s 3ms/step - loss: 57.2415
Epoch 40/100
8/8 [==============================] - 0s 3ms/step - loss: 55.5948
Epoch 41/100
8/8 [==============================] - 0s 3ms/step - loss: 53.8336
Epoch 42/100
8/8 [==============================] - 0s 3ms/step - loss: 52.1053
Epoch 43/100
8/8 [==============================] - 0s 3ms/step - loss: 50.5567
Epoch 44/100
8/8 [==============================] - 0s 3ms/step - loss: 49.1166
Epoch 45/100
8/8 [==============================] - 0s 3ms/step - loss: 47.4484
Epoch 46/100
8/8 [==============================] - 0s 3ms/step - loss: 46.1313
Epoch 47/100
8/8 [==============================] - 0s 3ms/step - loss: 44.6578
Epoch 48/100
8/8 [==============================] - 0s 3ms/step - loss: 43.3886
Epoch 49/100
8/8 [==============================] - 0s 3ms/step - loss: 42.1085
Epoch 50/100
8/8 [==============================] - 0s 3ms/step - loss: 41.0031
Epoch 51/100
8/8 [==============================] - 0s 3ms/step - loss: 39.7499
Epoch 52/100
8/8 [==============================] - 0s 3ms/step - loss: 38.6597
Epoch 53/100
8/8 [==============================] - 0s 3ms/step - loss: 37.4508
Epoch 54/100
8/8 [==============================] - 0s 4ms/step - loss: 36.4570
Epoch 55/100
8/8 [==============================] - 0s 3ms/step - loss: 35.4384
Epoch 56/100
8/8 [==============================] - 0s 3ms/step - loss: 34.4211
Epoch 57/100
8/8 [==============================] - 0s 3ms/step - loss: 33.4486
Epoch 58/100
8/8 [==============================] - 0s 3ms/step - loss: 32.5205
Epoch 59/100
8/8 [==============================] - 0s 3ms/step - loss: 31.6791
Epoch 60/100
8/8 [==============================] - 0s 4ms/step - loss: 30.8179
Epoch 61/100
8/8 [==============================] - 0s 3ms/step - loss: 29.9380
Epoch 62/100
8/8 [==============================] - 0s 3ms/step - loss: 29.1514
Epoch 63/100
8/8 [==============================] - 0s 3ms/step - loss: 28.3352
Epoch 64/100
8/8 [==============================] - 0s 3ms/step - loss: 27.6315
Epoch 65/100
8/8 [==============================] - 0s 3ms/step - loss: 26.9303
Epoch 66/100
8/8 [==============================] - 0s 3ms/step - loss: 26.1411
Epoch 67/100
8/8 [==============================] - 0s 3ms/step - loss: 25.4828
Epoch 68/100
8/8 [==============================] - 0s 3ms/step - loss: 24.8927
Epoch 69/100
8/8 [==============================] - 0s 3ms/step - loss: 24.2183
Epoch 70/100
8/8 [==============================] - 0s 3ms/step - loss: 23.6048
Epoch 71/100
8/8 [==============================] - 0s 3ms/step - loss: 22.9698
Epoch 72/100
8/8 [==============================] - 0s 4ms/step - loss: 22.3853
Epoch 73/100
8/8 [==============================] - 0s 3ms/step - loss: 21.8522
Epoch 74/100
8/8 [==============================] - 0s 3ms/step - loss: 21.2553
Epoch 75/100
8/8 [==============================] - 0s 4ms/step - loss: 20.7402
Epoch 76/100
8/8 [==============================] - 0s 3ms/step - loss: 20.2466
Epoch 77/100
8/8 [==============================] - 0s 3ms/step - loss: 19.7578
Epoch 78/100
8/8 [==============================] - 0s 3ms/step - loss: 19.2317
Epoch 79/100
8/8 [==============================] - 0s 3ms/step - loss: 18.7562
Epoch 80/100
8/8 [==============================] - 0s 3ms/step - loss: 18.3265
Epoch 81/100
8/8 [==============================] - 0s 3ms/step - loss: 17.8679
Epoch 82/100
8/8 [==============================] - 0s 3ms/step - loss: 17.4117
Epoch 83/100
8/8 [==============================] - 0s 3ms/step - loss: 16.9913
Epoch 84/100
8/8 [==============================] - 0s 3ms/step - loss: 16.5954
Epoch 85/100
8/8 [==============================] - 0s 3ms/step - loss: 16.1907
Epoch 86/100
8/8 [==============================] - 0s 3ms/step - loss: 15.8159
Epoch 87/100
8/8 [==============================] - 0s 3ms/step - loss: 15.4551
Epoch 88/100
8/8 [==============================] - 0s 3ms/step - loss: 15.0804
Epoch 89/100
8/8 [==============================] - 0s 4ms/step - loss: 14.7118
Epoch 90/100
8/8 [==============================] - 0s 3ms/step - loss: 14.3583
Epoch 91/100
8/8 [==============================] - 0s 3ms/step - loss: 14.0798
Epoch 92/100
8/8 [==============================] - 0s 3ms/step - loss: 13.7244
Epoch 93/100
8/8 [==============================] - 0s 3ms/step - loss: 13.3845
Epoch 94/100
8/8 [==============================] - 0s 3ms/step - loss: 13.1091
Epoch 95/100
8/8 [==============================] - 0s 3ms/step - loss: 12.7926
Epoch 96/100
8/8 [==============================] - 0s 3ms/step - loss: 12.5157
Epoch 97/100
8/8 [==============================] - 0s 3ms/step - loss: 12.2248
Epoch 98/100
8/8 [==============================] - 0s 3ms/step - loss: 11.9413
Epoch 99/100
8/8 [==============================] - 0s 3ms/step - loss: 11.6696
Epoch 100/100
8/8 [==============================] - 0s 3ms/step - loss: 11.4101
</pre></div>
</div>
<img alt="../../../_images/regression_master_class_22_1.png" src="../../../_images/regression_master_class_22_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">yhat</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_23_0.png" src="../../../_images/regression_master_class_23_0.png" />
</div>
</div>
<p>As you can see, our error bars change w.r.t. to the inputs.</p>
</div>
<div class="section" id="bayesian-linear-regression-variational">
<h3>Bayesian Linear Regression (Variational)<a class="headerlink" href="#bayesian-linear-regression-variational" title="Permalink to this headline">¶</a></h3>
<p>In this example, we will be looking at how we can extend the Bayesian LR method with variational features. From an uncvertainty perspective, we are looking at</p>
<p>Again, this will be very similar to the above linear regression formulation.</p>
<div class="math notranslate nohighlight">
\[y_i \sim \mathcal{N}(wx_i, \sigma^2)\]</div>
<div class="math notranslate nohighlight">
\[w \sim \mathcal{N}(0, \lambda^{-1})\]</div>
<p>We want to estimate the parameters of the posterior distribution over the weights.</p>
<div class="math notranslate nohighlight">
\[w \sim \mathcal{N}(m, v)\]</div>
<p>We can use the evidence lower bound (ELBO) which we can easily use with stochastic gradient descent (SGD) methods which also allows us to train in batches. For Bayesian linear regression, we have it of the form:</p>
<div class="math notranslate nohighlight">
\[\underset{m,\nu, \sigma}{\text{min}}-\sigma_{i=1}^N
\mathbb{E}_{\mathcal{N}(w|m,\nu)}
\left[\log \mathcal{N}(y_i | wx_i, \sigma^2) \right] + 
\text{KL}\left[ \mathcal{N}(w|m, \nu) || \mathcal{N}(w| 0, \lambda^{-1}) \right]
\]</div>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.DenseVariational</span></code> in order to define our distribution.</p>
<p>In addition, we can define some prior distributions for our variational parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dtype</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">kernel_size</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">bias_size</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">+</span> <span class="n">bias_size</span>

<span class="c1"># define prior</span>
<span class="n">priors</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">VariableLayer</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
        <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">))</span>
<span class="p">])</span>
</pre></div>
</div>
<div class="section" id="variational-distribution-poster">
<h4>Variational Distribution Poster<a class="headerlink" href="#variational-distribution-poster" title="Permalink to this headline">¶</a></h4>
<p>We will use one of the simplest: the mean field. This will give us a surrogate (pyro guide) distribution to learn for our layer with the weights and bias.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.</span>
<span class="k">def</span> <span class="nf">posterior_mean_field</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">bias_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">+</span> <span class="n">bias_size</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">VariableLayer</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
      <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
          <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">n</span><span class="p">],</span>
                     <span class="n">scale</span><span class="o">=</span><span class="mf">1e-5</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">c</span> <span class="o">+</span> <span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">n</span><span class="p">:])),</span>
          <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span>
    <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="prior-distribution">
<h4>Prior Distribution<a class="headerlink" href="#prior-distribution" title="Permalink to this headline">¶</a></h4>
<p>We also need to find the prior distribution to put over our layer with the weights and bias terms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify the prior over `keras.layers.Dense` `kernel` and `bias`.</span>
<span class="k">def</span> <span class="nf">prior_trainable</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">bias_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">+</span> <span class="n">bias_size</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
      <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">VariableLayer</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
      <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
          <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
          <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span>
    <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># model parameters</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">100.</span>
<span class="n">noise</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lambda_</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># weight std dev. prior</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="n">xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="c1"># build model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="c1"># Variational Layer</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseVariational</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="n">posterior_mean_field</span><span class="p">,</span>
        <span class="n">prior_trainable</span><span class="p">,</span>
        <span class="n">kl_weight</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">n_samples</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="c1"># Normal Distribution Output</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> 
            <span class="n">scale</span><span class="o">=</span><span class="n">noise</span>
        <span class="p">)</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">neg_log_likelihood</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">es</span><span class="p">])</span>

<span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 1000 samples
Epoch 1/500
1000/1000 [==============================] - 1s 881us/sample - loss: 1051.8567
Epoch 2/500
1000/1000 [==============================] - 0s 121us/sample - loss: 310.4524
Epoch 3/500
1000/1000 [==============================] - 0s 120us/sample - loss: 176.5165
Epoch 4/500
1000/1000 [==============================] - 0s 125us/sample - loss: 149.0721
Epoch 5/500
1000/1000 [==============================] - 0s 116us/sample - loss: 81.2045
Epoch 6/500
1000/1000 [==============================] - 0s 115us/sample - loss: 54.7781
Epoch 7/500
1000/1000 [==============================] - 0s 126us/sample - loss: 42.9768
Epoch 8/500
1000/1000 [==============================] - 0s 124us/sample - loss: 34.1406
Epoch 9/500
1000/1000 [==============================] - 0s 121us/sample - loss: 50.0728
Epoch 10/500
1000/1000 [==============================] - 0s 121us/sample - loss: 18.7812
Epoch 11/500
1000/1000 [==============================] - 0s 118us/sample - loss: 27.3416
Epoch 12/500
1000/1000 [==============================] - 0s 126us/sample - loss: 24.1858
Epoch 13/500
1000/1000 [==============================] - 0s 119us/sample - loss: 27.8181
Epoch 14/500
1000/1000 [==============================] - 0s 130us/sample - loss: 18.0781
Epoch 15/500
1000/1000 [==============================] - 0s 118us/sample - loss: 17.7182
Epoch 16/500
1000/1000 [==============================] - 0s 127us/sample - loss: 13.5446
Epoch 17/500
1000/1000 [==============================] - 0s 114us/sample - loss: 18.9252
Epoch 18/500
1000/1000 [==============================] - 0s 125us/sample - loss: 11.9005
Epoch 19/500
1000/1000 [==============================] - 0s 120us/sample - loss: 11.3718
Epoch 20/500
1000/1000 [==============================] - 0s 133us/sample - loss: 12.3544
Epoch 21/500
1000/1000 [==============================] - 0s 120us/sample - loss: 11.1220
Epoch 22/500
1000/1000 [==============================] - 0s 117us/sample - loss: 12.4893
Epoch 23/500
1000/1000 [==============================] - 0s 121us/sample - loss: 13.7702
Epoch 24/500
1000/1000 [==============================] - 0s 118us/sample - loss: 10.5066
Epoch 25/500
1000/1000 [==============================] - 0s 125us/sample - loss: 11.1874
Epoch 26/500
1000/1000 [==============================] - 0s 120us/sample - loss: 9.2671
Epoch 27/500
1000/1000 [==============================] - 0s 121us/sample - loss: 8.9355
Epoch 28/500
1000/1000 [==============================] - 0s 141us/sample - loss: 11.5637
Epoch 29/500
1000/1000 [==============================] - 0s 142us/sample - loss: 10.2753
Epoch 30/500
1000/1000 [==============================] - 0s 147us/sample - loss: 9.7203
Epoch 31/500
1000/1000 [==============================] - 0s 139us/sample - loss: 9.0806
Epoch 32/500
1000/1000 [==============================] - 0s 128us/sample - loss: 8.6142
Epoch 33/500
1000/1000 [==============================] - 0s 131us/sample - loss: 8.9697
Epoch 34/500
1000/1000 [==============================] - 0s 125us/sample - loss: 7.4841
Epoch 35/500
1000/1000 [==============================] - 0s 120us/sample - loss: 9.3590
Epoch 36/500
1000/1000 [==============================] - 0s 128us/sample - loss: 8.1423
Epoch 37/500
1000/1000 [==============================] - 0s 120us/sample - loss: 8.8797
Epoch 38/500
1000/1000 [==============================] - 0s 121us/sample - loss: 8.0964
Epoch 39/500
1000/1000 [==============================] - 0s 124us/sample - loss: 9.6021
Epoch 40/500
1000/1000 [==============================] - 0s 128us/sample - loss: 6.9901
Epoch 41/500
1000/1000 [==============================] - 0s 136us/sample - loss: 9.2934
Epoch 42/500
1000/1000 [==============================] - 0s 121us/sample - loss: 8.6313
Epoch 43/500
1000/1000 [==============================] - 0s 126us/sample - loss: 7.9173
Epoch 44/500
1000/1000 [==============================] - 0s 129us/sample - loss: 7.0447
Epoch 45/500
1000/1000 [==============================] - 0s 123us/sample - loss: 6.0707
Epoch 46/500
1000/1000 [==============================] - 0s 128us/sample - loss: 7.4112
Epoch 47/500
1000/1000 [==============================] - 0s 125us/sample - loss: 6.2182
Epoch 48/500
1000/1000 [==============================] - 0s 127us/sample - loss: 7.6416
Epoch 49/500
1000/1000 [==============================] - 0s 127us/sample - loss: 6.4902
Epoch 50/500
1000/1000 [==============================] - 0s 117us/sample - loss: 6.3669
Epoch 51/500
1000/1000 [==============================] - 0s 125us/sample - loss: 6.6177
Epoch 52/500
1000/1000 [==============================] - 0s 141us/sample - loss: 6.5610
Epoch 53/500
1000/1000 [==============================] - 0s 140us/sample - loss: 6.7969
Epoch 54/500
1000/1000 [==============================] - 0s 140us/sample - loss: 6.2981
Epoch 55/500
1000/1000 [==============================] - 0s 123us/sample - loss: 7.2727
Epoch 56/500
1000/1000 [==============================] - 0s 121us/sample - loss: 5.6639
Epoch 57/500
1000/1000 [==============================] - 0s 119us/sample - loss: 5.7161
Epoch 58/500
1000/1000 [==============================] - 0s 125us/sample - loss: 6.2051
Epoch 59/500
1000/1000 [==============================] - 0s 120us/sample - loss: 6.0068
Epoch 60/500
1000/1000 [==============================] - 0s 123us/sample - loss: 5.9463
Epoch 61/500
1000/1000 [==============================] - 0s 122us/sample - loss: 6.2070
Epoch 62/500
1000/1000 [==============================] - 0s 129us/sample - loss: 6.4505
Epoch 63/500
1000/1000 [==============================] - 0s 141us/sample - loss: 6.1540
Epoch 64/500
1000/1000 [==============================] - 0s 128us/sample - loss: 6.4854
Epoch 65/500
1000/1000 [==============================] - 0s 125us/sample - loss: 5.7087
Epoch 66/500
1000/1000 [==============================] - 0s 121us/sample - loss: 5.7137
Epoch 67/500
1000/1000 [==============================] - 0s 122us/sample - loss: 6.3516
Epoch 68/500
1000/1000 [==============================] - 0s 132us/sample - loss: 6.1732
Epoch 69/500
1000/1000 [==============================] - 0s 125us/sample - loss: 6.3164
Epoch 70/500
1000/1000 [==============================] - 0s 131us/sample - loss: 6.1752
Epoch 71/500
1000/1000 [==============================] - 0s 132us/sample - loss: 6.0294
Epoch 72/500
1000/1000 [==============================] - 0s 123us/sample - loss: 6.0254
Epoch 73/500
1000/1000 [==============================] - 0s 117us/sample - loss: 5.4970
Epoch 74/500
1000/1000 [==============================] - 0s 122us/sample - loss: 5.8618
Epoch 75/500
1000/1000 [==============================] - 0s 117us/sample - loss: 6.1550
Epoch 76/500
1000/1000 [==============================] - 0s 126us/sample - loss: 6.1017
Epoch 77/500
1000/1000 [==============================] - 0s 133us/sample - loss: 6.1331
Epoch 78/500
1000/1000 [==============================] - 0s 114us/sample - loss: 5.7901
Epoch 79/500
1000/1000 [==============================] - 0s 121us/sample - loss: 5.7143
Epoch 80/500
1000/1000 [==============================] - 0s 123us/sample - loss: 5.6703
Epoch 81/500
1000/1000 [==============================] - 0s 116us/sample - loss: 5.8908
Epoch 82/500
1000/1000 [==============================] - 0s 118us/sample - loss: 6.2491
Epoch 83/500
1000/1000 [==============================] - 0s 121us/sample - loss: 6.0259
Epoch 84/500
1000/1000 [==============================] - 0s 126us/sample - loss: 5.9545
Epoch 85/500
1000/1000 [==============================] - 0s 123us/sample - loss: 5.7686
Epoch 86/500
1000/1000 [==============================] - 0s 125us/sample - loss: 5.8091
Epoch 87/500
1000/1000 [==============================] - 0s 124us/sample - loss: 5.6683
Epoch 88/500
1000/1000 [==============================] - 0s 139us/sample - loss: 6.1053
Epoch 89/500
1000/1000 [==============================] - 0s 120us/sample - loss: 5.7628
Epoch 90/500
1000/1000 [==============================] - 0s 131us/sample - loss: 5.7552
Epoch 91/500
1000/1000 [==============================] - 0s 123us/sample - loss: 5.6831
Epoch 92/500
1000/1000 [==============================] - 0s 139us/sample - loss: 5.4303
Epoch 93/500
1000/1000 [==============================] - 0s 126us/sample - loss: 5.9431
Epoch 94/500
1000/1000 [==============================] - 0s 127us/sample - loss: 5.7167
Epoch 95/500
1000/1000 [==============================] - 0s 144us/sample - loss: 5.3367
Epoch 96/500
1000/1000 [==============================] - 0s 145us/sample - loss: 5.5336
Epoch 97/500
1000/1000 [==============================] - 0s 137us/sample - loss: 5.4039
Epoch 98/500
1000/1000 [==============================] - 0s 141us/sample - loss: 5.4637
Epoch 99/500
1000/1000 [==============================] - 0s 139us/sample - loss: 5.5878
Epoch 100/500
1000/1000 [==============================] - 0s 145us/sample - loss: 5.4922
Epoch 101/500
1000/1000 [==============================] - 0s 123us/sample - loss: 5.8743
Epoch 102/500
1000/1000 [==============================] - 0s 128us/sample - loss: 5.7109
Epoch 103/500
1000/1000 [==============================] - 0s 120us/sample - loss: 5.5652
Epoch 104/500
1000/1000 [==============================] - 0s 129us/sample - loss: 5.5316
Epoch 105/500
1000/1000 [==============================] - 0s 127us/sample - loss: 5.5454
Epoch 106/500
1000/1000 [==============================] - 0s 115us/sample - loss: 5.1872
Epoch 107/500
1000/1000 [==============================] - 0s 131us/sample - loss: 5.8518
Epoch 108/500
1000/1000 [==============================] - 0s 124us/sample - loss: 5.3208
Epoch 109/500
1000/1000 [==============================] - 0s 165us/sample - loss: 5.9143
Epoch 110/500
1000/1000 [==============================] - 0s 134us/sample - loss: 5.5872
Epoch 111/500
1000/1000 [==============================] - 0s 121us/sample - loss: 5.3485
Epoch 112/500
1000/1000 [==============================] - 0s 124us/sample - loss: 5.3230
Epoch 113/500
1000/1000 [==============================] - 0s 119us/sample - loss: 5.4384
Epoch 114/500
1000/1000 [==============================] - 0s 124us/sample - loss: 5.3930
Epoch 115/500
1000/1000 [==============================] - 0s 125us/sample - loss: 5.3478
Epoch 116/500
1000/1000 [==============================] - 0s 132us/sample - loss: 5.4495
Epoch 117/500
1000/1000 [==============================] - 0s 116us/sample - loss: 5.5497
Epoch 118/500
1000/1000 [==============================] - 0s 118us/sample - loss: 5.4122
Epoch 119/500
1000/1000 [==============================] - 0s 121us/sample - loss: 5.4313
Epoch 120/500
1000/1000 [==============================] - 0s 112us/sample - loss: 5.3015
Epoch 121/500
1000/1000 [==============================] - 0s 122us/sample - loss: 5.6338
Epoch 122/500
1000/1000 [==============================] - 0s 117us/sample - loss: 5.5364
Epoch 123/500
1000/1000 [==============================] - 0s 117us/sample - loss: 5.2956
Epoch 124/500
1000/1000 [==============================] - 0s 126us/sample - loss: 5.2773
Epoch 125/500
1000/1000 [==============================] - 0s 127us/sample - loss: 5.5227
Epoch 126/500
1000/1000 [==============================] - 0s 130us/sample - loss: 5.4633
</pre></div>
</div>
<img alt="../../../_images/regression_master_class_30_1.png" src="../../../_images/regression_master_class_30_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">yhat</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:Layer sequential_55 is casting an input tensor from dtype float64 to the layer&#39;s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it&#39;s dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(&#39;float64&#39;)`. To change just this layer, pass dtype=&#39;float64&#39; to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.
</pre></div>
</div>
<img alt="../../../_images/regression_master_class_31_1.png" src="../../../_images/regression_master_class_31_1.png" />
</div>
</div>
</div>
</div>
</div>
<div class="section" id="neural-network">
<h2>Neural Network<a class="headerlink" href="#neural-network" title="Permalink to this headline">¶</a></h2>
<p>We will do a classic neural network scenario.</p>
<div class="math notranslate nohighlight">
\[f=\text{NN}(x)\]</div>
<p>The NN refers to as a composition of functions.</p>
<div class="math notranslate nohighlight">
\[\underset{w,b}{\text{min}} \frac{1}{2N}\sum_{i=1}^{N}
|| \text{NN}(x_i) + b - y_i||_2^2 + 
\sum_{l=1}^{L}\frac{\lambda_l}{2} \left( ||w_l||_2^2 + ||b_l||_2^2 \right)
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda\)</span> - regularization coefficient to penalize large valued weights</p></li>
<li><p><span class="math notranslate nohighlight">\(l_2\)</span> - reconstruction error (MSE)</p></li>
<li><p><span class="math notranslate nohighlight">\(l_2\)</span> - regularization for the weights and bias</p></li>
<li><p><span class="math notranslate nohighlight">\(l_1\)</span> - kernel regularization</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single layer parameters</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">1e-5</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">sigmoid</span>                 <span class="c1"># activation function</span>
<span class="n">kernel_regularizer</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(lambda_) # l2 regularization</span>
<span class="n">bias_regularizer</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(lambda_)   # l2 regularizer</span>


<span class="c1"># build model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;mean_squared_error&#39;</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_33_0.png" src="../../../_images/regression_master_class_33_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_34_0.png" src="../../../_images/regression_master_class_34_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_35_0.png" src="../../../_images/regression_master_class_35_0.png" />
</div>
</div>
</div>
<div class="section" id="probabilistic-neural-network">
<h2>Probabilistic Neural Network<a class="headerlink" href="#probabilistic-neural-network" title="Permalink to this headline">¶</a></h2>
<p>So we will have the same NN architecture as above except this time we will add a Gaussian distribution as the last layer of the NN.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single layer parameters</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">sigmoid</span>                 <span class="c1"># activation function</span>
<span class="n">w_reg</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(lambda_) # l2 regularization</span>
<span class="n">b_reg</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(lambda_)   # l2 regularizer</span>

<span class="c1"># final prob layer parameters</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.1</span>        <span class="c1"># likelihood stddev</span>


<span class="c1"># build model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">w_reg</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">b_reg</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">w_reg</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">b_reg</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">w_reg</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">b_reg</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">noise</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">neg_log_likelihood</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">p_y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Negative log likelihood loss function&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">p_y</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
<span class="n">loss</span> <span class="o">=</span> <span class="n">neg_log_likelihood</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>

<span class="c1"># es = tf.keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;, mode=&#39;min&#39;, patience=20)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_38_0.png" src="../../../_images/regression_master_class_38_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">yhat</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_39_0.png" src="../../../_images/regression_master_class_39_0.png" />
</div>
</div>
</div>
<div class="section" id="probabilistic-neural-network-heteroscedasstic">
<h2>Probabilistic Neural Network (Heteroscedasstic)<a class="headerlink" href="#probabilistic-neural-network-heteroscedasstic" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># dense layer parameters</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">sigmoid</span>                 <span class="c1"># activation function</span>
<span class="n">w_reg</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(lambda_) # l2 regularization</span>
<span class="n">b_reg</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(lambda_)   # l2 regularizer</span>

<span class="c1"># dropout layer fixed params</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">jitter</span> <span class="o">=</span> <span class="mf">1e-5</span>

<span class="c1"># final prob layer parameters</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.1</span>        <span class="c1"># likelihood stddev</span>


<span class="c1"># build model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">2</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">],</span> 
            <span class="n">scale</span><span class="o">=</span><span class="n">jitter</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">noise</span> <span class="o">*</span> <span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]))</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">neg_log_likelihood</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1_500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_42_0.png" src="../../../_images/regression_master_class_42_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">yhat</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_43_0.png" src="../../../_images/regression_master_class_43_0.png" />
</div>
</div>
</div>
<div class="section" id="bayesian-neural-network-drop-out">
<h2>Bayesian Neural Network (Drop-Out)<a class="headerlink" href="#bayesian-neural-network-drop-out" title="Permalink to this headline">¶</a></h2>
<p>One nice trick to using a normal NN to train</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># dense layer parameters</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">sigmoid</span>                 <span class="c1"># activation function</span>
<span class="n">w_reg</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(lambda_) # l2 regularization</span>
<span class="n">b_reg</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(lambda_)   # l2 regularizer</span>

<span class="c1"># dropout layer fixed params</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># final prob layer parameters</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.1</span>        <span class="c1"># likelihood stddev</span>


<span class="c1"># build model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">noise</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">neg_log_likelihood</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1_500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>


</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_46_0.png" src="../../../_images/regression_master_class_46_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(500, 100)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_test_samples</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">yhat</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_test_samples</span><span class="p">)]</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">y_mu</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_std</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">y_mu</span><span class="p">,</span> <span class="n">y_std</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_48_0.png" src="../../../_images/regression_master_class_48_0.png" />
</div>
</div>
</div>
<div class="section" id="probabilisitc-nn-dropout-heteroscedastic">
<h2>Probabilisitc NN (Dropout - Heteroscedastic)<a class="headerlink" href="#probabilisitc-nn-dropout-heteroscedastic" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># dense layer parameters</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">sigmoid</span>                 <span class="c1"># activation function</span>
<span class="n">w_reg</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(lambda_) # l2 regularization</span>
<span class="n">b_reg</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(lambda_)   # l2 regularizer</span>

<span class="c1"># dropout layer fixed params</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">jitter</span> <span class="o">=</span> <span class="mf">1e-5</span>

<span class="c1"># final prob layer parameters</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.1</span>        <span class="c1"># likelihood stddev</span>


<span class="c1"># build model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">2</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">],</span> 
            <span class="n">scale</span><span class="o">=</span><span class="n">jitter</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">noise</span> <span class="o">*</span> <span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]))</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">neg_log_likelihood</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2_000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_51_0.png" src="../../../_images/regression_master_class_51_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">yhat</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_52_0.png" src="../../../_images/regression_master_class_52_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_test_samples</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">yhat</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_test_samples</span><span class="p">)]</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">y_mu</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_std</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">y_mu</span><span class="p">,</span> <span class="n">y_std</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_53_0.png" src="../../../_images/regression_master_class_53_0.png" />
</div>
</div>
</div>
<div class="section" id="bayesian-neural-network-vanilla">
<h2>Bayesian Neural Network (Vanilla)<a class="headerlink" href="#bayesian-neural-network-vanilla" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.</span>
<span class="c1"># def posterior_mean_field(kernel_size, bias_size=0, dtype=None):</span>
<span class="c1">#   n = kernel_size + bias_size</span>
<span class="c1">#   c = np.log(np.expm1(1.))</span>
<span class="c1">#   return tf.keras.Sequential([</span>
<span class="c1">#       tfp.layers.VariableLayer(2 * n, dtype=dtype),</span>
<span class="c1">#       tfp.layers.DistributionLambda(lambda t: tfd.Independent(</span>
<span class="c1">#           tfd.Normal(loc=t[..., :n],</span>
<span class="c1">#                      scale=1e-5 + tf.nn.softplus(c + t[..., n:])),</span>
<span class="c1">#           reinterpreted_batch_ndims=1)),</span>
<span class="c1">#   ])</span>

<span class="k">def</span> <span class="nf">posterior</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">bias_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">+</span> <span class="n">bias_size</span>
    <span class="n">posterior_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">VariableLayer</span><span class="p">(</span>
                <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MultivariateNormalTriL</span><span class="o">.</span><span class="n">params_size</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
            <span class="p">),</span>
            <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MultivariateNormalTriL</span><span class="p">(</span><span class="n">n</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">posterior_model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Specify the prior over `keras.layers.Dense` `kernel` and `bias`.</span>
<span class="c1"># def prior_trainable(kernel_size, bias_size=0, dtype=None):</span>
<span class="c1">#   n = kernel_size + bias_size</span>
<span class="c1">#   return tf.keras.Sequential([</span>
<span class="c1">#       tfp.layers.VariableLayer(n, dtype=dtype),</span>
<span class="c1">#       tfp.layers.DistributionLambda(lambda t: tfd.Independent(</span>
<span class="c1">#           tfd.Normal(loc=t, scale=1),</span>
<span class="c1">#           reinterpreted_batch_ndims=1)),</span>
<span class="c1">#   ])</span>

<span class="k">def</span> <span class="nf">prior</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">bias_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">+</span> <span class="n">bias_size</span>
    <span class="n">prior_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
                    <span class="n">loc</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">scale_diag</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">prior_model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># model parameters</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">set_floatx</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">100.</span>
<span class="n">noise</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lambda_</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># weight std dev. prior</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="n">xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="c1"># build model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="c1"># Variational Layer</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseVariational</span><span class="p">(</span>
        <span class="mi">32</span><span class="p">,</span>
        <span class="n">posterior</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">,</span>
        <span class="n">kl_weight</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">n_samples</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span>
    <span class="p">),</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseVariational</span><span class="p">(</span>
        <span class="mi">32</span><span class="p">,</span>
        <span class="n">posterior</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">,</span>
        <span class="n">kl_weight</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">n_samples</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="c1"># Normal Distribution Output</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> 
            <span class="n">scale</span><span class="o">=</span><span class="n">noise</span>
        <span class="p">)</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">neg_log_likelihood</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>


<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_59_0.png" src="../../../_images/regression_master_class_59_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">yhat</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_60_0.png" src="../../../_images/regression_master_class_60_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_test_samples</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">yhat</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_test_samples</span><span class="p">)]</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">y_mu</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_std</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">y_mu</span><span class="p">,</span> <span class="n">y_std</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span>---------------------------------------------------------------------------
ResourceExhaustedError                    Traceback (most recent call last)
&lt;ipython-input-82-82f8a5170b06&gt; in &lt;module&gt;()
      1 n_test_samples = 100
      2 
----&gt; 3 yhat = [model(xtest, training=True) for _ in range(n_test_samples)]
      4 yhat = tf.concat(yhat, axis=1).numpy()
      5 y_mu = yhat.mean(axis=1)

&lt;ipython-input-82-82f8a5170b06&gt; in &lt;listcomp&gt;(.0)
      1 n_test_samples = 100
      2 
----&gt; 3 yhat = [model(xtest, training=True) for _ in range(n_test_samples)]
      4 yhat = tf.concat(yhat, axis=1).numpy()
      5 y_mu = yhat.mean(axis=1)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1028         with autocast_variable.enable_auto_cast_variables(
   1029             self._compute_dtype_object):
-&gt; 1030           outputs = call_fn(inputs, *args, **kwargs)
   1031 
   1032         if self._activity_regularizer:

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py in call(self, inputs, training, mask)
    378       if not self.built:
    379         self._init_graph_network(self.inputs, self.outputs)
--&gt; 380       return super(Sequential, self).call(inputs, training=training, mask=mask)
    381 
    382     outputs = inputs  # handle the corner case where self.layers is empty

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)
    419     &quot;&quot;&quot;
    420     return self._run_internal_graph(
--&gt; 421         inputs, training=training, mask=mask)
    422 
    423   def compute_output_shape(self, input_shape):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)
    554 
    555         args, kwargs = node.map_arguments(tensor_dict)
--&gt; 556         outputs = node.layer(*args, **kwargs)
    557 
    558         # Update tensor_dict.

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1028         with autocast_variable.enable_auto_cast_variables(
   1029             self._compute_dtype_object):
-&gt; 1030           outputs = call_fn(inputs, *args, **kwargs)
   1031 
   1032         if self._activity_regularizer:

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/layers/dense_variational_v2.py in call(self, inputs)
    119     inputs = tf.cast(inputs, dtype, name=&#39;inputs&#39;)
    120 
--&gt; 121     q = self._posterior(inputs)
    122     r = self._prior(inputs)
    123     self.add_loss(self._kl_divergence_fn(q, r))

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1028         with autocast_variable.enable_auto_cast_variables(
   1029             self._compute_dtype_object):
-&gt; 1030           outputs = call_fn(inputs, *args, **kwargs)
   1031 
   1032         if self._activity_regularizer:

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py in call(self, inputs, training, mask)
    378       if not self.built:
    379         self._init_graph_network(self.inputs, self.outputs)
--&gt; 380       return super(Sequential, self).call(inputs, training=training, mask=mask)
    381 
    382     outputs = inputs  # handle the corner case where self.layers is empty

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)
    419     &quot;&quot;&quot;
    420     return self._run_internal_graph(
--&gt; 421         inputs, training=training, mask=mask)
    422 
    423   def compute_output_shape(self, input_shape):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)
    554 
    555         args, kwargs = node.map_arguments(tensor_dict)
--&gt; 556         outputs = node.layer(*args, **kwargs)
    557 
    558         # Update tensor_dict.

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/layers/distribution_layer.py in __call__(self, inputs, *args, **kwargs)
    244     self._enter_dunder_call = True
    245     distribution, _ = super(DistributionLambda, self).__call__(
--&gt; 246         inputs, *args, **kwargs)
    247     self._enter_dunder_call = False
    248     return distribution

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1028         with autocast_variable.enable_auto_cast_variables(
   1029             self._compute_dtype_object):
-&gt; 1030           outputs = call_fn(inputs, *args, **kwargs)
   1031 
   1032         if self._activity_regularizer:

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/layers/distribution_layer.py in call(self, inputs, *args, **kwargs)
    250   def call(self, inputs, *args, **kwargs):
    251     distribution, value = super(DistributionLambda, self).call(
--&gt; 252         inputs, *args, **kwargs)
    253     # We always save the most recently built distribution for variable tracking
    254     # purposes.

/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, mask, training)
    917     with backprop.GradientTape(watch_accessed_variables=True) as tape,\
    918         variable_scope.variable_creator_scope(_variable_creator):
--&gt; 919       result = self.function(inputs, **kwargs)
    920     self._check_variables(created_variables, tape.watched_variables())
    921     return result

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/layers/distribution_layer.py in _fn(*fargs, **fkwargs)
    170     def _fn(*fargs, **fkwargs):
    171       &quot;&quot;&quot;Wraps `make_distribution_fn` to return both dist and concrete value.&quot;&quot;&quot;
--&gt; 172       d = make_distribution_fn(*fargs, **fkwargs)
    173       value_is_seq = isinstance(d.dtype, collections.Sequence)
    174       maybe_composite_convert_to_tensor_fn = (

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/layers/distribution_layer.py in &lt;lambda&gt;(t)
    384     &quot;&quot;&quot;
    385     super(MultivariateNormalTriL, self).__init__(
--&gt; 386         lambda t: MultivariateNormalTriL.new(t, event_size, validate_args),
    387         convert_to_tensor_fn,
    388         **kwargs)

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/layers/distribution_layer.py in new(params, event_size, validate_args, name)
    398       return mvn_tril_lib.MultivariateNormalTriL(
    399           loc=params[..., :event_size],
--&gt; 400           scale_tril=scale_tril(params[..., event_size:]),
    401           validate_args=validate_args)
    402 

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/bijector.py in __call__(self, value, name, **kwargs)
    785       return chain.Chain([self, value], name=name, **kwargs)
    786 
--&gt; 787     return self.forward(value, name=name or &#39;forward&#39;, **kwargs)
    788 
    789   def _forward_event_shape_tensor(self, input_shape):

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/bijector.py in forward(self, x, name, **kwargs)
    945       NotImplementedError: if `_forward` is not implemented.
    946     &quot;&quot;&quot;
--&gt; 947     return self._call_forward(x, name, **kwargs)
    948 
    949   @classmethod

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/bijector.py in _call_forward(self, x, name, **kwargs)
    927       if not self._is_injective:  # No caching for non-injective
    928         return self._forward(x, **kwargs)
--&gt; 929       return self._cache.forward(x, **kwargs)
    930 
    931   def forward(self, x, name=&#39;forward&#39;, **kwargs):

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/cache_util.py in forward(self, x, **kwargs)
    336       The output of the bijector&#39;s `_forward` method, or a cached result.
    337     &quot;&quot;&quot;
--&gt; 338     return self._lookup(x, self._forward_name, self._inverse_name, **kwargs)
    339 
    340   def inverse(self, y, **kwargs):

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/cache_util.py in _lookup(self, input, forward_name, inverse_name, **kwargs)
    495       output = nest.map_structure(
    496           _containerize,
--&gt; 497           self._invoke(input, forward_name, kwargs, attrs))
    498       output_ref = WeakStructRef(
    499           output,

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/cache_util.py in _invoke(self, input, fn_name, kwargs, attributes)
    534   def _invoke(self, input, fn_name, kwargs, attributes):  # pylint: disable=unused-argument
    535     &quot;&quot;&quot;Invokes the wrapped function. Override to customize behavior.&quot;&quot;&quot;
--&gt; 536     return getattr(self.bijector, fn_name)(input, **kwargs)
    537 
    538   def clear(self):

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/composition.py in _forward(self, x, **kwargs)
    515     return self._call_walk_forward(
    516         lambda b, x, **kwargs: b.forward(x, **kwargs),
--&gt; 517         x, **kwargs)
    518 
    519   def _inverse(self, y, **kwargs):

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/composition.py in _call_walk_forward(self, step_fn, *args, **kwargs)
    219 
    220     if len(args) == 1:
--&gt; 221       return self._walk_forward(step_fn, *args, **kwargs)
    222 
    223     # Convert a tuple of structures to a structure of tuples. This

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/chain.py in _walk_forward(self, step_fn, x, **kwargs)
    146     &quot;&quot;&quot;Applies `transform_fn` to `x` sequentially over nested bijectors.&quot;&quot;&quot;
    147     for bij in reversed(self._bijectors):
--&gt; 148       x = step_fn(bij, x, **kwargs.get(bij.name, {}))
    149     return x  # Now `y`
    150 

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/composition.py in &lt;lambda&gt;(b, x, **kwargs)
    514   def _forward(self, x, **kwargs):
    515     return self._call_walk_forward(
--&gt; 516         lambda b, x, **kwargs: b.forward(x, **kwargs),
    517         x, **kwargs)
    518 

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/bijector.py in forward(self, x, name, **kwargs)
    945       NotImplementedError: if `_forward` is not implemented.
    946     &quot;&quot;&quot;
--&gt; 947     return self._call_forward(x, name, **kwargs)
    948 
    949   @classmethod

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/bijector.py in _call_forward(self, x, name, **kwargs)
    927       if not self._is_injective:  # No caching for non-injective
    928         return self._forward(x, **kwargs)
--&gt; 929       return self._cache.forward(x, **kwargs)
    930 
    931   def forward(self, x, name=&#39;forward&#39;, **kwargs):

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/cache_util.py in forward(self, x, **kwargs)
    336       The output of the bijector&#39;s `_forward` method, or a cached result.
    337     &quot;&quot;&quot;
--&gt; 338     return self._lookup(x, self._forward_name, self._inverse_name, **kwargs)
    339 
    340   def inverse(self, y, **kwargs):

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/cache_util.py in _lookup(self, input, forward_name, inverse_name, **kwargs)
    495       output = nest.map_structure(
    496           _containerize,
--&gt; 497           self._invoke(input, forward_name, kwargs, attrs))
    498       output_ref = WeakStructRef(
    499           output,

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/cache_util.py in _invoke(self, input, fn_name, kwargs, attributes)
    534   def _invoke(self, input, fn_name, kwargs, attributes):  # pylint: disable=unused-argument
    535     &quot;&quot;&quot;Invokes the wrapped function. Override to customize behavior.&quot;&quot;&quot;
--&gt; 536     return getattr(self.bijector, fn_name)(input, **kwargs)
    537 
    538   def clear(self):

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/fill_triangular.py in _forward(self, x)
     90 
     91   def _forward(self, x):
---&gt; 92     return fill_triangular(x, upper=self._upper)
     93 
     94   def _inverse(self, y):

/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/math/linalg.py in fill_triangular(x, upper, name)
    930     x = tf.reshape(tf.concat(x_list, axis=-1), new_shape)
    931     x = tf.linalg.band_part(
--&gt; 932         x, num_lower=(0 if upper else -1), num_upper=(-1 if upper else 0))
    933     tensorshape_util.set_shape(x, static_final_shape)
    934     return x

/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py in matrix_band_part(input, num_lower, num_upper, name)
   4664       return _result
   4665     except _core._NotOkStatusException as e:
-&gt; 4666       _ops.raise_from_not_ok_status(e, name)
   4667     except _core._FallbackException:
   4668       pass

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6895   message = e.message + (&quot; name: &quot; + name if name is not None else &quot;&quot;)
   6896   # pylint: disable=protected-access
-&gt; 6897   six.raise_from(core._status_to_exception(e.code, message), None)
   6898   # pylint: enable=protected-access
   6899 

/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)

ResourceExhaustedError: OOM when allocating tensor with shape[4160,4160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatrixBandPart]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="support-vector-regression-rff-approximations">
<h2>Support Vector Regression (RFF Approximations)<a class="headerlink" href="#support-vector-regression-rff-approximations" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers.experimental</span> <span class="kn">import</span> <span class="n">RandomFourierFeatures</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">set_floatx</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="c1"># initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

<span class="c1"># single layer parameters</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">sigmoid</span>           <span class="c1"># activation function</span>
<span class="n">kernel_regularizer</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(0.01) # l2 regularization</span>
<span class="n">bias_regularizer</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(0.01)   # l2 regularizer</span>

<span class="n">n_features</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="s1">&#39;gaussian&#39;</span>
<span class="n">scale</span><span class="o">=</span><span class="kc">None</span> <span class="c1"># 0.1</span>
<span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">RandomFourierFeatures</span><span class="p">(</span>        
        <span class="n">output_dim</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(),</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;rff&#39;</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># configure model for mean-squared error regression</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;mean_squared_error&#39;</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mean_absolute_error&#39;</span><span class="p">]</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span>
<span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_65_0.png" src="../../../_images/regression_master_class_65_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_66_0.png" src="../../../_images/regression_master_class_66_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">set_floatx</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="c1"># initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

<span class="c1"># single layer parameters</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">sigmoid</span>           <span class="c1"># activation function</span>
<span class="n">kernel_regularizer</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(0.01) # l2 regularization</span>
<span class="n">bias_regularizer</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(0.01)   # l2 regularizer</span>

<span class="n">n_features</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="s1">&#39;gaussian&#39;</span>
<span class="n">scale</span><span class="o">=</span><span class="kc">None</span> <span class="c1"># 0.1</span>
<span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">RandomFourierFeatures</span><span class="p">(</span>        
        <span class="n">output_dim</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(),</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;rffl0&#39;</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">RandomFourierFeatures</span><span class="p">(</span>        
        <span class="n">output_dim</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(),</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;rffl1&#39;</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">64</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">RandomFourierFeatures</span><span class="p">(</span>        
        <span class="n">output_dim</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(),</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;rffl2&#39;</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># configure model for mean-squared error regression</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;mean_squared_error&#39;</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mean_absolute_error&#39;</span><span class="p">]</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span>
<span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_68_0.png" src="../../../_images/regression_master_class_68_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_69_0.png" src="../../../_images/regression_master_class_69_0.png" />
</div>
</div>
</div>
<div class="section" id="svr-deep">
<h2>SVR (Deep)<a class="headerlink" href="#svr-deep" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">set_floatx</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="c1"># initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

<span class="c1"># single layer parameters</span>
<span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">sigmoid</span>           <span class="c1"># activation function</span>
<span class="n">kernel_regularizer</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(0.01) # l2 regularization</span>
<span class="n">bias_regularizer</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.keras.regularizers.l2(0.01)   # l2 regularizer</span>

<span class="n">n_features</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="s1">&#39;gaussian&#39;</span>
<span class="n">scale</span><span class="o">=</span><span class="kc">None</span> <span class="c1"># 0.1</span>
<span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">RandomFourierFeatures</span><span class="p">(</span>        
        <span class="n">output_dim</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(),</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;rff_l1&#39;</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">16</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">RandomFourierFeatures</span><span class="p">(</span>        
        <span class="n">output_dim</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">RandomNormal</span><span class="p">(),</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;rff_l2&#39;</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="mi">2</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="n">bias_regularizer</span><span class="o">=</span><span class="n">bias_regularizer</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">],</span> 
            <span class="n">scale</span><span class="o">=</span><span class="n">jitter</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">noise</span> <span class="o">*</span> <span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]))</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">neg_log_likelihood</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1_500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_72_0.png" src="../../../_images/regression_master_class_72_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>


<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">yhat</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_73_0.png" src="../../../_images/regression_master_class_73_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_test_samples</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">yhat</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_test_samples</span><span class="p">)]</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">y_mu</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_std</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">y_mu</span><span class="p">,</span> <span class="n">y_std</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_74_0.png" src="../../../_images/regression_master_class_74_0.png" />
</div>
</div>
</div>
<div class="section" id="gaussian-processes-rff-approx">
<h2>Gaussian Processes (RFF Approx.)<a class="headerlink" href="#gaussian-processes-rff-approx" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># For numeric stability, set the default floating-point dtype to float64</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">set_floatx</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="c1"># model parameters</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">100.</span>
<span class="n">noise</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lambda_</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># weight std dev. prior</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="n">xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="n">n_features</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">scale</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.Variable(.5)</span>
<span class="n">kernel_initializer</span> <span class="o">=</span> <span class="s1">&#39;gaussian&#39;</span>
<span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="c1"># RFF Layer</span>
    <span class="n">RandomFourierFeatures</span><span class="p">(</span>        
        <span class="n">output_dim</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">scale</span> <span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;rff&#39;</span>
    <span class="p">),</span>
    <span class="c1"># Variational Layer</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseVariational</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="n">posterior</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">,</span>
        <span class="n">kl_weight</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">n_samples</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="c1"># Normal Distribution Output</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> 
            <span class="n">scale</span><span class="o">=</span><span class="n">noise</span>
        <span class="p">)</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>


<span class="n">loss</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">,</span> <span class="n">rv_y</span><span class="p">:</span> <span class="n">rv_y</span><span class="o">.</span><span class="n">variational_loss</span><span class="p">(</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">kl_weight</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1_500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_77_0.png" src="../../../_images/regression_master_class_77_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">yhat</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_78_0.png" src="../../../_images/regression_master_class_78_0.png" />
</div>
</div>
<div class="section" id="heteroscedastic">
<h3>Heteroscedastic<a class="headerlink" href="#heteroscedastic" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># For numeric stability, set the default floating-point dtype to float64</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">set_floatx</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="c1"># model parameters</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">1000.</span>
<span class="n">noise</span> <span class="o">=</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">lambda_</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># weight std dev. prior</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="n">xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="n">n_features</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">scale</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tf.Variable(.5)</span>
<span class="n">kernel_initializer</span> <span class="o">=</span> <span class="s1">&#39;gaussian&#39;</span>
<span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="c1"># RFF Layer</span>
    <span class="n">RandomFourierFeatures</span><span class="p">(</span>        
        <span class="n">output_dim</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
        <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">,</span>
        <span class="n">scale</span> <span class="o">=</span><span class="n">scale</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;rff&#39;</span>
    <span class="p">),</span>
    <span class="c1"># Variational Layer</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseVariational</span><span class="p">(</span>
        <span class="mi">2</span><span class="p">,</span>
        <span class="n">posterior</span><span class="p">,</span>
        <span class="n">prior</span><span class="p">,</span>
        <span class="n">kl_weight</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">n_samples</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="c1"># Normal Distribution Output</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">],</span> 
            <span class="n">scale</span><span class="o">=</span><span class="n">jitter</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">noise</span> <span class="o">*</span> <span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]))</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>


<span class="n">loss</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">,</span> <span class="n">rv_y</span><span class="p">:</span> <span class="n">rv_y</span><span class="o">.</span><span class="n">variational_loss</span><span class="p">(</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">kl_weight</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2_000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_81_0.png" src="../../../_images/regression_master_class_81_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">yhat</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">yhat</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/regression_master_class_82_0.png" src="../../../_images/regression_master_class_82_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="deep-gaussian-processes-rff">
<h2>Deep Gaussian Processes (RFF)<a class="headerlink" href="#deep-gaussian-processes-rff" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># model parameters</span>
<span class="n">lambda_</span> <span class="o">=</span> <span class="mf">100.</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">.5</span> <span class="c1">#( 1 / lambda_) ** (1/2)  # weight std dev. prior</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="n">xtrain</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="n">n_features</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># initialize model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="c1"># RFF Layer</span>
    <span class="n">RandomFourierFeatures</span><span class="p">(</span>        
        <span class="n">output_dim</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span>
<span class="c1">#         kernel_initializer=tf.initializers.RandomNormal(),</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;rff&#39;</span>
    <span class="p">),</span>
    <span class="c1"># Variational Layer</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DenseVariational</span><span class="p">(</span>
        <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">posterior_mean_field</span><span class="p">,</span>
        <span class="n">prior_trainable</span><span class="p">,</span>
<span class="c1">#         kl_weight=1/n_samples,</span>
    <span class="p">),</span>
    <span class="c1"># Normal Distribution Output</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">DistributionLambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
            <span class="n">loc</span><span class="o">=</span><span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span><span class="mi">1</span><span class="p">:],</span> 
            <span class="n">scale</span><span class="o">=</span><span class="mf">1e-3</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="mf">0.01</span> <span class="o">*</span> <span class="n">t</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:])</span>
        <span class="p">)</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># do inference</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">neg_log_likelihood</span>


<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">neg_log_likelihood</span>
<span class="p">)</span>

<span class="n">es</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">es</span><span class="p">])</span>

<span class="n">plot_loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train on 1000 samples
Epoch 1/500
WARNING:tensorflow:Gradients do not exist for variables [&#39;sequential_69/rff/random_features_scale:0&#39;] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables [&#39;sequential_69/rff/random_features_scale:0&#39;] when minimizing the loss.
1000/1000 [==============================] - 2s 2ms/sample - loss: 36.8524
Epoch 2/500
1000/1000 [==============================] - 0s 141us/sample - loss: 29.8914
Epoch 3/500
1000/1000 [==============================] - 0s 135us/sample - loss: 28.3100
Epoch 4/500
1000/1000 [==============================] - 0s 132us/sample - loss: 27.5571
Epoch 5/500
1000/1000 [==============================] - 0s 137us/sample - loss: 27.0303
Epoch 6/500
1000/1000 [==============================] - 0s 150us/sample - loss: 25.3025
Epoch 7/500
1000/1000 [==============================] - 0s 127us/sample - loss: 25.1574
Epoch 8/500
1000/1000 [==============================] - 0s 132us/sample - loss: 29.5244
Epoch 9/500
1000/1000 [==============================] - 0s 137us/sample - loss: 23.2684
Epoch 10/500
1000/1000 [==============================] - 0s 136us/sample - loss: 27.7809
Epoch 11/500
1000/1000 [==============================] - 0s 143us/sample - loss: 27.6934
Epoch 12/500
1000/1000 [==============================] - 0s 138us/sample - loss: 27.3766
Epoch 13/500
1000/1000 [==============================] - 0s 145us/sample - loss: 25.4614
Epoch 14/500
1000/1000 [==============================] - 0s 152us/sample - loss: 27.6924
Epoch 15/500
1000/1000 [==============================] - 0s 129us/sample - loss: 27.9307
Epoch 16/500
1000/1000 [==============================] - 0s 149us/sample - loss: 24.4741
Epoch 17/500
1000/1000 [==============================] - 0s 130us/sample - loss: 28.6062
Epoch 18/500
1000/1000 [==============================] - 0s 127us/sample - loss: 25.5995
Epoch 19/500
1000/1000 [==============================] - 0s 132us/sample - loss: 27.5728
Epoch 20/500
1000/1000 [==============================] - 0s 127us/sample - loss: 28.7692
Epoch 21/500
1000/1000 [==============================] - 0s 133us/sample - loss: 27.7140
Epoch 22/500
1000/1000 [==============================] - 0s 125us/sample - loss: 27.2762
Epoch 23/500
1000/1000 [==============================] - 0s 135us/sample - loss: 27.9165
Epoch 24/500
1000/1000 [==============================] - 0s 147us/sample - loss: 27.7493
Epoch 25/500
1000/1000 [==============================] - 0s 132us/sample - loss: 26.4487
Epoch 26/500
1000/1000 [==============================] - 0s 156us/sample - loss: 26.4019
Epoch 27/500
1000/1000 [==============================] - 0s 149us/sample - loss: 29.5797
Epoch 28/500
1000/1000 [==============================] - 0s 140us/sample - loss: 26.3094
Epoch 29/500
1000/1000 [==============================] - 0s 149us/sample - loss: 26.5446
</pre></div>
</div>
<img alt="../../../_images/regression_master_class_84_1.png" src="../../../_images/regression_master_class_84_1.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/tutorials/regression_masterclass"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By J. Emmanuel Johnson<br/>
    
        &copy; Copyright 2020.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>