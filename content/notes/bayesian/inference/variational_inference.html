
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Variational Inference &#8212; Research Notebook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js"></script>
    <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/notes/bayesian/inference/variational_inference';</script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Conditional Variational Inference" href="cond_vi.html" />
    <link rel="prev" title="Inference Schemes" href="../inference.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../../../../intro.html">

  
  
  
  
  
  
  

  
    <img src="../../../../_static/book_v2.jpeg" class="logo__image only-light" alt="Logo image">
    <img src="../../../../_static/book_v2.jpeg" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../resources/python/overview.html">
                        Python
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../tutorials/jax_journey/overview.html">
                        My JAX Journey
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../tutorials/remote/overview.html">
                        Remote Computing
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../gmt/overview.html">
                        GMT of Learning
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../concepts/uncertainty.html">
                        Modeling Uncertainty
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../overview.html">
                        Bayesian
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../concepts/overview.html">
                        Sleeper Concepts
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../kernels/overview.html">
                        Kernel Methods
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../gps/intro.html">
                        Gaussian Processes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../info_theory/similarity.html">
                        Similarity
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../info_theory/overview.html">
                        Information Theory
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../normalizing_flows/overview.html">
                        Normalizing Flows
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../inr/overview.html">
                        Implicit Neural Representations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../data_assimilation/overview.html">
                        Data Assimilation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../misc/overview.html">
                        Miscellaneous Notes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../cheatsheets/bash.html">
                        Bash
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../cheatsheets/cli.html">
                        Command Line
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../cheatsheets/python.html">
                        Python
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../resources/python/overview.html">
                        Python
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../tutorials/jax_journey/overview.html">
                        My JAX Journey
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../tutorials/remote/overview.html">
                        Remote Computing
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../gmt/overview.html">
                        GMT of Learning
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../concepts/uncertainty.html">
                        Modeling Uncertainty
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../overview.html">
                        Bayesian
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../concepts/overview.html">
                        Sleeper Concepts
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../kernels/overview.html">
                        Kernel Methods
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../gps/intro.html">
                        Gaussian Processes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../info_theory/similarity.html">
                        Similarity
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../info_theory/overview.html">
                        Information Theory
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../normalizing_flows/overview.html">
                        Normalizing Flows
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../inr/overview.html">
                        Implicit Neural Representations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../data_assimilation/overview.html">
                        Data Assimilation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../misc/overview.html">
                        Miscellaneous Notes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../cheatsheets/bash.html">
                        Bash
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../cheatsheets/cli.html">
                        Command Line
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../cheatsheets/python.html">
                        Python
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="../../../../intro.html">

  
  
  
  
  
  
  

  
    <img src="../../../../_static/book_v2.jpeg" class="logo__image only-light" alt="Logo image">
    <img src="../../../../_static/book_v2.jpeg" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../intro.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../resources/python/overview.html">Python</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../resources/python/ides.html">Integraded Development Environment (IDE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../resources/python/stack.html">Standard Python Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../resources/python/earthsci_stack.html">Earth Science Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../resources/python/dl_stack.html">Deep Learning Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../resources/python/scale_stack.html">Scaling Stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../resources/python/good_code.html">Good Code</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../tutorials/jax_journey/overview.html">My JAX Journey</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/jax_journey/ecosystem.html">Ecosystem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/jax_journey/vmap.html">vmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/jax_journey/jit.html">Jit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/jax_journey/classes.html">Classes</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../tutorials/jax_journey/algorithms/overview.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/jax_journey/algorithms/bisection.html">Bisection search</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/jax_journey/algorithms/kernel_derivatives.html">Kernel Derivatives</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../tutorials/jax_journey/gfs_with_jax.html">Gaussianization Flows</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../tutorials/remote/overview.html">Remote Computing</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/remote/ssh.html">SSH Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/remote/conda.html">Conda 4 Remote Servers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../tutorials/remote/jlab.html">Jupyter Lab 4 Remote Servers</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../gmt/overview.html">GMT of Learning</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../gmt/hierarchical_rep.html">Hierarchical Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gmt/functa.html">Functa</a></li>








<li class="toctree-l2"><a class="reference internal" href="../../gmt/discretization.html">Discretization</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../gmt/learning.html">Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../concepts/uncertainty.html">Modeling Uncertainty</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../overview.html">Bayesian</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../intro.html">Language of Uncertainty</a></li>
<li class="toctree-l2"><a class="reference internal" href="../models.html">Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../inference.html">Inference Schemes</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Variational Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="cond_vi.html">Conditional Variational Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../confidence_intervals.html">Confidence Intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../regression.html">Regression</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../concepts/overview.html">Sleeper Concepts</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/gaussian.html">Gaussian Distributions</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../concepts/change_of_variables.html">Change of Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/identity_trick.html">Identity Trick</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/inverse_function.html">Inverse Function Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/jensens.html">Jensens Inequality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/lin_alg.html">Linear Algebra Tricks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../kernels/overview.html">Kernel Methods</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../kernels/kernel_derivatives.html">Kernel Derivatives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kernels/rv.html">RV Coefficient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kernels/congruence_coeff.html">Congruence Coefficient</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kernels/hsic.html">HSIC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kernels/mmd.html">Maximum Mean Discrepancy (MMD)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../gps/intro.html">Gaussian Processes</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../gps/gps.html">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gps/literature.html">Literature Review</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gps/cg.html">Conjugate Gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gps/sgps.html">Sparse Gaussian Processes</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../gps/algorithms.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../gps/gpr_code.html">GP from Scratch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../gps/sgp_code.html">Sparse GP From Scratch</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../egps/overview.html">Input Uncertainty in GPs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../info_theory/similarity.html">Similarity</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../info_theory/overview.html">Information Theory</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../info_theory/measures.html">Measures</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../info_theory/information.html">Information Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../info_theory/entropy.html">Entropy &amp; Relative Entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../info_theory/mutual_info.html">Mutual Information and Total Correlation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../info_theory/estimators.html">Information Theory Measures</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../info_theory/classic.html">Classic Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../info_theory/histogram.html">Entropy Estimator - Histogram</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../info_theory/experiments/rbig_sample_consistency.html">Experiment - RBIG Sample Consistency</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../normalizing_flows/overview.html">Normalizing Flows</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../normalizing_flows/linear.html">Linear Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normalizing_flows/coupling_layers.html">Coupling Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normalizing_flows/conditional.html">Conditional Normalizing Flows</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normalizing_flows/multiscale.html">Multiscale</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normalizing_flows/inverse.html">Minimization Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normalizing_flows/losses.html">Losses</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../normalizing_flows/lecture_1_ig.html">Lecture I - Iterative Gaussianization</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../normalizing_flows/1.0_univariate_gauss.html">1.1 - Univariate Gaussianization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../normalizing_flows/1.1_marginal_gauss.html">1.2 - Marginal Gaussianization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../normalizing_flows/1.2_gaussianization.html">1.2 - Iterative Gaussianization</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../normalizing_flows/lecture_2_gf.html">Lecture II - Gaussianization Flows</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../normalizing_flows/lecture_3_gfs_pt1_mg.html">Parameterized Marginal Gaussianization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../normalizing_flows/lecture_3_gfs_pt2_rot.html">Parameterized Rotations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../normalizing_flows/lecture_3_gfs_pt3_plane.html">Example - 2D Plane</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../inr/overview.html">Implicit Neural Representations</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../inr/formulation.html">Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../inr/literature_review.html">Literature Review</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../inr/pinns.html">Physics-Informed Loss</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../inr/qg.html">QG PDE</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../data_assimilation/overview.html">Data Assimilation</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../data_assimilation/dynamical_sys.html">Dynamical Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../data_assimilation/oi.html">Optimal Interpolation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../data_assimilation/interp.html">Interpolation Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../data_assimilation/emu.html">Emulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../data_assimilation/inv_problems.html">Inverse Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../data_assimilation/projects.html">Projects</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../data_assimilation/algorithms.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../data_assimilation/markov_models.html">Markov Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../data_assimilation/gauss_markov.html">Gauss-Markov Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../data_assimilation/kf.html">Kalman Filter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../data_assimilation/nkf.html">Normalizing Kalman Filter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../data_assimilation/enskf.html">Ensemble Kalman Filter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../data_assimilation/dmm.html">Deep Markov Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../data_assimilation/4dvarnet.html">4DVarNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../data_assimilation/markov_gp.html">Markovian Gaussian Processes</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../data_assimilation/nbs/notebooks.html">Notebooks</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../misc/overview.html">Miscellaneous Notes</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../misc/generative_models.html">Generative Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../misc/diffusion_models.html">Diffusion Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../misc/fixed_point.html">Fixed-Point Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../misc/bilevel_opt.html">Bi-Level Optimization</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Cheat Sheets</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../cheatsheets/bash.html">Bash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cheatsheets/cli.html">Command Line</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cheatsheets/python.html">Python</a></li>
</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-repository-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://github.com/jejjohnson/research_notebook" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">repository</span>
</a>
</a>
      
      <li><a href="https://github.com/jejjohnson/research_notebook/issues/new?title=Issue%20on%20page%20%2Fcontent/notes/bayesian/inference/variational_inference.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">open issue</span>
</a>
</a>
      
  </ul>
</div>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="../../../../_sources/content/notes/bayesian/inference/variational_inference.md" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Variational Inference</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivations">
   Motivations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pros-and-cons">
   Pros and Cons
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-variational-inference">
     Why Variational Inference?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-not-variational-inference">
     Why Not Variational Inference?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variational-distribution">
   Variational Distribution
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-q-z">
     Simple, $q(z)$
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mixture-distribution">
     Mixture Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bijective-transformation-flow">
     Bijective Transformation (Flow)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-transformation-encoder-amortization">
     Stochastic Transformation (Encoder, Amortization)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elbo-encoder-derivation">
   ELBO (Encoder) - Derivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reconstruction-loss">
   Reconstruction Loss
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#volume-correction">
   Volume Correction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variational-free-energy-vfe">
   Variational Free Energy (VFE)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elbo-non-encoder-derivation">
   ELBO (Non-Encoder) - Derivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elbo-derivation-old">
   ELBO - Derivation (Old)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comments-on-q-x">
     Comments on $q(x)$
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resources">
   Resources
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#from-scratch">
     From Scratch
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Variational Inference
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <section id="variational-inference">
<h1>Variational Inference<a class="headerlink" href="#variational-inference" title="Permalink to this heading">#</a></h1>
<hr class="docutils" />
<section id="motivations">
<h2>Motivations<a class="headerlink" href="#motivations" title="Permalink to this heading">#</a></h2>
<p>Variational inference is the most scalable inference method the machine learning community has (as of 2019).</p>
<p>Ultimately, we are interested in approximating the marginal distribution of our data, $\mathcal{X}$.</p>
<p>$$
\mathbf{x} \in \mathcal{X}\sim \mathbb{P}_*
$$</p>
<p>We write some sort of approximation of the <em>true</em> (or best) underlying distribution via some parameterized form like so</p>
<p>$$
p_*(\mathbf{x}) \approx p_{\boldsymbol \theta}(\mathbf{x}).
$$</p>
<p>However, in order to obtain this, we need to assume some latent variable, $\mathbf{z}$, plays a role in estimating the underlying density. In the simplest form, we assume a generative model for the joint distribution can be written as</p>
<p>$$
p_\theta(z, x) = p(x|z)p(z)
$$</p>
<p>When fitting a model, we are interested in maximizing the marginal likelihood</p>
<p>$$
p_\theta(x) = \int p_\theta(x|z)p_\theta(z)dz
$$</p>
<p>However, this quantity is intractable because we have a non-linear function thats within an integral.  So we use an variational distribution, $q_\phi(z|x)$, (sometimes called an encoder).</p>
<p>$$
\log p_\theta(x) = \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log p_\theta(x) \right]
$$</p>
</section>
<hr class="docutils" />
<section id="pros-and-cons">
<h2>Pros and Cons<a class="headerlink" href="#pros-and-cons" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>These were taken from the slides of Shakir Mohamed (prob methods MLSS 2019)</p>
</div></blockquote>
<section id="why-variational-inference">
<h3>Why Variational Inference?<a class="headerlink" href="#why-variational-inference" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Applicable to all probabilistic models</p></li>
<li><p>Transforms a problem from integration to one of optimization</p></li>
<li><p>Convergence assessment</p></li>
<li><p>Principled and Scalable approach to model selection</p></li>
<li><p>Compact representation of posterior distribution</p></li>
<li><p>Faster to converge</p></li>
<li><p>Numerically stable</p></li>
<li><p>Modern Computing Architectures (GPUs)</p></li>
<li><p><strong>There is a LOT of research already</strong>!</p></li>
</ul>
</section>
<section id="why-not-variational-inference">
<h3>Why Not Variational Inference?<a class="headerlink" href="#why-not-variational-inference" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Approximate posterior only</p></li>
<li><p>Difficulty in optimization due to local minima</p></li>
<li><p>Under-estimates the variance of posterior</p></li>
<li><p>Limited theory and guarantees for variational mehtods</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="variational-distribution">
<h2>Variational Distribution<a class="headerlink" href="#variational-distribution" title="Permalink to this heading">#</a></h2>
<p>We defined the variationa distribution as $q(z|x)$. However, we have many types of variational distributions we can impose. For example, we have some of the following:</p>
<ul class="simple">
<li><p><em>Gaussian</em>, $q(z)$</p></li>
<li><p><em>Mixture Distribution</em>, $\sum_{k}^{K}\pi_k \mathbb{P}$</p></li>
<li><p><em>Bijective Transform</em> (Flow), $q(z|\tilde{z})$</p></li>
<li><p><em>Stochastic Transform</em> (Encoder, Amortized), $q(z|x)$</p></li>
<li><p><em>Conditional</em>, $q(z|x,y)$</p></li>
</ul>
<p>Below we will go through each of them and outline some potential strengths and weaknesses of each of the methods.</p>
<hr class="docutils" />
<section id="simple-q-z">
<h3>Simple, $q(z)$<a class="headerlink" href="#simple-q-z" title="Permalink to this heading">#</a></h3>
<p>This is the simplest case where we often assume a very simple distribution can describe the distribution.</p>
<p>$$
q(z) = \mathcal{N}(z|\boldsymbol{\mu_\theta},\boldsymbol{\Sigma_\theta})
$$</p>
<p>If we take each of the Gaussian parameters as full matrices, we end up with:</p>
<p>$$
\boldsymbol{\mu_\theta}:=\boldsymbol{\mu} \in \mathbb{R}^D, \hspace{5mm} \boldsymbol{\Sigma_\theta}:=\boldsymbol{\Sigma} \in \mathbb{R}^{D\times D};
$$</p>
<p>For very high dimensional problems, these are a lot of parameters to learn. Now, we can have various simplifications (or complications) with this. For example, we can simplify the mean, $\boldsymbol{\mu}$, to be zero. The majority of the changes will come from the covariance. Here are a few modifications.</p>
<p><strong>Full Covariance</strong></p>
<p>This is when we parameterize our covariance to be a full covariance matrix. $\boldsymbol{\Sigma_\theta} := \boldsymbol{\Sigma}$. This is easily the most expensive and the most complex of the Gaussian types.</p>
<p><strong>Lower Cholesky</strong></p>
<p>We can also parameterize our covariance to be a lower triangular matrix, i.e. $\boldsymbol{\Sigma_\theta} := \mathbf{L}$, that satisfies the cholesky decomposition, i.e. $\mathbf{LL}^\top = \boldsymbol{\Sigma}$. This reduces the number of parameters of the full covariance by a factor. It also has desireable properties when parameterizing covariance matrices that are computationally attractive, e.g. positive definite.</p>
<p><strong>Diagonal Covariance</strong></p>
<p>We can parameterize our covariance matrix to be a diagonal, i.e. $\boldsymbol{\Sigma_\theta} := \text{diag}(\boldsymbol{\sigma})$. This is a very drastic simplification of our model which limits the expressivity. However, there are immense computational benefits For example, a d-dimensional multivariate Gaussian rv with a mean and a diagonal covariance is the same as the product of $d$ univeriate Gaussians.</p>
<p>$$
q(z) = \mathcal{N}\left(\boldsymbol{\mu_\theta}, \text{diag}(\boldsymbol{\sigma_\theta})\right) = \prod_{d}^D \mathcal{N}(\mu_d, \sigma_d )
$$</p>
<p>This is also known as the <strong>mean-field</strong> approximation and it is a very common starting point in practical VI algorithms.</p>
<p><strong>Low Rank Multivariate Normal</strong></p>
<p>Another parameterization is a low rank matrix with a diagonal matrix, i.e. $\boldsymbol{\Sigma_\theta} := \mathbf{W}\mathbf{W}^\top + \mathbf{D}$ where $\mathbf{W} \in \mathbb{R}^{D\times d}, \mathbf{D} \in \mathbb{R}^{D\times D}$. We assume that our parameterization can be low dimensional which might be appropriate for some applications. This allows for some computationally efficient schemes that make use of the Woodbury Identity and the matrix determinant lemma.</p>
<p><strong>Orthogonal Decoupled</strong></p>
<p>One interesting approach is to map the variational parameters via a subspace parameterization. For exaple, we can define the mean and variance like so:</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu_\theta} &amp;= \boldsymbol{\Psi}<em>{\boldsymbol{\mu}} \mathbf{a} \
\boldsymbol{\Sigma</em>\theta} &amp;= \boldsymbol{\Psi}<em>{\boldsymbol{\Sigma}} \mathbf{A} \boldsymbol{\Psi}</em>{\boldsymbol{\Sigma}}^\top + \mathbf{I}
\end{aligned}
$$</p>
<p>This is a bit of a spin off of the Low-Rank Multivariate Normal approach. However, this method takes care and provides a low-rank method for both the mean and the covariance. They argue that we would be able to put more computational effort in the mean function (computationally easy) and less computational effort for the covariance (computationally intensive).</p>
<p><em>Source</em>: <a class="reference internal" href="../inference.html"><span class="doc std std-doc">Orthogonally Decoupled Variational Gaussian Process</span></a> - Salimbeni et al (2018)</p>
<p><strong>Delta Distribution</strong></p>
<p>This is probably the distribution with the least amount of parameters. We set the covariance matrix to $0$, i.e. $\boldsymbol{\Sigma_\theta}:=\mathbf{0}$, and we let all of the mass rest on mean points, $\boldsymbol{\mu_\theta}:=\boldsymbol{\mu}=\mathbf{u}$.</p>
<p>$$
q(z) = \delta(z - \hat{z})
$$</p>
</section>
<hr class="docutils" />
<section id="mixture-distribution">
<h3>Mixture Distribution<a class="headerlink" href="#mixture-distribution" title="Permalink to this heading">#</a></h3>
<p>The principal behind this is that a simple base distribution, e.g. Gaussian, is not expressive enough. However, a mixture of simple distributions, e.g. Mixture of Gaussians, will be more expressive. So the idea is to choose simple base distribution and replicate it $k$ times. Then, we then do a normalized weighted summation of each component to produce our mixture distribution.</p>
<p>$$
q(z) = \sum_{k}^K\pi_k \mathbb{P}_k
$$</p>
<p>where $0 \leq \pi_k \leq 1$ and $\sum_{k}^K\pi_k=1$. For example, we can use a Gaussian distribution</p>
<p>$$
p_\theta(z) = \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})
$$</p>
<p>where $\theta = {\pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k }_k^K$ are potentially learned parameters.. And the mixture distribution will be</p>
<p>$$
q_{\boldsymbol \theta}(\mathbf{z}) = \sum_{k}^K \pi_k \mathcal{N}(\mathbf{z} |\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
$$</p>
<p>Again, we are free to parameterize the covariances as flexible or restrictive as possible. For example we can have full, cholesky, low-rank or diagonal. In addition we can <em>tie</em> some of the parameters together. For example, we can have the same covariance matrix for every $k^\text{th}$ component, e.g. $\boldsymbol{\Sigma}_k=\boldsymbol{\Sigma}$. Even for VAEs, this becomes a prior distribution which has noticable improvement over the standard Gaussian prior.</p>
<p><strong>Note</strong>: in principal, a mixture distribution is very powerful and has the ability to estimate any distribution, e.g. univariate with enough components. However, like with most problems, the issue is estimating the best parameters just from observations.</p>
</section>
<hr class="docutils" />
<section id="bijective-transformation-flow">
<h3>Bijective Transformation (Flow)<a class="headerlink" href="#bijective-transformation-flow" title="Permalink to this heading">#</a></h3>
<p>It may be that the variational distribution, $q$, is not sufficiently expressive enough even with the complex Gaussian parameterization and/or the mixture distribution. So another option is to use a bijective transformation to map the data from a simple base distribution, e.g. Gaussian, to a more complex distribution for our variational parameter, $z$.</p>
<p>$$
\mathbf{z} = \boldsymbol{T_\phi}(\tilde{\mathbf{z}})
$$</p>
<p>We hope that the resulting variational distribution, $q(z)$, acts a better approximation to the data. Because our transformation is bijective, we can</p>
<p>variational parameter, $z$, to a simple base distribution st we ha
$$
q(z) = p_e(\tilde{z})|\boldsymbol{\nabla}<em>\mathbf{z}\boldsymbol{T</em>\phi}^{-1}(\mathbf{z})|
$$</p>
<p>where $|\boldsymbol{\nabla}<em>\mathbf{z} \cdot|$ is the determinant Jacobian of the transformation, $\boldsymbol{T</em>\phi}$.</p>
</section>
<hr class="docutils" />
<section id="stochastic-transformation-encoder-amortization">
<h3>Stochastic Transformation (Encoder, Amortization)<a class="headerlink" href="#stochastic-transformation-encoder-amortization" title="Permalink to this heading">#</a></h3>
<p>Another type of transformation is a stochastic transformation. This is given by $q(z|x)$. In this case, we assume some non-linear. For example, a Gaussian distribution with a parameterized mean and variance via neural networks</p>
<p>$$
q(\mathbf{z}|\mathbf{x}) = \mathcal{N}\left(\boldsymbol{\mu_\phi}(\mathbf{x}), \boldsymbol{\sigma_\phi}(\mathbf{x})\right)
$$</p>
<p>or more appropriately</p>
<p>$$
q(\mathbf{z}|\mathbf{x}) = \mathcal{N}\left(\boldsymbol{\mu}, \text{diag}(\exp (\boldsymbol{\sigma}^2_{\log}) )\right), \hspace{4mm} (\boldsymbol{\mu}, \boldsymbol{\sigma}^2_{\log}) = \text{NN}_{\boldsymbol \theta}(\mathbf{x})
$$</p>
<p>It can be very difficult to try and have a variational distribution that is complicated enough to cover the whole posterior. So often, we use a variational distribution that is conditioned on the observations, i.e. $q(z|x)$. This is known as an encoder because we encode the observations to obey th</p>
</section>
</section>
<hr class="docutils" />
<section id="elbo-encoder-derivation">
<h2>ELBO (Encoder) - Derivation<a class="headerlink" href="#elbo-encoder-derivation" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>This derivation comes from the book by Probabilistic Machine Learning by Kevin Murphy. I find it to be a much better and intuitive derivation.</p>
</div></blockquote>
<p><strong>Note</strong>: I put the <em>encoder</em> tag in the title. This is because there are other ELBOs that have different purposes, for example, variational distributions without an encoder and also an encoder for conditional likelihoods. In this first one, we will like at the ELBO derivation</p>
<p>As mentioned above, we are interested in expanding the expectation of the marginal likelihood wrt the encoder variational distribution</p>
<p>$$
\log p_\theta(x) = \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log p_\theta(x) \right]
$$</p>
<p>We will do a bit of mathematical manipulation to expand this expectation. Firstly, we will start with Bayes rule:</p>
<p>$$
p_\theta(x) = \frac{p_\theta(z,x)}{p_\theta(z|x)}
$$</p>
<p>Plugging this into our expectation gives us:</p>
<p>$$
\log p_\theta(x) = \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log \frac{p_\theta(z,x)}{p_\theta(z|x)} \right]
$$</p>
<p>Now we will do the identity trick (multiply by $\frac{1}{1}$ :) ) within the log term to incorporate the variational distribution, $q_\phi$.</p>
<p>$$
\log p_\theta(x) = \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log \frac{p_\theta(z,x)q_\phi(z|x)}{p_\theta(z|x)q_\phi(z|x)} \right] = \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log \frac{p_\theta(z,x)q_\phi(z|x)}{q_\phi(z|x)p_\theta(z|x)} \right]
$$</p>
<p>Using the log rules, we can split this fraction into two fractions;</p>
<p>$$
\log p_\theta(x) =  \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log \frac{p_\theta(z,x)}{q_\phi(z|x)} + \log \frac{q_\phi(z|x)}{p_\theta(z|x)} \right]
$$</p>
<p>Now, we can expand the expectation term across the additive operator</p>
<p>$$
\log p_\theta(x) =  \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log \frac{p_\theta(z,x)}{q_\phi(z|x)} \right] + \mathbb{E}<em>{q</em>\phi(z|x)}\left[\log \frac{q_\phi(z|x)}{p_\theta(z|x)} \right]
$$</p>
<p>Here, we notice that the second term is actually the Kullback-Leibler divergence term.</p>
<p>$$
\text{D}_{\text{KL}} [Q||P] = \mathbb{E}_Q\left[\log \frac{Q}{P} \right] = - \mathbb{E}_Q\left[\log \frac{P}{Q} \right]
$$</p>
<p>so we can replace this with the more compact form.</p>
<p>$$
\log p_\theta(x) =  \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log \frac{p_\theta(z,x)}{q_\phi(z|x)} \right] + \text{D}<em>{\text{KL}} \left[q</em>\phi(z|x)||p_\theta(z|x) \right]
$$</p>
<p>We know from theory that the KL divergence term is always zero or positive. So this means that we can draw a bound on the first term in terms of the marginal log-likelihood.</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}:=\mathbb{E}</em>{q_\phi(z|x)}\left[ \log \frac{p_\theta(z,x)}{q_\phi(z|x)} \right] \leq \log p_\theta(x)
$$</p>
<p>This term is called the Evidence Lower Bound (ELBO). So the objective is to <em>maximize</em> this term which will also minimize the KLD.</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}=\mathbb{E}</em>{q_\phi(z|x)}\left[ \log \frac{p_\theta(z,x)}{q_\phi(z|x)} \right]
$$</p>
<p>So now, we can expand the joint distribution using Bayes rule, i.e. $p(z,x)=p(x|z)p(z)$, to give us.</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}=\mathbb{E}</em>{q_\phi(z|x)}\left[ \log \frac{p(x|z)p(z)}{q_\phi(z|x)} \right]
$$</p>
<p>We can also expand this fraction using the log rules,</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}=\mathbb{E}</em>{q_\phi(z|x)}\left[ \log p(x|z) + \log p(z) - \log q_\phi(z|x) \right].
$$</p>
<p>where:</p>
<ul class="simple">
<li><p>$q_\phi(z|x)$ - encoder network</p></li>
<li><p>$p_\theta(x|z)$ - decoder network</p></li>
<li><p>$p_\theta(z)$ - prior network</p></li>
</ul>
<p>Now, we have some options on how we can group the likelihood, the prior and the variational distribution together and each of them will offer a slightly different interpretation and application.</p>
</section>
<hr class="docutils" />
<section id="reconstruction-loss">
<h2>Reconstruction Loss<a class="headerlink" href="#reconstruction-loss" title="Permalink to this heading">#</a></h2>
<p>If we group the prior probability and the variational distribution together, we get:</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}=\mathbb{E}</em>{q_\phi(z|x)}\left[ \log p(x|z) \right] + \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log \frac{p(z)}{q_\phi(z|x)} \right].
$$</p>
<p>This is the same KLD term as before but in the reverse order. So with a slight of hand in terms of the signs, we can rearrange the term to be</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}= \mathbb{E}</em>{q_\phi(z|x)}\left[ \log p(x|z) \right] - \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log \frac{q_\phi(z|x)} {p(z)}\right].
$$</p>
<hr class="docutils" />
<p><strong>Proof</strong>:</p>
<p>$$
\mathbb{E}_q[ \log p - \log q] = - \mathbb{E}_q[\log q - \log p] = - \mathbb{E}_q[\log\frac{q}{p}]
$$</p>
<p><strong>QED</strong>.</p>
<hr class="docutils" />
<p>So now, we have the exact same KLD term as before. So lets use the simplified form.</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}={\color{blue}\mathbb{E}</em>{q_\phi(z|x)}\left[ \log p(x|z) \right]} - {\color{green}\text{D}<em>\text{KL}\left[q</em>\phi(z|x)||p(z)\right]}.
$$</p>
<p>where:</p>
<ul class="simple">
<li><p>${\color{blue}\mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log p(x|z) \right]}$ - is the $\color{blue}\text{reconstruction loss}$.</p></li>
<li><p>${\color{green}\text{D}<em>\text{KL}\left[q</em>\phi(z|x)||p(z)\right]}$ - is the complexity, i.e. the $\color{green}\text{KL divergence}$ (a distance metric) between the prior and the variational distribution.</p></li>
</ul>
<p>This is easily the most common ELBO term especially with Variational AutoEncoders (VAEs). The first term is the expectation of the likelihood term wrt the variational distribution. The second term is the KLD between the prior and the variational distribution.</p>
</section>
<hr class="docutils" />
<section id="volume-correction">
<h2>Volume Correction<a class="headerlink" href="#volume-correction" title="Permalink to this heading">#</a></h2>
<p>Another approach is more along the lines of the transform distribution. Assume we have our original data domain $\mathcal{X}$ and we have some stochastic transformation, p(z|x), which transforms the data from our original domain to a transform domain, $\mathcal{Z}$.</p>
<p>$$
z \sim p(z|x)
$$</p>
<p>To acquire this, lets look at the equation again</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}=\mathbb{E}</em>{q_\phi(z|x)}\left[ \log p(x|z) + \log p(z) - \log q_\phi(z|x) \right].
$$</p>
<p>except this time we will isolate the prior and combine the likelihood and the variational distribution.</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}={\color{blue}\mathbb{E}</em>{q_\phi(z|x)}\left[ \log p(z) \right]} + {\color{green}\mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log \frac{p(x|z)}{q_\phi(z|x)} \right]}.
$$</p>
<p>where:</p>
<ul class="simple">
<li><p>${\color{blue}\mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log p(z) \right]}$ - is the expectation of the transformed distribution, aka the ${\color{blue}\text{reparameterized probability}}$.</p></li>
<li><p>${\color{green}\mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log \frac{p(x|z)}{q_\phi(z|x)} \right]}$ - is the ratio between the inverse transform and the forward transform , i.e. ${\color{green}\text{Volume Correction Factor}}$ or <em>likelihood contribution</em>.</p></li>
</ul>
<p><strong>Source</strong>: I first saw this approach in the SurVAE Flows paper.</p>
</section>
<hr class="docutils" />
<section id="variational-free-energy-vfe">
<h2>Variational Free Energy (VFE)<a class="headerlink" href="#variational-free-energy-vfe" title="Permalink to this heading">#</a></h2>
<p>There is one more main derivation that remains (thats often seen in the literature). Looking at the equation again</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}=\mathbb{E}</em>{q_\phi(z|x)}\left[ \log p(x|z) + \log p(z) - \log q_\phi(z|x) \right],
$$</p>
<p>we now isolate the likelihood <em>and</em> the prior under the variational expectation. This gives us:</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}={\color{blue}\mathbb{E}</em>{q_\phi(z|x)}\left[ \log p(x|z) p(z)\right]} - {\color{green} \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log q_\phi(z|x) \right]}.
$$</p>
<p>where:</p>
<ul class="simple">
<li><p>${\color{blue}\mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log p(x|z) p(z)\right]}$ - is the ${\color{blue}\text{energy}}$ function</p></li>
<li><p>${\color{green} \mathbb{E}<em>{q</em>\phi(z|x)}\left[ \log q_\phi(z|x) \right]}$ - is the ${\color{green}\text{entropy}}$</p></li>
</ul>
<p><strong>Source</strong>: I see this approach a lot in the Gaussian process literature when they are deriving the Sparse Gaussian Process from Titsias.</p>
</section>
<hr class="docutils" />
<section id="elbo-non-encoder-derivation">
<h2>ELBO (Non-Encoder) - Derivation<a class="headerlink" href="#elbo-non-encoder-derivation" title="Permalink to this heading">#</a></h2>
<p>In all of these formulas, we have an <em>encoder</em> as our variational distribution, i.e. $q(z|x)$, which seeks to amortize the inference. Sometimes this is not necessary and we can find a complicated enough variational distribution, i.e. $q(z)$. This often happens in very simple models, e.g. $y = \mathbf{Wx} + \mathbf{b} + \epsilon$</p>
<p>So this will be a similar derivation as the above, however we will</p>
<p>$$
\log p_\theta(x) = \mathbb{E}<em>{q</em>\phi(z)}\left[ \log p_\theta(x) \right]
$$</p>
<p>I am going to make some grandiose assumptions and skip ahead of the derivation. But I think it might be useful to think ahead and then work my backwards.</p>
<hr class="docutils" />
<p><strong>Reconstruction Loss</strong></p>
<p>This is the easiest term to show because it shows up in many simpler applications when we have very simple models and we believe that</p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}={\color{blue}\mathbb{E}</em>{q_\phi(z)}\left[ \log p(x|z) \right]} - {\color{green}\text{D}<em>\text{KL}\left[q</em>\phi(z)||p(z)\right]}.
$$</p>
<hr class="docutils" />
<p><strong>Volume Correction</strong></p>
<p>This doesnt actually work because <strong>we need</strong> a transformation from $x$ to $z$.</p>
<hr class="docutils" />
<p><strong>Variational Free Energy</strong></p>
<p>$$
\mathcal{L}<em>{\text{ELBO}}={\color{blue}\mathbb{E}</em>{q_\phi(z)}\left[ \log p(x|z) p(z)\right]} - {\color{green} \mathbb{E}<em>{q</em>\phi(z)}\left[ \log q_\phi(z) \right]}.
$$</p>
</section>
<hr class="docutils" />
<section id="elbo-derivation-old">
<h2>ELBO - Derivation (Old)<a class="headerlink" href="#elbo-derivation-old" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p>This is my own derivation from a few years ago. I have improved it (see above) but I keep it here for reference. :)</p>
</div></blockquote>
<p>Lets start with the marginal likelihood function.</p>
<p>$$\mathcal{P}(y| \theta)=\int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot \mathcal{P}(\mathbf x) \cdot d\mathbf{x}$$</p>
<p>where we have effectively marginalized out the $f$s. We already know that its difficult to propagate the $\mathbf x$s through the nonlinear functions $\mathbf K^{-1}$ and $|$det $\mathbf K|$ (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution $q(\mathbf x)$ to approximate the posterior distribution $\mathcal{P}(\mathbf x| y)$. The distribution is normally chosen to be Gaussian:</p>
<p>$$q(\mathbf x) = \prod_{i=1}^{N}\mathcal{N}(\mathbf x|\mathbf \mu_z, \mathbf \Sigma_z)$$</p>
<p>So at this point, we aree interested in trying to find a way to measure the difference between the approximate distribution $q(\mathbf x)$ and the true posterior distribution $\mathcal{P} (\mathbf x)$. Using some algebra, lets take the log of the marginal likelihood (evidence):</p>
<p>$$\log \mathcal{P}(y|\theta) = \log \int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot \mathcal{P}(\mathbf x) \cdot d\mathbf x$$</p>
<p>So now we are going to use the some tricks that you see within almost every derivation of the VI framework. The first one consists of using the Identity trick. This allows us to change the expectation to incorporate the new variational distribution $q(\mathbf x)$. We get the following equation:</p>
<p>$$\log \mathcal{P}(y|\theta) = \log \int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot \mathcal{P}(\mathbf x) \cdot \frac{q(\mathbf x)}{q(\mathbf x)} \cdot d\mathbf x$$</p>
<p>Now that we have introduced our new variational distribution, we can regroup and reweight our expectation. Because I know what I want, I get the following:</p>
<p>$$\log \mathcal{P}(y|\theta) = \log \int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot  q(\mathbf x) \cdot \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \cdot d\mathbf x$$</p>
<p>Now with Jensens inequality, we have the relationship $f(\mathbb{E}[x]) \leq \mathbb{E} [f(x)]$. We would like to put the $\log$ function inside of the integral. Jensens inequality allows us to do this. If we let $f(\cdot)= \log(\cdot)$ then we get the Jensens equality for a concave function, $f(\mathbb{E}[x]) \geq \mathbb{E} [f(x)]$. In this case if we match the terms to each component to the inequality, we have</p>
<p>$$\log \cdot \mathbb{E}<em>\mathcal{q(\mathbf x)} \left[ \mathcal{P}(y|\mathbf x, \theta) \cdot  \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \right]
\geq
\mathbb{E}</em>\mathcal{q(\mathbf x)} \left[\log  \mathcal{P}(y|\mathbf x, \theta) \cdot \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \right]$$</p>
<p>So now finally we have both terms in the inequality. Summarizing everything we have the following relationship:</p>
<p>$$log \mathcal{P}(y|\theta) = \log \int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot  q(\mathbf x) \cdot \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \cdot d\mathbf x $$</p>
<p>$$
\log \mathcal{P}(y|\theta) \geq  \int_\mathcal{X} \left[\log  \mathcal{P}(y|\mathbf x, \theta) \cdot \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \right] q(\mathbf x) \cdot d\mathbf x
$$</p>
<p>Im going to switch up the terminology just to make it easier aesthetically. Im going to let $\mathcal{L}(\theta)$ be $\log \mathcal{P}(y|\theta)$ and $\mathcal{F}(q, \theta) \leq \mathcal{L}(\theta)$. So basically:</p>
<p>$$
\mathcal{L}(\theta) =\log \mathcal{P}(y|\theta) \geq  \int_\mathcal{X} \left[\log  \mathcal{P}(y|\mathbf x, \theta) \cdot \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \right] q(\mathbf x) \cdot d\mathbf x = \mathcal{F}(q, \theta)
$$</p>
<p>With this simple change I can talk about each of the parts individually. Now using log rules we can break apart the likelihood and the quotient. The quotient will be needed for the KL divergence.</p>
<p>$$
\mathcal{F}(q) =
\underbrace{\int_\mathcal{X} q(\mathbf x) \cdot \log  \mathcal{P}(y|\mathbf x, \theta) \cdot d\mathbf x}<em>{\mathbb{E}</em>{q(\mathbf{x})}} +
\underbrace{\int_\mathcal{X} q(\mathbf x) \log  \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)}   \cdot d\mathbf x}_{\text{KL}}
$$</p>
<p>The punchline of this (after many calculated manipulations), is that we obtain an optimization equation $\mathcal{F}(\theta)$:</p>
<p>$$\mathcal{F}(q)=\mathbb{E}<em>{q(\mathbf x)}\left[ \log \mathcal{P}(y|\mathbf x, \theta) \right] - \text{D}</em>\text{KL}\left[ q(\mathbf x) || \mathcal{P}(\mathbf x) \right]$$</p>
<p>where:</p>
<ul class="simple">
<li><p>Approximate posterior distribution: $q(x)$</p>
<ul>
<li><p>The best match to the true posterior $\mathcal{P}(y|\mathbf x, \theta)$. This is what we want to calculate.</p></li>
</ul>
</li>
<li><p>Reconstruction Cost: $\mathbb{E}_{q(\mathbf x)}\left[ \log \mathcal{P}(y|\mathbf x, \theta) \right]$</p>
<ul>
<li><p>The expected log-likelihood measure of how well the samples from $q(x)$ are able to explain the data $y$.</p></li>
</ul>
</li>
<li><p>Penalty: $\text{D}_\text{KL}\left[ q(\mathbf x) || \mathcal{P}(\mathbf x) \right]$</p>
<ul>
<li><p>Ensures that the explanation of the data $q(x)$ doesnt deviate too far from your beliefs $\mathcal{P}(x)$. (Okhams razor constraint)</p></li>
</ul>
</li>
</ul>
<p><strong>Source</strong>: <a class="reference external" href="https://www.shakirm.com/papers/VITutorial.pdf">VI Tutorial</a> - Shakir Mohamed</p>
<p>If we optimize $\mathcal{F}$ with respect to $q(\mathbf x)$, the KL is minimized and we just get the likelihood. As weve seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the $\mathbf x$s through. So thats nothing new and weve done nothing useful. If we introduce some special structure in $q(f)$ by introducing sparsity, then we can achieve something useful with this formulation.
But through augmentation of the variable space with $\mathbf u$ and $\mathbf Z$ we can bypass this problem. The second term is simple to calculate because theyre both chosen to be Gaussian.</p>
<section id="comments-on-q-x">
<h3>Comments on $q(x)$<a class="headerlink" href="#comments-on-q-x" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>We have now transformed our problem from an integration problem to an optimization problem where we optimize for $q(x)$ directly.</p></li>
<li><p>Many people tend to simplify $q$ but we could easily write some dependencies on the data for example $q(x|\mathcal{D})$.</p></li>
<li><p>We can easily see the convergence as we just have to wait until the loss (free energy) reaches convergence.</p></li>
<li><p>Typically $q(x)$ is a Gaussian whereby the variational parameters are the mean and the variance. Practically speaking, we could freeze or unfreeze any of these parameters if we have some prior knowledge about our problem.</p></li>
<li><p>Many people say tighten the bound but they really just mean optimization: modifying the hyperparameters so that we get as close as possible to the true marginal likelihood.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Tutorial Series - <a class="reference external" href="https://chrisorm.github.io/VI-Why.html">Why?</a> | <a class="reference external" href="https://chrisorm.github.io/VI-ELBO.html">ELBO</a> | <a class="reference external" href="https://chrisorm.github.io/VI-MC.html">MC ELBO</a> | <a class="reference external" href="https://chrisorm.github.io/VI-reparam.html">Reparameterization</a> | <a class="reference external" href="https://chrisorm.github.io/VI-ELBO-MC-approx.html">MC ELBO unBias</a> | <a class="reference external" href="https://chrisorm.github.io/VI-MC-PYT.html">MC ELBO PyTorch</a> | <a class="reference external" href="https://chrisorm.github.io/pydata-2018.html">Talk</a></p></li>
<li><p>Blog Posts: Neural Variational Inference</p>
<ul>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2016-07-01-neural-variational-inference-classical-theory.html">Classical Theory</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2016-07-04-neural-variational-inference-stochastic-variational-inference.html">Scaling Up</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2016-07-05-neural-variational-inference-blackbox.html">BlackBox Mode</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2016-07-11-neural-variational-inference-variational-autoencoders-and-Helmholtz-machines.html">VAEs and Helmholtz Machines</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2016-07-14-neural-variational-importance-weighted-autoencoders.html">Importance Weighted AEs</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html">Neural Samplers and Hierarchical VI</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2019-05-10-importance-weighted-hierarchical-variational-inference.html">Importance Weighted Hierarchical VI</a> | <a class="reference external" href="https://www.youtube.com/watch?v=pdSu7XfGhHw&amp;feature=youtu.be">Video</a></p></li>
</ul>
</li>
<li><p>Normal Approximation to the Posterior Distribution - <a class="reference external" href="http://bjlkeng.github.io/posts/normal-approximations-to-the-posterior-distribution/">blog</a></p></li>
</ul>
<p><strong>Lower Bound</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf">Understaing the Variational Lower Bound</a></p></li>
<li><p><a class="reference external" href="http://paulrubenstein.co.uk/deriving-the-variational-lower-bound/">Deriving the Variational Lower Bound</a></p></li>
</ul>
<p><strong>Summaries</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1711.05597.pdf">Advances in Variational Inference</a></p></li>
</ul>
<p><strong>Presentations</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.shakirm.com/papers/VITutorial.pdf">VI Shakir</a></p></li>
<li><p>Deisenroth - <a class="reference external" href="https://drive.google.com/file/d/1sAIF0rqgNbVbp7ZbuiS7kh96Yns04k1i/view">VI</a> | <a class="reference external" href="https://drive.google.com/open?id=14WOcbwn011rJbFFsSbeoeuSxY4sMG4KY">IT</a></p></li>
<li><p><a class="reference external" href="https://www.doc.ic.ac.uk/~mpd37/teaching/ml_tutorials/2017-11-22-Ek-BNP-and-priors-over-functions.pdf">Bayesian Non-Parametrics and Priors over functions</a></p></li>
<li><p><a class="reference external" href="https://filebox.ece.vt.edu/~s14ece6504/slides/Moran_I_ECE_6504_VB.pdf">here</a></p></li>
</ul>
<p><strong>Reviews</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="http://krasserm.github.io/2018/04/03/variational-inference/">From EM to SVI</a></p></li>
<li><p><a class="reference external" href="https://ermongroup.github.io/cs228-notes/inference/variational/">Variational Inference</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1601.00670.pdf">VI- Review for Statisticians</a></p></li>
<li><p><a class="reference external" href="http://www.robots.ox.ac.uk/~sjrob/Pubs/vbTutorialFinal.pdf">Tutorial on VI</a></p></li>
<li><p><a class="reference external" href="https://zhiyzuo.github.io/VI/">VI w/ Code</a></p></li>
<li><p><a class="reference external" href="https://blog.evjang.com/2016/08/variational-bayes.html">VI - Mean Field</a></p></li>
<li><p><a class="reference external" href="https://github.com/philschulz/VITutorial">VI Tutorial</a></p></li>
<li><p>GMM</p>
<ul>
<li><p><a class="reference external" href="https://github.com/bertini36/GMM">VI in GMM</a></p></li>
<li><p><a class="reference external" href="https://mattdickenson.com/2018/11/18/gmm-python-pyro/">GMM Pyro</a> | <a class="reference external" href="http://pyro.ai/examples/gmm.html">Pyro</a></p></li>
<li><p><a class="reference external" href="https://github.com/ldeecke/gmm-torch">GMM PyTorch</a> | <a class="reference external" href="https://angusturner.github.io/generative_models/2017/11/03/pytorch-gaussian-mixture-model.html">PyTorch</a> | <a class="reference external" href="https://github.com/RomainSabathe/dagmm/blob/master/gmm.py">PyTorchy</a></p></li>
</ul>
</li>
</ul>
<p><strong>Code</strong></p>
<p><strong>Extensions</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html">Neural Samplers and Hierarchical Variational Inference</a></p></li>
</ul>
<section id="from-scratch">
<h3>From Scratch<a class="headerlink" href="#from-scratch" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Programming a Neural Network from Scratch - Ritchie Vink (2017) - <a class="reference external" href="https://www.ritchievink.com/blog/2017/07/10/programming-a-neural-network-from-scratch/">blog</a></p></li>
<li><p>An Introduction to Probability and Computational Bayesian Statistcs - Eric Ma 0<a class="reference external" href="https://ericmjl.github.io/essays-on-data-science/machine-learning/computational-bayesian-stats/">Blog</a></p></li>
<li><p>Variational Inference from Scratch - Ritchie Vink (2019) - <a class="reference external" href="https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/">blog</a></p></li>
<li><p>Bayesian inference; How we are able to chase the Posterior - Ritchie Vink (2019) - [blog](Bayesian inference; How we are able to chase the Posterior)</p></li>
<li><p>Algorithm Breakdown: Expectation Maximization - <a class="reference external" href="https://www.ritchievink.com/blog/2019/05/24/algorithm-breakdown-expectation-maximization/">blog</a></p></li>
</ul>
</section>
</section>
<section id="id1">
<h2>Variational Inference<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Variational Bayes and The Mean-Field Approximation - Keng (2017) - <a class="reference external" href="http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/">blog</a></p></li>
<li><p>https://www.ritchievink.com/blog/2019/06/10/bayesian-inference-how-we-are-able-to-chase-the-posterior/</p></li>
<li><p>https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/notes/bayesian/inference"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="../inference.html" title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title">Inference Schemes</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="cond_vi.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title">Conditional Variational Inference</p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivations">
   Motivations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pros-and-cons">
   Pros and Cons
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-variational-inference">
     Why Variational Inference?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-not-variational-inference">
     Why Not Variational Inference?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variational-distribution">
   Variational Distribution
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simple-q-z">
     Simple, $q(z)$
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mixture-distribution">
     Mixture Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bijective-transformation-flow">
     Bijective Transformation (Flow)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-transformation-encoder-amortization">
     Stochastic Transformation (Encoder, Amortization)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elbo-encoder-derivation">
   ELBO (Encoder) - Derivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reconstruction-loss">
   Reconstruction Loss
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#volume-correction">
   Volume Correction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variational-free-energy-vfe">
   Variational Free Energy (VFE)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elbo-non-encoder-derivation">
   ELBO (Non-Encoder) - Derivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elbo-derivation-old">
   ELBO - Derivation (Old)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comments-on-q-x">
     Comments on $q(x)$
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resources">
   Resources
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#from-scratch">
     From Scratch
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Variational Inference
  </a>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By J. Emmanuel Johnson
</p>

  </div>
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2023.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on None.<br>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>