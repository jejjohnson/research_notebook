
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Gaussian Processes &#8212; Research Notebook</title>
    
  <link href="../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
      <img src="../../../_static/book.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Research Notebook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../intro.html">
   Overview
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../resources/python/overview.html">
   Python
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/jax_journey/overview.html">
   My JAX Journey
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tutorials/remote/overview.html">
   Remote Computing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../bayesian/overview.html">
   Bayesian
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../concepts/overview.html">
   Sleeper Concepts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../kernels/overview.html">
   Kernel Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../egps/overview.html">
   Uncertain Gaussian Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../info_theory/overview.html">
   Information Theory
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Data
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../data/overview.html">
   Data
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../bib.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/notes/gps/gps.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference">
   Inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gp-training">
   GP Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#drawbacks">
   Drawbacks
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="gaussian-processes">
<h1>Gaussian Processes<a class="headerlink" href="#gaussian-processes" title="Permalink to this headline">Â¶</a></h1>
<p>Consider the regression setting where we assume the following model:
$<span class="math notranslate nohighlight">\(
 y = \boldsymbol{f}(\mathbf{x}) + \epsilon
\)</span>$</p>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a discriminate vector of inputs, <span class="math notranslate nohighlight">\(\mathbf{f}(\cdot)=\left[f_1, \ldots, f_N \right]\)</span> is a latent GP function, and <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N} (0, \sigma_y^2)\)</span> is a independently, identically distributed (i.i.d.) Gaussian noise parameter. We place a GP <span class="math notranslate nohighlight">\(\textbf{prior}\)</span> for <span class="math notranslate nohighlight">\(p(\boldsymbol{f})\)</span> s.t.</p>
<div class="math notranslate nohighlight">
\[
p (\boldsymbol{f}|\mathbf{X}, \boldsymbol{\theta}) \sim \mathcal{GP} \left(\mathbf{m}_{\boldsymbol\psi}, \mathbf{K}_{\boldsymbol\phi}\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_\mathcal{GP}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{K}_\mathcal{GP}\)</span> are the mean and covariance matrix for the GP, <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = \left\{ \boldsymbol{\psi,\phi}\right\}\)</span> are the parameters of the model and <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is the data. Combining this prior with the regression problem model from the previous equation, we assume a <strong>likelihood</strong> function:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
    p (y | \boldsymbol{f}, \mathbf{X}) \sim \mathcal{N} (y | \boldsymbol{f} (\mathbf{x}), \sigma_y^2 \mathbf{I})
\end{equation}
\]</div>
<p>We can invoke Bayes rule giving us the joint posterior distribution:
$<span class="math notranslate nohighlight">\(
\begin{equation}
    p (\boldsymbol{f}, \boldsymbol{f}_* | y) = \frac{p(\boldsymbol{f}, \boldsymbol{f}_*)p(y|\boldsymbol{f})}{p(y)}
\end{equation}
\)</span><span class="math notranslate nohighlight">\(
where \)</span>p(y)<span class="math notranslate nohighlight">\( is the marginal likelihood which we can obtain by integrating out the latent variables \)</span>\boldsymbol{f}<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(
\begin{aligned}
    p(y) &amp;= \int_{\boldsymbol{f}} p(y,\boldsymbol{f})d\boldsymbol{f} \\
    &amp;= \mathcal{N} (y | \boldsymbol{\mu}_\mathcal{GP}, \mathbf{K} + \sigma^2\mathbf{I}) \\
    &amp;= \mathcal{N} (y | \boldsymbol{\mu}_\mathcal{GP}, \mathbf{K}_{\mathcal{GP}})
\end{aligned}
\)</span><span class="math notranslate nohighlight">\(
In a regression setting, we are more interested in predictions; given some parameters and some data, what is the predictive function \)</span>\boldsymbol{f}$? This is known as  the {posterior} distribution:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
    p( \boldsymbol{f} |  y, \mathbf{X}, \boldsymbol{\theta}) \sim \mathcal{N} (\boldsymbol{\mu}_{\mathcal{GP}} , \boldsymbol{\nu}^2_{GP})
\end{equation}
\]</div>
<hr class="docutils" />
<div class="section" id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">Â¶</a></h2>
<p>First, given the joint distribution of <span class="math notranslate nohighlight">\(\boldsymbol{f}, \boldsymbol{f}_*\)</span> conditioned on <span class="math notranslate nohighlight">\(\mathbf{X,X_*}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation} 
p(\boldsymbol{f}, \boldsymbol{f}_*|\mathbf{x}, \mathbf{x}_*)=\mathcal{N}\left( 
    \begin{bmatrix}  
    \boldsymbol{f} \\ \boldsymbol{f}_*
    \end{bmatrix}; 
    \begin{bmatrix}
    \boldsymbol{m}(\mathbf{x}) \\ \boldsymbol{m}(\mathbf{x}_*)
    \end{bmatrix},
    \begin{bmatrix}
    \mathbf{K} &amp; \mathbf{K}_* \\
    \mathbf{K}_* &amp; \mathbf{K}_{**}
    \end{bmatrix} \right)
\end{equation}
\end{split}\]</div>
<p>If we condition on our training inputs <span class="math notranslate nohighlight">\(D=(\mathbf{X}, y)\)</span>, we can come up with a <strong>predictive distribution</strong> for test points <span class="math notranslate nohighlight">\(\mathbf{x}_*\)</span> via</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
    p(\boldsymbol{f}_* | \boldsymbol{f}) = \mathcal{N} (\boldsymbol{\mu}_{\mathcal{GP}*} , \boldsymbol{\nu}^2_{\mathcal{GP}**})
\end{equation}
\]</div>
<p>and we can give the GP predictive mean and variance functions as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \boldsymbol{\mu}_{\mathcal{GP}} &amp;= \underbrace{m (\mathbf{x}_*)}_{\text{Prior Mean}} + \underbrace{\boldsymbol{k}_{*} \mathbf{K}^{-1}}_{\text{Kalman Gain}}\underbrace{(y- m (\mathbf{X}))}_{\text{Error}}\\
    \boldsymbol{\nu}^2_{\mathcal{GP}} &amp;= k_{**} - \boldsymbol{k}_{*} \mathbf{K}^{-1}\boldsymbol{k}_{*}^{\top}.
\end{aligned}
\end{split}\]</div>
<p>If we integrate out the <span class="math notranslate nohighlight">\(\boldsymbol{f}\)</span> (or just take the conditional distribution of the joint PDF), then we get:
$<span class="math notranslate nohighlight">\(
\begin{aligned}
    p(\boldsymbol{f}_*|\mathbf{x}_*, \mathbf{x}, y) &amp;= \int_{\boldsymbol{f}} p(\boldsymbol{f}|\mathbf{x}, y) p(\boldsymbol{f}_*|\mathbf{x}_*,y)d\boldsymbol{f} \\
    &amp;= \mathcal{N}(\boldsymbol{f}_*|\mu_*, \Sigma_*)
\end{aligned}
\)</span><span class="math notranslate nohighlight">\(
and the joint distribution of \)</span>\boldsymbol{f}<em>*<span class="math notranslate nohighlight">\( and unobserved \)</span>y<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(
\begin{equation} 
p(y, \boldsymbol{f}_*|\mathbf{x}, \mathbf{x}_*)=\mathcal{N}\left( 
    \begin{bmatrix}  
    \boldsymbol{f} \\ \boldsymbol{f}_*
    \end{bmatrix}; 
    \begin{bmatrix}
    \mathcal{GP}M(\mathbf{x}) \\ \mathcal{GP}M(\mathbf{x}_*)
    \end{bmatrix},
    \begin{bmatrix}
    \mathcal{GP}K(\mathbf{x}, \mathbf{x}) + \sigma^2\mathbf{I} &amp; \mathcal{GP}K(\mathbf{x}, \mathbf{x}_*) \\
    \mathcal{GP}K(\mathbf{x}, \mathbf{x}_*) &amp; \mathcal{GP}K(\mathbf{x}_*, \mathbf{x}_*)
    \end{bmatrix} \right)
\end{equation}
\)</span><span class="math notranslate nohighlight">\(
which gives us the mean predictions and the variance in our predictions:
\)</span><span class="math notranslate nohighlight">\(
\begin{align}
    \boldsymbol{\mu}_{\mathcal{GP}} &amp;= \underbrace{ \boldsymbol{m}(\mathbf{x}_*)}_{\text{Prior Mean}} + \underbrace{\mathbf{k}_{*} \mathbf{K}^{-1}}_{\text{Kalman Gain}}\underbrace{(y- \boldsymbol{m}(\mathbf{X}))}_{\text{Error}}= \boldsymbol{m}(\mathbf{x}_*) + \mathbf{K}_{*} \alpha \\
    \boldsymbol{\nu}^2_{\mathcal{GP}} &amp;= \underbrace{k_{**}}_{\text{Prior Variance}} - \mathbf{k}_{*} \mathbf{K}_{\mathcal{GP}}^{-1}\mathbf{k}_{*}^{\top}
\end{align}
\)</span><span class="math notranslate nohighlight">\(
where \)</span>\alpha = \mathbf{K}^{-1}(y-m(\mathbf{X}))<span class="math notranslate nohighlight">\( and \)</span>\mathbf{K}</em>\mathcal{GP}=\mathbf{K}<em>\theta(\mathbf{X,X})+\sigma^2\mathbf{I}<span class="math notranslate nohighlight">\(. This is the typical formulation~\ref{fig:intro-probabilistic} which assumes that the output of \)</span>\mathbf{x}<span class="math notranslate nohighlight">\( (and \)</span>\mathbf{x}</em><em><span class="math notranslate nohighlight">\() is deterministic. In section~\ref{chapter3:egp}, we will look at the case where \)</span>\mathbf{x}_</em>$ is stochastic.</p>
</div>
<div class="section" id="gp-training">
<h2>GP Training<a class="headerlink" href="#gp-training" title="Permalink to this headline">Â¶</a></h2>
<p>In GP model inference, one maximizes the likelihood of the data <span class="math notranslate nohighlight">\(D\)</span> given the hyper-parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}, \boldsymbol{\sigma}_y^2\)</span>. The marginal likelihood is given by:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
    p (y | \mathbf{X}, \theta) = \mathcal{N} \left( y | \mathcal{GP}M, \mathcal{GP}K + \sigma_y^2\mathbf{I} \right)
\end{equation}
\]</div>
<p>We can find the hyper-parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> by maximizing the marginal log-likelihood. So fully expanding of the eq: \ref{eq:gp_ml}, we get:
$<span class="math notranslate nohighlight">\(
\begin{equation}
    \log p (y | \mathbf{X}, \theta) = -\underbrace{\frac{1}{2}(y - \mathcal{GP}M)^{\top} \mathbf{K}_{\mathcal{GP}}^{-1} (y - \mathcal{GP}M)}_{\text{Data-Fit}}  - \underbrace{\frac{1}{2} \log \left| \mathbf{K}_{GP} \right|}_{\text{Complexity}} - \frac{N}{2}\log 2\pi
\end{equation}
\)</span>$</p>
<p>This maximization automatically embodies Occamâs razor which does a trade-off between model complexity and overfitting. This is closed form for all GPs but these days, we typically use automatic differentiation toolboxes to alleviate some of the burden.  Irregardless, the two most expensive calculations are within this procedure as the inversion of <span class="math notranslate nohighlight">\(\matinv{\mathbf{K}}_{\mathcal{GP}}\)</span> and the <span class="math notranslate nohighlight">\(\Det{\mathbf{K}_{\mathcal{GP}}}\)</span>; since <span class="math notranslate nohighlight">\(\mathbf{K} \in \Real^{N \times N}\)</span>, then these calculations are <span class="math notranslate nohighlight">\(\bigO (N^3)\)</span> in operations and <span class="math notranslate nohighlight">\(\bigO (N^2)\)</span> in memory costs. The kernel function is one of the most important aspects within the GP training regime. Once the kernel has been chosen to best reflect the problem at hand it has been found in \cite{GPPRIOR2018} that any prior over the hyper-parameters does not provide significant improvements in GP predictions. However, note that the community is notorious for using the isotropic RBF kernel by default when conducting research. This kernel is the most flexible among the kernel family but not necessarily the most expressive~\cite{AUTOGP2017}.</p>
</div>
<div class="section" id="drawbacks">
<h2>Drawbacks<a class="headerlink" href="#drawbacks" title="Permalink to this headline">Â¶</a></h2>
<p>\begin{displayquote}
â\textit{It is important to keep in mind that Gaussian processes are not appropriate priors for all problems.}â \
â Neal, 1998
\end{displayquote}</p>
<p>It is important to note that although the GP algorithm is one of the most trusted and reliable algorithms, it is not always the best algorithm to use for all problems. Below we mention a few drawbacks that the standard GP algorithm has along with some of the standard approaches to overcoming these drawbacks.</p>
<p>\vspace{2mm} \noindent \textbf{Gaussian Marginals}. GPs have problems modeling heavy-tailed, asymmetric or multi-modal marginal distributions. There are some methods that change the likelihood so that it is heavy tailed~\citep{GPTSTUDENT2011,GPTSTUDENT2014} but this would remove the conjugacy of the likelihood term which would incur difficulties during fitting. Deep GPs and latent covariate models are an improvement to this limitation. A very popular approach is to construct a fully Bayesian model. This entails hyperpriors over the kernel parameters and Monte carlo sampling methods such as Gibbs sampling~\citep{GPGIBBS08}, slice sampling~\citep{GPSLICE2010}, Hamiltonian Monte Carlo~\citep{GPHMC2018}, and Sequential Monte Carlo~\citep{GPSMC15}. These techniques will capture more complex distributions. With the advent of better software~\citep{PYMC16,NUMPYRO2019} and more advanced sampling techniques like a differentiable iterative NUTS implementation~\citep{NUMPYRO2019}, the usefulness of MC schemes is resurfacing.</p>
<p>\vspace{2mm} \noindent \textbf{Limited Number of Moments}. This is related to the previous limitation: the idea that an entire function can be captured in terms of two moments: a mean and a covariance. There are some relationships which are difficult to capture without an adequate description, e.g. discontinuities~\citep{Neal96} and non-stationary processes, and thus is a limitation of the GP priors we choose. The advent of warping the inputs or outputs of a GP has becoming a very popular technique to deal with the limited expressivity of kernels. Input warping is popular in methods such as deep kernel learning whereby a Neural network is used to capture the features and are used as inputs to the kernel function output warping is common in chained~\citep{GPCHAINED2016} and heteroscedastic methods where the function output is warped by another GP to capture the noise model of the data. Deep Gaussian processes~\citep{Damianou2015} can be thought of input and output warping methods due the multi-layer composition of function inputs and outputs.</p>
<p>\vspace{2mm} \noindent \textbf{Linearity of Predictive Mean}. The predictive mean of a GP is linear to the observations, i.e. <span class="math notranslate nohighlight">\(\mu_{GP}=\mathbf{K}\alpha\)</span>. This essentially is a smoother which can be very powerful but also will miss key features. If there is some complex structured embedded within the dataset, then a GP model can never really capture this irregardless of the covariance function found.</p>
<p>\vspace{2mm} \noindent \textbf{Predictive Covariance}. The GP predictive variance is a function of the training inputs and it is independent of the observed inputs. This is important if the input data has some information which could be used to help determine the regions of uncertainty, e.g. the gradient. An example would be data on a spatial grid whereby some regions points would have more certainty than others which could be obtained by knowing the input location and not necessarily the expected output.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/notes/gps"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By J. Emmanuel Johnson<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>