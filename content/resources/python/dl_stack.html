
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Deep Learning Stack &#8212; Research Notebook</title>
    
  <link href="../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Scaling Stack" href="scale_stack.html" />
    <link rel="prev" title="Earth Science Stack" href="earthsci_stack.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/book.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Research Notebook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../intro.html">
   Overview
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="overview.html">
   Python
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="ides.html">
     Integraded Development Environment (IDE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="stack.html">
     Standard Python Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="earthsci_stack.html">
     Earth Science Stack
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Deep Learning Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="scale_stack.html">
     Scaling Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="good_code.html">
     Good Code
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/jax_journey/overview.html">
   My JAX Journey
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/ecosystem.html">
     Ecosystem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/vmap.html">
     vmap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/jit.html">
     Jit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/classes.html">
     Classes
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/jax_journey/algorithms/overview.html">
     Algorithms
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/algorithms/bisection.html">
       Bisection search
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/algorithms/kernel_derivatives.html">
       Kernel Derivatives
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/algorithms/gpr.html">
       GP from Scratch
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/gfs_with_jax.html">
       Gaussianization Flows
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/remote/overview.html">
   Remote Computing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/remote/vscode_jlab.html">
     JupyterLab + VSCode
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../notes/bayesian/overview.html">
   Bayesian
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/intro.html">
     Bayesian: Language of Uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/gaussian.html">
     Gaussian Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/regression.html">
     Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/inference/inference.html">
     Solving Hard Integral Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/inference/variational_inference.html">
     Variational Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/bayesian/confidence_intervals.html">
     Confidence Intervals
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../notes/concepts/overview.html">
   Sleeper Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/notation.html">
     Notation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/change_of_variables.html">
     Change of Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/identity_trick.html">
     Identity Trick
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/inverse_function.html">
     Inverse Function Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/jensens.html">
     Jensens Inequality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/concepts/lin_alg.html">
     Linear Algebra Tricks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../notes/kernels/overview.html">
   Kernel Methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/kernels/kernel_derivatives.html">
     Kernel Derivatives
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../notes/egps/overview.html">
   Uncertain Gaussian Processes
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/predictions.html">
     Uncertain Predictions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/gauss_approx.html">
     Gaussian Approximation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/mcmc.html">
     Monte Carlo Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/taylor.html">
     Linearization (Taylor Expansions)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/moment_matching.html">
     Moment Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/bgplvm.html">
     Bayesian GPLVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/error_prop.html">
     Error Propagation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/egps/next.html">
     Next Steps
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../notes/egps/experiments.html">
     Notebooks
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../notes/egps/notebooks/gpytorch_egp_taylor.html">
       Gaussian Process Gradients with GPyTorch
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../notes/info_theory/overview.html">
   Information Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/info_theory/histogram.html">
     Entropy Estimator - Histogram
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../notes/info_theory/experiments/rbig_sample_consistency.html">
     Experiment - RBIG Sample Consistency
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/normalizing_flows/overview.html">
   Normalizing Flows
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/normalizing_flows/lecture_1_ig.html">
     Lecture I - Iterative Gaussianization
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/1.0_univariate_gauss.html">
       1.1 - Univariate Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/1.1_marginal_gauss.html">
       1.2 - Marginal Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/1.2_gaussianization.html">
       1.2 - Iterative Gaussianization
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/normalizing_flows/lecture_2_gf.html">
     Lecture II - Gaussianization Flows
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/lecture_3_gfs_pt1_mg.html">
       Parameterized Marginal Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/lecture_3_gfs_pt2_rot.html">
       Parameterized Rotations
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/lecture_3_gfs_pt3_plane.html">
       Example - 2D Plane
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/resources/python/dl_stack.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-deep-learning">
   What is Deep Learning?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#anatomy-of-good-dl-software">
   Anatomy of good DL software
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convergence-of-the-libraries">
   Convergence of the Libraries
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#so-what-to-choose">
   So what to choose?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#list-of-software">
   List of Software
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#core-packages">
     Core Packages
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tensorflow-tf">
       TensorFlow (TF)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pytorch">
       PyTorch
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-packages">
     Other Packages
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-resources">
   Other Resources
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#automatic-differentiation">
     Automatic Differentiation
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Deep Learning Stack</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-deep-learning">
   What is Deep Learning?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#anatomy-of-good-dl-software">
   Anatomy of good DL software
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convergence-of-the-libraries">
   Convergence of the Libraries
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#so-what-to-choose">
   So what to choose?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#list-of-software">
   List of Software
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#core-packages">
     Core Packages
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tensorflow-tf">
       TensorFlow (TF)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pytorch">
       PyTorch
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-packages">
     Other Packages
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-resources">
   Other Resources
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#automatic-differentiation">
     Automatic Differentiation
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="deep-learning-stack">
<h1>Deep Learning Stack<a class="headerlink" href="#deep-learning-stack" title="Permalink to this headline">¶</a></h1>
<div class="section" id="what-is-deep-learning">
<h2>What is Deep Learning?<a class="headerlink" href="#what-is-deep-learning" title="Permalink to this headline">¶</a></h2>
<p>Before we get into the software, I just wanted to quickly define deep learning. A recent debate on <a class="reference external" href="https://twitter.com/yudapearl/status/1215174538087948288">twitter</a> got me thinking about an appropriate definition and it helped me think about how this definition relates to the software. It gave me perspective.</p>
<p><strong>Definition 1</strong> by Yann LeCun - <a class="reference external" href="https://twitter.com/ylecun/status/1215286749477384192">tweet</a> (paraphrased)</p>
<blockquote>
<div><p>Deep Learning is methodology: building a model by assembling parameterized modules into (possibly dynamic) graphs and optimizing it with gradient-based methods.</p>
</div></blockquote>
<p><strong>Definition II</strong> by Danilo Rezende - <a class="reference external" href="https://twitter.com/DeepSpiker/status/1209862283368816641">tweet</a> (paraphrased)</p>
<blockquote>
<div><p>Deep Learning is a collection of tools to build complex modular differentiable functions.</p>
</div></blockquote>
<p>These definitions are more or less the same: deep learning is a tool to facilitate gradient-based optimization scheme for models. The data we use, the exact way we construct it, and how we train it aren’t really in the definition. Most people might think a DL tool is the ensemble of different neural networks like <a class="reference external" href="https://pbs.twimg.com/media/EOWJc2KWsAA8xDF?format=jpg&amp;name=4096x4096">these</a>. But from henceforth, I refer to DL in the terms of facilitating the development of those neural networks, not the network library itself.</p>
<p>So in terms of DL software, we need only a few components:</p>
<ul class="simple">
<li><p>Tensor structures</p></li>
<li><p>Automatic differentiation (AutoGrad)</p></li>
<li><p>Model Framework (Layers, etc)</p></li>
<li><p>Optimizers</p></li>
<li><p>Loss Functions</p></li>
</ul>
<p>Anything built on top of that can be special cases where we need special structures to create models for special cases. The simple example is a Multi-Layer Perceptron (MLP) model where we need some <code class="docutils literal notranslate"><span class="pre">weight</span></code> parameter, a <code class="docutils literal notranslate"><span class="pre">bias</span></code> parameter and an <code class="docutils literal notranslate"><span class="pre">activation</span></code> function. A library that allows you to train this model using an optimizer and a loss function, I would consider this autograd software (e.g. JAX). A library that has this functionality built-in (a.k.a. a <code class="docutils literal notranslate"><span class="pre">layer</span></code>), I would consider this deep learning software (e.g. TensorFlow, PyTorch). While the only difference is the level of encapsulation, the latter makes it much easier to build ‘<em>complex modular</em>’ neural networks whereas the former, not so much. You could still do it with the autograd library but you would have to design your entire model structure from scratch as well. So, there are still a LOT of things we can do with parameters and autograd alone but I wouldn’t classify it as DL software. This isn’t super important in the grand scheme of things but I think it’s important to think about when creating a programming language and/or package and thinking about the target user.</p>
</div>
<hr class="docutils" />
<div class="section" id="anatomy-of-good-dl-software">
<h2>Anatomy of good DL software<a class="headerlink" href="#anatomy-of-good-dl-software" title="Permalink to this headline">¶</a></h2>
<p>Francios Chollet (the creator of <code class="docutils literal notranslate"><span class="pre">keras</span></code>) has been very vocal about the benefits of how TensorFlow caters to a broad audience ranging from applied users and algorithm developers. Both sides of the audience have different needs so building software for both audiences can very, very challenging. Below I have included a really interesting figure which highlights the axis of operations.</p>
<p align="center">
  <img src="https://keras-dev.s3.amazonaws.com/tutorials-img/spectrum-of-workflows.png" alt="drawing" width="800"/>
</p>
<p><strong>Photo Credit</strong>: Francois Chollet <a class="reference external" href="https://twitter.com/fchollet/status/1052228463300493312/photo/1">Tweet</a></p>
<p>As shown, there are two axis which define one way to split the DL software styles: the x-axis covers the <strong>model</strong> construction process and the y-axis covers the <strong>training</strong> process. I am sure that this is just one way to break apart DL software but I find it a good abstract way to look at it because I find that we can classify most use cases somewhere along this graph. I’ll briefly outline a few below:</p>
<ul class="simple">
<li><p><strong>Case 1</strong>: All I care about is using a prebuilt model on some new data that my company has given me. I would probably fall somewhere on the upper right corner of the graph with the <code class="docutils literal notranslate"><span class="pre">Sequential</span></code> model and the built-in <code class="docutils literal notranslate"><span class="pre">training</span></code> scheme.</p></li>
<li><p><strong>Case II</strong>: I need a slightly more complex training scheme because I want to learn two models that share hidden nodes but they’re not the same size. I also want to do some sort of cycle training, i.e. train one model first and then train the other. Then I would probably fall somewhere near the middle, and slightly to the right with the <code class="docutils literal notranslate"><span class="pre">Functional</span></code> model and a custom <code class="docutils literal notranslate"><span class="pre">training</span></code> scheme.</p></li>
<li><p><strong>Case III</strong>: I am a DL researcher and I need to control every single aspect of my model. I belong to the left and on the bottom with the full <code class="docutils literal notranslate"><span class="pre">subclass</span></code> model and completely custom <code class="docutils literal notranslate"><span class="pre">training</span></code> scheme.</p></li>
</ul>
<p>So there are many more special cases but by now you can imagine that most general cases can be found on the graph. I would like to stress that designing software to do all of these cases is not easy as these cases require careful design individually. It needs to be flexible.</p>
<p>Maybe I’m old school, but I like the modular way of design. So in essence, I think we should design libraries that focus on one aspect, one audience and do it well. I also like a standard practice and integration so that everything can fit together in the end and we can transfer information or products from one part to another. This is similar to how the Japanese revolutionized building cars by having one machine do one thing at a time and it all fit together via a standard assembly line. So in the end, I want people to be able to mix and match as they see fit. To try to please everyone with “<em>one DL library that rules them all</em>” seems a bit silly in my opinion because you’re spreading out your resources. But then again, I’ve never built software from scratch and I’m not a mega coorperation like Google or Facebook, so what do I know? I’m just one user…in a sea of many.</p>
<blockquote>
<div><p>With great power, comes great responsibility - Uncle Ben</p>
</div></blockquote>
<p>On a side note, when you build popular libraries, you shape how a massive amount of people think about the problem. Just like expressiveness is only as good as your vocabulary and limited by your language, the software you create actively morphs how your users think about framing and solving their problems. Just something to think about.</p>
</div>
<hr class="docutils" />
<div class="section" id="convergence-of-the-libraries">
<h2>Convergence of the Libraries<a class="headerlink" href="#convergence-of-the-libraries" title="Permalink to this headline">¶</a></h2>
<p>Originally, there was a lot of differences between the deep learning libraries, e.g. <code class="docutils literal notranslate"><span class="pre">static</span></code> v.s. <code class="docutils literal notranslate"><span class="pre">dynamic</span></code>, <code class="docutils literal notranslate"><span class="pre">Sequential</span></code> v.s. <code class="docutils literal notranslate"><span class="pre">Subclass</span></code>. But now they are all starting to converge or at least have similar ways of constructing models and training. Below is a quick example of 4 deep learning libraries. If you know your python DL libraries trivia, try and guess which library do you think it is. Click on the details below to find out the answer.</p>
<p align="center">
  <img src="https://pbs.twimg.com/media/DppB0xJUUAAjGi-?format=jpg&name=4096x4096" alt="drawing" width="800"/>
</p>
<p><strong>Photo Credit</strong>: Francois Chollet <a class="reference external" href="https://twitter.com/fchollet/status/1052228463300493312/photo/1">Tweet</a></p>
<p><strong>Answer here</strong>:</p>
<details>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Gluon</p></td>
<td><p>TensorFlow</p></td>
</tr>
<tr class="row-odd"><td><p>PyTorch</p></td>
<td><p>Chainer</p></td>
</tr>
</tbody>
</table>
</details>
<p>It does begs the question: if all of the libraries are basically the same, why are their multiple libraries? That’s a great question and I do not know the answer to that. I think options are good as competition generally stimulates innovation. But at some point, there should be a limit no? But then again, the companies backing each of these languages are quite huge (Google, Microsoft, Uber, Facebook, etc). So I’m sure they have more than enough employees to justify the existence of their own library. But then again, imagine if they all put their efforts into making one great library. It could be an epic success! Or an epic disaster. I guess we will never know.</p>
</div>
<hr class="docutils" />
<div class="section" id="so-what-to-choose">
<h2>So what to choose?<a class="headerlink" href="#so-what-to-choose" title="Permalink to this headline">¶</a></h2>
<p>There are many schools of thought. Some people suggest <a class="reference external" href="https://ericmjl.github.io/blog/2019/10/31/reimplementing-and-testing-deep-learning-models/">doing things from scratch</a> while some favour software to allow users to <a class="reference external" href="https://scale.com/interviews/jeremy-howard/transcript">jumping right in</a>. Fortunately, whatever the case may be or where you’re at in your ML journey, there is a library to suit your needs. And as seen above, most of them are converging so learning one python package will have be transferable to another. In the end, people are going to choose whatever based on personal factors such as “what is my style” or environmental factors such as “what is my research lab using now?”.</p>
<p>I have a personal short list below just from observations, trends and reading but it is by no means concrete. Do whatever works for you!</p>
<p><strong>Jump Right In</strong> - <a class="reference external" href="https://docs.fast.ai/">fastai</a></p>
<blockquote>
<div><p>If you’re interesting in applying your models to new and interesting datasets and are not necessarily interested in development then I suggest you start with fastai. This is a library that simplifies deep learning usage with all of the SOTA tricks built-in so I think it would save the average user a lot of time.</p>
</div></blockquote>
<p><strong>From Scratch</strong> - <a class="reference external" href="https://github.com/google/jax">JAX</a></p>
<blockquote>
<div><p>If you like to do things from scratch in a very numpy-like way but also want all of the benefits of autograd on CPU/GPU/TPUs, then this is for you.</p>
</div></blockquote>
<p><strong>Deep Learning Researcher</strong> - <a class="reference external" href="https://pytorch.org/">PyTorch</a></p>
<blockquote>
<div><p>If you’re doing research, then I suggest you use PyTorch. It is currently the most popular library for doing ML research. If you’re looking at many of the SOTA algorithms, you’ll find most of them being written in PyTorch these days. The API is similar to TensorFlow so you can easily transfer your skills to TF if needed.</p>
</div></blockquote>
<p><strong>Production/Industry</strong> - <a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a></p>
<blockquote>
<div><p>TensorFlow holds the market in production. By far. So if you’re looking to go into industry, it’s highly likely that you’ll be using TensorFlow. There are still a lot of researchers that use TF too. Fortunately, the API is similar to PyTorch if you use the subclass system so the skills are transferable.</p>
</div></blockquote>
<p>!&gt; <strong>Warning</strong>: The machine learning community changes rapidly so any trends you observe are extremely volatile. Just like the machine learning literature, what’s popular today can change within 6 months. So don’t ever lock yourself in and stay flexible to cope with the changes. But also don’t jump on bandwagons either as you’ll be jumping every weekend. Keep a good balance and maintain your mental health.</p>
</div>
<hr class="docutils" />
<div class="section" id="list-of-software">
<h2>List of Software<a class="headerlink" href="#list-of-software" title="Permalink to this headline">¶</a></h2>
<p>There are many autograd libraries available right now. All of the big tech companies (Google, Facebook, Amazon, Microsoft, Uber, etc.) have a piece of the python software package pie. Fortunately for us, many of them have open-sourced so we get to enjoy high-quality, feature-filled, polished software for us to work with. I believe this, more than anything, has accelerated research in the computational world, in particular for people doing research related to machine learning. I’ll also include some of my favourites for Probabilistic Programming Languages (PPL) as well as Gaussian process (GP) library.</p>
<hr class="docutils" />
<div class="section" id="core-packages">
<h3>Core Packages<a class="headerlink" href="#core-packages" title="Permalink to this headline">¶</a></h3>
<div class="section" id="tensorflow-tf">
<h4><a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a> (TF)<a class="headerlink" href="#tensorflow-tf" title="Permalink to this headline">¶</a></h4>
<p>This is by far the most popular autograd library currently. Backed by Google (Alphabet, Inc), it is the go to python package for production and it is very popular for researchers as well. Recently (mid-2019) there was a huge update which made the python API much easier to use by having a tight keras integration and allowing for a more pythonic-style of coding.</p>
<p><strong>PPL</strong>: <a class="reference external" href="https://www.tensorflow.org/probability">TensorFlow Probability</a></p>
<p>As the name suggests, this is a probabilistic library that is built on top of TensorFlow. It has many distributions, priors, and inference methods. In addition, it uses the same <code class="docutils literal notranslate"><span class="pre">layers</span></code> as the <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code> library with some <code class="docutils literal notranslate"><span class="pre">edward2</span></code> integration.</p>
<p><strong>GP</strong>: <a class="reference external" href="https://github.com/GPflow/GPflow">GPFlow</a></p>
<p>This is a special library for SOTA GPs that’s built on top of TF. It is the success to the original GP library, GPy but it has a much cleaner code base and a bit better documentation due to its use of autograd.</p>
<p><strong>Bayesian Layers</strong>: <a class="reference external" href="https://github.com/google/edward2">Edward2</a></p>
<p>While there is some integration of <code class="docutils literal notranslate"><span class="pre">edward2</span></code> into the TFP library, there are some stand alone functions in the original Ed2 library. Some more advanced <code class="docutils literal notranslate"><span class="pre">layers</span></code> such as Sparse Gaussian processes and Noise contrastive priors. It all works seemlessly with TF and TFP.</p>
</div>
<hr class="docutils" />
<div class="section" id="pytorch">
<h4><a class="reference external" href="https://pytorch.org/">PyTorch</a><a class="headerlink" href="#pytorch" title="Permalink to this headline">¶</a></h4>
<p>This is the most popular DL library for the machine learning community. Backed by Facebook, this is a rather new library that came out 2 years ago. It took a bit of time, but eventually people started using it more and more especially in a research setting. The reason is because it is very pythonic, it was designed by researchers and for researchers, and it keeps the design simple even sacrificing speed if needed. If you’re starting out, most people will recommend you start with PyTorch.</p>
<p><strong>PPL</strong>: <a class="reference external" href="https://pyro.ai/">Pyro</a></p>
<p>This is a popular Bayesian DL library that is built on top of PyTorch. Backed by Uber, you’ll find a lot of different inference scheme. This library is a bit of a learning curve because their API. But, their documentation and tutorial section is great so if you take your time you should pick it up. Unfortunately I don’t find too many papers with Pyro code examples, but when the Bayesian community gets bigger, I’m sure this will change.</p>
<p><strong>GP</strong>: <a class="reference external" href="https://gpytorch.ai/">GPyTorch</a></p>
<p>This is the most scalable GP library currently that’s built on top of PyTorch. They mainly use Matrix-Vector-Multiplication strategies to scale exact GPs up to 1 million points using multiple GPUs. They recently just revamped their documentation as well with many examples. If you plan to put GPs in production, you should definitely check out this library.</p>
<p><strong>Jump In:</strong> <a class="reference external" href="https://docs.fast.ai/">fastai</a></p>
<p>This is a DL library that was created to facilate people using some of the SOTA NN methods to solve problems. It was created by Jeremy Howard and has gained popularity due to its simplicity. It has all of the SOTA parameters by default and there are many options to tune your models as you see fit. In addition, it has a huge community with a very active <a class="reference external" href="https://forums.fast.ai/">forum</a>. I think it’s a good first pass if you’re looking to solve real problems and get your feet wet while not getting too hung up on the model building code. They also have 2 really good <a class="reference external" href="https://course18.fast.ai/">courses</a> that teach you the mechanics of ML and DL while still giving you enough tools to make you a competitive.</p>
<hr class="docutils" />
<p><strong><a class="reference external" href="https://github.com/google/jax">JAX</a></strong></p>
<p>This is the successor for the popular <a class="reference external" href="https://github.com/HIPS/autograd">autograd</a> package that is now backed by Google. It is basically <code class="docutils literal notranslate"><span class="pre">numpy</span></code> on steroids giving you access to an autograd function and it can be used on CPUs, GPUs and TPUs. The gradients are very intuitive as it is just using a <code class="docutils literal notranslate"><span class="pre">grad</span></code> function; recursively if you want high-order gradients. It’s still fairly early in development but it’s gaining popularity very fast and has shown to be <a class="reference external" href="https://github.com/dionhaefner/pyhpc-benchmarks"><strong>very</strong> competitive</a>. It’s fast. Really fast. To really take advantage of this library, you will need to do a more functional-style of programming but there have been many benefits, e.g. using it for MCMC sampling schemes has become <a class="reference external" href="https://twitter.com/remilouf/status/1215740986195922944">popular</a>. Keep in mind that although JAX does have the <span class="xref myst">flax</span> API which allows you to have predefined NN architectures, it’s primarily an autograd library, not a Deep learning library. There are many</p>
<p><strong>PPL</strong>: <a class="reference external" href="https://github.com/pyro-ppl/numpyro#numpyro">Numpyro</a></p>
<p>This is a great PPL which took it’s inspiration from <strong>Pyro</strong> but is built on top of JAX. It really focuses on MCMC methods but it also has quite a few other nuggets such as stochastic variational inference (SVI) and normalizing flows (NFs). I would say this is the most popular (and oldest) library for probabilistic programming under the JAX framework. My favourite tutorials include the ones about <a class="reference external" href="https://pyro.ai/numpyro/bayesian_regression.html">Bayesian Regression</a>, <a class="reference external" href="https://pyro.ai/numpyro/examples/gp.html">Gaussian processes</a> and a <a class="reference external" href="http://num.pyro.ai/en/stable/primitives.html#random-flax-module">Bayesian Neural Network</a>.</p>
<p><a class="reference external" href="https://objax.readthedocs.io/en/latest/"><strong>Objax</strong></a></p>
<p>This is an OOP that’s built on top of JAX. It allows you to use the PyTorch method but having JAX underneath. It also has a lot of pre-defined layers and optimization methods.</p>
<p><a class="reference external" href="https://poets-ai.github.io/elegy/"><strong>Elegy</strong></a></p>
<p>This is a keras-like wrapper for JAX. It has all of the features that allow you to use JAX as if you were using Keras. It does follow a more functional style so it’s a bit more advanced that Objax for example.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="other-packages">
<h3>Other Packages<a class="headerlink" href="#other-packages" title="Permalink to this headline">¶</a></h3>
<p>Although these are in the ‘other’ category, it doesn’t mean that they are lower tier by any means. I just put them here because I’m not too familiar with them outside of the media.</p>
<p><strong><a class="reference external" href="https://chainer.org/">Chainer</a></strong></p>
<p>Preferred Networks (<em>Japanese Company</em>)</p>
<p><strong><a class="reference external" href="https://mxnet.apache.org/">MXNet</a></strong></p>
<p>Amazon</p>
<p><strong><span class="xref myst">Theano</span></strong></p>
<p>maintained by the PyMC3 developers.</p>
<p><strong><span class="xref myst">PyMC3</span></strong> &amp; <strong><span class="xref myst">PyMC4</span></strong></p>
<p><strong><span class="xref myst">CNTK</span></strong></p>
<p>Microsoft</p>
<p>–</p>
</div>
</div>
<div class="section" id="other-resources">
<h2>Other Resources<a class="headerlink" href="#other-resources" title="Permalink to this headline">¶</a></h2>
<div class="section" id="automatic-differentiation">
<h3>Automatic Differentiation<a class="headerlink" href="#automatic-differentiation" title="Permalink to this headline">¶</a></h3>
<p><strong><a class="reference external" href="https://d2l.ai/chapter_preliminaries/autograd.html">D2L Tutorial</a></strong></p>
<blockquote>
<div><p>A programming tutorial where they show how to do automatic differentiation using 3 different frameworks: tensorflow, pytorch and mxnet. Useful to show the differences between some popular libraries (MXNet isn’t so mainstream).</p>
</div></blockquote>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1502.05767">Automatic differentiation in machine learning: a survey</a> - Baydin et al (2018)</p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=NG21KWZSiok"> Colin Carroll - Getting started with automatic differentiation</a> - PyCon 2020</p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/resources/python"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="earthsci_stack.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Earth Science Stack</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="scale_stack.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Scaling Stack</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By J. Emmanuel Johnson<br/>
    
        &copy; Copyright 2020.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>