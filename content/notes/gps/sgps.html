
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Sparse Gaussian Processes &#8212; Research Notebook</title>
    
  <link href="../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/book_v2.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Research Notebook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../intro.html">
   Overview
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../resources/python/overview.html">
   Python
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/ides.html">
     Integraded Development Environment (IDE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/stack.html">
     Standard Python Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/earthsci_stack.html">
     Earth Science Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/dl_stack.html">
     Deep Learning Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/scale_stack.html">
     Scaling Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/good_code.html">
     Good Code
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/jax_journey/overview.html">
   My JAX Journey
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/ecosystem.html">
     Ecosystem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/vmap.html">
     vmap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/jit.html">
     Jit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/classes.html">
     Classes
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/jax_journey/algorithms/overview.html">
     Algorithms
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/algorithms/bisection.html">
       Bisection search
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/algorithms/kernel_derivatives.html">
       Kernel Derivatives
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/algorithms/gpr.html">
       GP from Scratch
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/gfs_with_jax.html">
       Gaussianization Flows
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/remote/overview.html">
   Remote Computing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/remote/ssh.html">
     SSH Config
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/remote/conda.html">
     Conda 4 Remote Servers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/remote/jlab.html">
     Jupyter Lab
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/remote/vscode_jlab.html">
     VSCode + JLab
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../bayesian/overview.html">
   Bayesian
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/intro.html">
     Bayesian: Language of Uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/gaussian.html">
     Gaussian Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/regression.html">
     Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/inference/inference.html">
     Solving Hard Integral Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/inference/variational_inference.html">
     Variational Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/confidence_intervals.html">
     Confidence Intervals
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../concepts/overview.html">
   Sleeper Concepts
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/notation.html">
     Notation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/change_of_variables.html">
     Change of Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/identity_trick.html">
     Identity Trick
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/inverse_function.html">
     Inverse Function Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/jensens.html">
     Jensens Inequality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/lin_alg.html">
     Linear Algebra Tricks
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../kernels/overview.html">
   Kernel Methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../kernels/kernel_derivatives.html">
     Kernel Derivatives
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../egps/overview.html">
   Uncertain Gaussian Processes
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../egps/predictions.html">
     Uncertain Predictions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../egps/gauss_approx.html">
     Gaussian Approximation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../egps/mcmc.html">
     Monte Carlo Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../egps/taylor.html">
     Linearization (Taylor Expansions)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../egps/moment_matching.html">
     Moment Matching
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../egps/bgplvm.html">
     Bayesian GPLVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../egps/error_prop.html">
     Error Propagation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../egps/next.html">
     Next Steps
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../egps/experiments.html">
     Notebooks
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../egps/notebooks/gpytorch_egp_taylor.html">
       Gaussian Process Gradients with GPyTorch
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../info_theory/overview.html">
   Information Theory
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../info_theory/histogram.html">
     Entropy Estimator - Histogram
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../info_theory/experiments/rbig_sample_consistency.html">
     Experiment - RBIG Sample Consistency
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/normalizing_flows/overview.html">
   Normalizing Flows
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/normalizing_flows/lecture_1_ig.html">
     Lecture I - Iterative Gaussianization
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/1.0_univariate_gauss.html">
       1.1 - Univariate Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/1.1_marginal_gauss.html">
       1.2 - Marginal Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/1.2_gaussianization.html">
       1.2 - Iterative Gaussianization
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/normalizing_flows/lecture_2_gf.html">
     Lecture II - Gaussianization Flows
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/lecture_3_gfs_pt1_mg.html">
       Parameterized Marginal Gaussianization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/lecture_3_gfs_pt2_rot.html">
       Parameterized Rotations
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/normalizing_flows/lecture_3_gfs_pt3_plane.html">
       Example - 2D Plane
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/notes/gps/sgps.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/jejjohnson/research_notebook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/jejjohnson/research_notebook/issues/new?title=Issue%20on%20page%20%2Fcontent/notes/gps/sgps.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subset-of-data">
   Subset of Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-approximations">
   Kernel Approximations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inducing-points">
   Inducing Points
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sparse-gps-inducing-points-summary">
   Sparse GPs - Inducing Points Summary
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#objective-function">
     Objective Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#observations-about-the-sparse-gps">
     Observations about the Sparse GPs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variational-compression">
     Variational Compression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#joint-distribution-augmented-space-mathcal-p-f-u">
       Joint Distribution - Augmented Space
       <span class="math notranslate nohighlight">
        \(\mathcal{P}(f,u)\)
       </span>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#conditional-distribution-mathcal-p-y-u">
       Conditional Distribution -
       <span class="math notranslate nohighlight">
        \(\mathcal{P}(y|u)\)
       </span>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variational-bound-on-mathcal-p-y-u">
       Variational Bound on
       <span class="math notranslate nohighlight">
        \(\mathcal P (y|u)\)
       </span>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#titsias-innovation-et-q-f-mathcal-p-f-u">
       Titsias Innovation: et
       <span class="math notranslate nohighlight">
        \(q(f) = \mathcal{P}(f|u)\)
       </span>
       .
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elbos">
   ELBOs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lower-bound">
     Lower Bound
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variational-bound-on-mathcal-p-y">
       Variational Bound on
       <span class="math notranslate nohighlight">
        \(\mathcal P (y)\)
       </span>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-variational-inference">
     Stochastic Variational Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supplementary-material">
   Supplementary Material
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#important-formulas">
     Important Formulas
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#nystrom-approximation">
       Nystrom Approximation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sherman-morrison-woodbury-formula">
       Sherman-Morrison-Woodbury Formula
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sylvester-determinant-theorem">
       Sylvester Determinant Theorem
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resources">
   Resources
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code-walk-throughs">
   Code Walk-Throughs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#math-walkthroughs">
   Math Walkthroughs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#papers">
     Papers
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#thesis-explain">
       Thesis Explain
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#presentations">
     Presentations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notes">
     Notes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#blogs">
     Blogs
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Sparse Gaussian Processes</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subset-of-data">
   Subset of Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-approximations">
   Kernel Approximations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inducing-points">
   Inducing Points
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sparse-gps-inducing-points-summary">
   Sparse GPs - Inducing Points Summary
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#objective-function">
     Objective Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#observations-about-the-sparse-gps">
     Observations about the Sparse GPs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variational-compression">
     Variational Compression
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#joint-distribution-augmented-space-mathcal-p-f-u">
       Joint Distribution - Augmented Space
       <span class="math notranslate nohighlight">
        \(\mathcal{P}(f,u)\)
       </span>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#conditional-distribution-mathcal-p-y-u">
       Conditional Distribution -
       <span class="math notranslate nohighlight">
        \(\mathcal{P}(y|u)\)
       </span>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variational-bound-on-mathcal-p-y-u">
       Variational Bound on
       <span class="math notranslate nohighlight">
        \(\mathcal P (y|u)\)
       </span>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#titsias-innovation-et-q-f-mathcal-p-f-u">
       Titsias Innovation: et
       <span class="math notranslate nohighlight">
        \(q(f) = \mathcal{P}(f|u)\)
       </span>
       .
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elbos">
   ELBOs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lower-bound">
     Lower Bound
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variational-bound-on-mathcal-p-y">
       Variational Bound on
       <span class="math notranslate nohighlight">
        \(\mathcal P (y)\)
       </span>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-variational-inference">
     Stochastic Variational Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supplementary-material">
   Supplementary Material
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#important-formulas">
     Important Formulas
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#nystrom-approximation">
       Nystrom Approximation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sherman-morrison-woodbury-formula">
       Sherman-Morrison-Woodbury Formula
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sylvester-determinant-theorem">
       Sylvester Determinant Theorem
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resources">
   Resources
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code-walk-throughs">
   Code Walk-Throughs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#math-walkthroughs">
   Math Walkthroughs
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#papers">
     Papers
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#thesis-explain">
       Thesis Explain
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#presentations">
     Presentations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notes">
     Notes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#blogs">
     Blogs
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="sparse-gaussian-processes">
<h1>Sparse Gaussian Processes<a class="headerlink" href="#sparse-gaussian-processes" title="Permalink to this headline">¶</a></h1>
<p>[toc]</p>
<hr class="docutils" />
<p>Sparse GPs refer to a family of methods that seek to take a subset of points in order to approximate the full dataset. Typically we can break them down into 5 categories:</p>
<ul class="simple">
<li><p>Subset of Data (Transformation, Random Sampling)</p></li>
<li><p>Data Approximation Methods (Nystrom, Random Fourer Features, Random Kitchen Sinks, Sparse-Spectrum, FastFood, A la Carte)</p></li>
<li><p>Inducing Points (SoR, FITC, DTC, KISS-GP)</p></li>
<li><p>Linear Algebra (Toeplitz, Kronecker, )</p></li>
<li><p>Approximate Inference (Variational Methods)</p></li>
</ul>
<p>Each of these methods ultimately augment the model so that the largest computation goes from <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}(MN^2)\)</span> where <span class="math notranslate nohighlight">\(M&lt;&lt;N\)</span>.</p>
<hr class="docutils" />
<div class="section" id="subset-of-data">
<h2>Subset of Data<a class="headerlink" href="#subset-of-data" title="Permalink to this headline">¶</a></h2>
<p>This is the simplest way to approximate the data. The absolute simplest way is to take a random subsample of your data. However this is often not a good idea because the more data you have the more information you’re more likely to have. It’s an age old rule that says if you want better predictions, it’s often better just to have more data.</p>
<p>A more sophisticated way to get a subsample of your data is to do some sort of pairwise similarity comparison scheme - i.e. Kernel methods. There are a family of methods like the Nystrom approximation or Random Fourier Features (RFF) which takes a subset of the points through pairwise comparisons. These are kernel matrix approximations so we can transform our data from our data space <span class="math notranslate nohighlight">\(\mathcal{X} \in \mathbb{R}^{N \times D}\)</span> to subset data space <span class="math notranslate nohighlight">\(\mathcal{Z} \in \mathbb{R}^{M \times d}\)</span> which is found through an eigen decomposition scheme.</p>
<p>In GPs we calculate a kernel matrix <span class="math notranslate nohighlight">\(\mathbf K \in \mathbb{R}^{N \times N}\)</span>. If <span class="math notranslate nohighlight">\(N\)</span> is large enough, then throughout the marginal likelihood, we need to calculate <span class="math notranslate nohighlight">\(\mathbf K^{-1}\)</span> and <span class="math notranslate nohighlight">\(|\mathbf K|\)</span> which has <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> operations and <span class="math notranslate nohighlight">\(\mathcal{O}(N^2)\)</span> memory costs. So we make an approximate matrix <span class="math notranslate nohighlight">\(\mathbf {\tilde{K}}\)</span> given by the following formula:</p>
<div class="math notranslate nohighlight">
\[\tilde{K}=K_{z}K_{zz}^{-1}K_z^{\top}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(K_{zz}=K(z,z)\in \mathbb{R}^{M\times M}\)</span> - the kernel matrix for the subspace <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(K_z=K(x,z)\in \mathbb{R}^{N\times M}\)</span> - the transformation matrix from the data space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to the subspace <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(K \approx \tilde{K} \in \mathbb{R}^{N \times N}\)</span> - the approximate kernel matrix of the data space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span></p></li>
</ul>
<p>Below is an example of where this would be applicable where we just implement this method where we just transform the day.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.kernel_approximation</span> <span class="kn">import</span> <span class="n">Nystroem</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span> <span class="k">as</span> <span class="n">GPR</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_processes.kernels</span> <span class="kn">import</span> <span class="n">RBF</span>

<span class="c1"># Initialize Nystrom transform</span>
<span class="n">nystrom_map</span> <span class="o">=</span> <span class="n">Nystrom</span><span class="p">(</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_components</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Transform Data</span>
<span class="n">X_transformed</span> <span class="o">=</span> <span class="n">nystrom_map</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># initialize GPR</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPR</span><span class="p">()</span>

<span class="c1"># fit GP model</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="kernel-approximations">
<h2>Kernel Approximations<a class="headerlink" href="#kernel-approximations" title="Permalink to this headline">¶</a></h2>
<p>Pivoting off of the method above, we
So now when we calculate the log likelihood term <span class="math notranslate nohighlight">\(\log \mathcal{P}(y|X,\theta)\)</span> we can have an approximation:</p>
<div class="math notranslate nohighlight">
\[\log \mathcal{N}(y | 0, K + \sigma^2I) \approx \log \mathcal{N}(y | 0, \tilde{K} + \sigma^2I)\]</div>
<p>Notice how we haven’t actually changing our formulation because we still have to calculate the inverse of <span class="math notranslate nohighlight">\(\tilde{K}\)</span> which is <span class="math notranslate nohighlight">\(\mathbb{R}^{N \times N}\)</span>. Using the <a class="reference external" href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury matrix identity</a> for the kernel approximation form (<a class="reference external" href="https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula">Sherman-Morrison Formula</a>):</p>
<div class="math notranslate nohighlight">
\[(\tilde{K} + \sigma^2 I)^{-1}=\sigma^{-2}I - \sigma^{-4}K_z(K_{zz}+\sigma^{-2}K_z^{\top}K_z)^{-1}K_z^{\top}\]</div>
<p>Now the matrix that we need to invert is <span class="math notranslate nohighlight">\((K_{zz}+\sigma^{-2}K_z^{\top}K_z)^{-1}\)</span> which is <span class="math notranslate nohighlight">\((M \times M)\)</span> which is considerably smaller if <span class="math notranslate nohighlight">\(M &lt;&lt; N\)</span>. So the overall computational complexity reduces to <span class="math notranslate nohighlight">\(\mathcal{O}(NM^2)\)</span>.</p>
</div>
<hr class="docutils" />
<div class="section" id="inducing-points">
<h2>Inducing Points<a class="headerlink" href="#inducing-points" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Deisenroth - GPs for Big Data - <a class="reference external" href="https://www.doc.ic.ac.uk/~mpd37/teaching/tutorials/2015-04-14-mlss.pdf">MLSS2015</a></p></li>
<li><p>Dai - Scalable GPs - <a class="reference external" href="http://zhenwendai.github.io/slides/gpss2018_slides.pdf">MLSS2018</a></p></li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="sparse-gps-inducing-points-summary">
<h2>Sparse GPs - Inducing Points Summary<a class="headerlink" href="#sparse-gps-inducing-points-summary" title="Permalink to this headline">¶</a></h2>
<hr class="docutils" />
<div class="section" id="objective-function">
<h3>Objective Function<a class="headerlink" href="#objective-function" title="Permalink to this headline">¶</a></h3>
<p>So I think it is important to make note of the similarities between methods; specifically between FITC and VFE which are some staple methods one would use to scale GPs naively. Not only is it helpful for understanding the connection between all of the methods but it also helps with programming and seeing where each method differs algorithmically. Each sparse method is a method of using some set of inducing points or subset of data <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> from the data space <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. We typically have some approximate matrix <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> which approximates the kernel matrix <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Q}_{ff}=\mathbf{K}_{fu}\mathbf{K}_{uu}^{-1}\mathbf{K}_{uf}\]</div>
<p>Then we would use the Sherman-Morrison formula to reduce the computation cost of inverting the matrix <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>. Below is the negative marginal log likelihood cost function that is minimized where we can see the each term broken down:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta)= \frac{N}{2}\log 2\pi + \underbrace{\frac{1}{2} \log\left| \mathbf{Q}_{ff}+\mathbf{G}\right|}_{\text{Complexity Penalty}} + \underbrace{\frac{1}{2}\mathbf{y}^{\top}(\mathbf{Q}_{ff}+\mathbf{G})^{-1}\mathbf{y}}_{\text{Data Fit}} + \underbrace{\frac{1}{2\sigma_n^2}\text{trace}(\mathbf{T})}_{\text{Trace Term}}
\]</div>
<p>The <strong>data fit</strong> term penalizes the data lying outside the covariance ellipse, the <strong>complexity penalty</strong> is the integral of the data fit term over all possible observations <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> which characterizes the volume of possible datasets, the <strong>trace term</strong> ensures the objective function is a true lower bound to the MLE of the full GP. Now, below is a table that shows the differences between each of the methods.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Algorithm</p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(\mathbf{G}\)</span></p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(\mathbf{T}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>FITC</p></td>
<td class="text-align:center"><p>diag <span class="math notranslate nohighlight">\((\mathbf{K}_{ff}-\mathbf{Q}_{ff}) + \sigma_n^2\mathbf{I}\)</span></p></td>
<td class="text-align:center"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>VFE</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\sigma_n^2 \mathbf{I}\)</span></p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\mathbf{K}_{ff}-\mathbf{Q}_{ff}\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>DTC</p></td>
<td class="text-align:center"><p><span class="math notranslate nohighlight">\(\sigma_n^2 \mathbf{I}\)</span></p></td>
<td class="text-align:center"><p>0</p></td>
</tr>
</tbody>
</table>
<p>Another thing to keep in mind is that the FITC algorithm approximates the model whereas the VFE algorithm approximates the inference step (the posterior). So here we just a have a difference in philosophy in how one should approach this problem. Many people in the Bayesian community will <a class="reference external" href="https://www.prowler.io/blog/sparse-gps-approximate-the-posterior-not-the-model">argue</a> for approximating the inference but I think it’s important to be pragmatic about these sorts of things.</p>
</div>
<div class="section" id="observations-about-the-sparse-gps">
<h3>Observations about the Sparse GPs<a class="headerlink" href="#observations-about-the-sparse-gps" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>VFE</p>
<ul>
<li><p>Overestimates noise variance</p></li>
<li><p>Improves with additional inducing inputs</p></li>
<li><p>Recovers the full GP Posterior</p></li>
<li><p>Hindered by local optima</p></li>
</ul>
</li>
<li><p>FITC</p>
<ul>
<li><p>Can severly underestimate the noise variance</p></li>
<li><p>May ignore additional inducing inputs</p></li>
<li><p>Does not recover the full GP posterior</p></li>
<li><p>Relies on Local Optima</p></li>
</ul>
</li>
</ul>
<p>Some parameter initialization strategies:</p>
<ul class="simple">
<li><p>K-Means</p></li>
<li><p>Initially fixing the hyperparameters</p></li>
<li><p>Random Restarts</p></li>
</ul>
<p>An interesting solution to find good hyperparameters for VFE:</p>
<ol class="simple">
<li><p>Find parameters with FITC solution</p></li>
<li><p>Initialize GP model of VFE with FITC solutions</p></li>
<li><p>Find parameters with VFE.</p></li>
</ol>
<p><strong>Source:</strong></p>
<ul class="simple">
<li><p>Understanding Probabilistic Sparse GP Approximations - Bauer et. al. (2017) - <a class="reference external" href="https://arxiv.org/pdf/1606.04820.pdf">Paper</a></p></li>
<li><p>Efficient Reinforcement Learning using Gaussian Processes - Deisenroth (2010) - <span class="xref myst">Thesis</span></p></li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="variational-compression">
<h3>Variational Compression<a class="headerlink" href="#variational-compression" title="Permalink to this headline">¶</a></h3>
<p align="center">
  <img src="pics/variational_compression.png" alt="drawing" width="500"/>
</p>
<p><strong>Figure</strong>: This graphical model shows the relationship between the data <span class="math notranslate nohighlight">\(X\)</span>, the labels <span class="math notranslate nohighlight">\(y\)</span> and the augmented labels <span class="math notranslate nohighlight">\(z\)</span>.</p>
<p>This is a concept I’ve came across that seeks to give a stronger argument for using an augmented space <span class="math notranslate nohighlight">\(\mathcal Z\in \mathbb{R}^{M \times D}\)</span> instead of just the data space <span class="math notranslate nohighlight">\(\mathcal X \in \mathbb{R}^{N \times D}\)</span>. This has allowed us to reduce the computational complexity of all of our most expensive calculations from <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> to <span class="math notranslate nohighlight">\(\mathcal{O}(NM^2)\)</span> when we are learning the best parameters for our GP models. The term <strong>variational compression</strong> comes from the notion that we want to suppress the function valuse <span class="math notranslate nohighlight">\(f\)</span> with some auxilary variables <span class="math notranslate nohighlight">\(u\)</span>. It’s kind of like reducing the data space <span class="math notranslate nohighlight">\(\mathcal X\)</span> with the auxilary data space <span class="math notranslate nohighlight">\(\mathcal Z\)</span> in a principled way. This approach is very useful as it allows us to use a suite of variational inference techniques which in turn allows us to scale GP methods. In addition, we even have access to advanced optimization strategies such as stochastic variational inference and parallization strategies. You’ll also notice that the GP literature has essentially formulated almost all major GP algorithm families (e.g. GP regression, GP classification and GP latent variable modeling) through this variation compression strategy. Below we will look at a nice argument; presented by Neil Lawrence (MLSS 2019); which really highlights the usefulness and cleverness of this approach and how it relates to many GP algorithms.</p>
<div class="section" id="joint-distribution-augmented-space-mathcal-p-f-u">
<h4>Joint Distribution - Augmented Space <span class="math notranslate nohighlight">\(\mathcal{P}(f,u)\)</span><a class="headerlink" href="#joint-distribution-augmented-space-mathcal-p-f-u" title="Permalink to this headline">¶</a></h4>
<p>Let’s add an additional set of variables <span class="math notranslate nohighlight">\(u\)</span> that’s jointly Gaussian with our original function <span class="math notranslate nohighlight">\(f\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p(f,u)=\mathcal{N}\left( 
    \begin{bmatrix}  
    f \\ u
    \end{bmatrix}; 
    \begin{bmatrix}
    0 \\ 0
    \end{bmatrix},
    \begin{bmatrix}
    K_{ff} &amp; K_{fu} \\
    K_{uf} &amp; K_{uu}
    \end{bmatrix} \right)\end{split}\]</div>
<p>We have a new space where we have introduced some auxilary variables <span class="math notranslate nohighlight">\(u\)</span> to be modeled jointly with <span class="math notranslate nohighlight">\(f\)</span>. Using all of the nice properties of Gaussian distributions, we can easily write down the conditional distribution <span class="math notranslate nohighlight">\(\mathcal{P}(f|u)\)</span> and marginal distribution <span class="math notranslate nohighlight">\(\mathcal{P}(u)\)</span> in terms of the joint distribution <span class="math notranslate nohighlight">\(\mathcal P (f,u)\)</span> using conditional probability rules.</p>
<div class="math notranslate nohighlight">
\[\mathcal P (f,u) = \mathcal{P}(f|u) \cdot \mathcal{P}(u)\]</div>
<p>where:</p>
<ul class="simple">
<li><p>Conditional Dist.: <span class="math notranslate nohighlight">\(\mathcal{P}(\mathbf{f | u}) = \mathcal N (f| \mathbf {\mu_u, \nu^2_{uu}})\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mu_u = \mathbf{K_{fu}K_{uu}^{-1}u}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\nu^2_{uu} = \mathbf{K_{ff} - K_{fu}K_{uu}^{-1}K_{uf}}\)</span></p></li>
</ul>
</li>
<li><p>Augmented space Prior: <span class="math notranslate nohighlight">\(\mathcal P (\mathbf u) = \mathcal N\left( \mathbf u | 0, \mathbf K_{uu} \right)\)</span></p></li>
</ul>
<p>We <strong>could</strong> actually marginalize out <span class="math notranslate nohighlight">\(u\)</span> to get back to the standard GP prior <span class="math notranslate nohighlight">\(\mathcal P (f) = \mathcal{GP} (f | \mathbf{ m, K_{ff}})\)</span>. But keep in mind that the reason why we did the conditional probability is this way is because of the computationally decreased complexity that we gain ,<span class="math notranslate nohighlight">\(\mathcal{O}(N^3) \rightarrow \mathcal{O}(NM^2)\)</span>. We want to ‘compress’ the data space <span class="math notranslate nohighlight">\(\mathcal X\)</span> and subsequently the function space of <span class="math notranslate nohighlight">\(f\)</span>. So now let’s write the complete joint distribution which includes the data likelihood and the augmented latent variable space:</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y,f,u|X,Z)= \underbrace{\mathcal{P}(y|f)}_{\text{Likelihood}} \cdot \underbrace{\mathcal{P} (f|u, X, Z)}_{\text{Conditional Dist.}} \cdot \underbrace{\mathcal{P}(u|Z)}_{\text{Prior}}\]</div>
<p>We have a new term which is the familiar GP likelihood term <span class="math notranslate nohighlight">\(\mathcal P (y|f) = \mathcal{N}(y|f, \sigma_y^2\mathbf I)\)</span>. The rest of the terms we have already defined above. So now you can kind of see how we’re attempting to compress the conditional distribution <span class="math notranslate nohighlight">\(f\)</span>. We no longer need the prior for <span class="math notranslate nohighlight">\(X\)</span> or <span class="math notranslate nohighlight">\(f\)</span> in order to obtain the joint distribution for our model. The prior we have is <span class="math notranslate nohighlight">\(\mathcal P (u)\)</span> which is kind of a made up variable.  From henceforth, I will be omitting the dependency on <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> as they’re not important for the argument that follows. But keep it in the back of your mind that that dependency does exist.</p>
</div>
<div class="section" id="conditional-distribution-mathcal-p-y-u">
<h4>Conditional Distribution - <span class="math notranslate nohighlight">\(\mathcal{P}(y|u)\)</span><a class="headerlink" href="#conditional-distribution-mathcal-p-y-u" title="Permalink to this headline">¶</a></h4>
<p>The next step would be to try and condition on <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(u\)</span> to obtain the conditional distribution of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(u\)</span>, <span class="math notranslate nohighlight">\(\mathcal{P}(y|u)\)</span>. We can rearrange the terms of the formula above like so:</p>
<div class="math notranslate nohighlight">
\[\frac{\mathcal{P}(y,f,u)}{\mathcal{P}(u)}=  \mathcal{P}(y|f) \cdot\mathcal{P}(f|u)\]</div>
<p>and using the conditional probability rules <span class="math notranslate nohighlight">\(P(A,B)=P(A|B) \cdot P(B) \rightarrow P(A|B)=\frac{P(A,B)}{P(B)}\)</span> we can simplify the formula even further:</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y,f|u)=\mathcal{P}(y|f) \cdot \mathcal{P}(f|u)\]</div>
<p>So, what are we looking at? We are looking at the new joint distribution of <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(f\)</span> given the augmented variable space that we have defined. One step closer to the conditional density. In the nature of GP models and Bayesian inference in general, the next step is to see how we obtain the marginal likelihood where we marginalize out the <span class="math notranslate nohighlight">\(f\)</span>’s. In doing so, we obtain the conditional density that we set off to explore:</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y|u)=\int_f \mathcal{P}(y|f) \cdot \mathcal{P}(f|u) \cdot df\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{P}(y|f)\)</span> - Likelihood</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{P}(f|u)\)</span> - Conditional Distribution</p></li>
</ul>
<p>The last step would be to try and see if we can calculate <span class="math notranslate nohighlight">\(\mathcal{P}(y)\)</span> because if we can get a distribution there, then we can actually train our model using marginal likelihood. Unfortunately we are going to see a problem with this line of thinking when we try to do it directly. If I marginalize out the <span class="math notranslate nohighlight">\(u\)</span>’s I get after grouping the terms:</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y)=\int_u \underbrace{\left[\int_f  \mathcal{P}(y|f) \cdot \mathcal{P}(f|u) \cdot df \right]}_{\mathcal{P}(y|u)}  \cdot \mathcal P(u) \cdot du\]</div>
<p>which reduces to:</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y)=\int_u \mathcal{P}(y|u) \cdot \mathcal P(u) \cdot du\]</div>
<p>This looks very similar to the parameter form of the marginal likelihood. And technically speaking this would allow us to make predictions by conditioning on the trained data <span class="math notranslate nohighlight">\(\mathcal P (y*|y)\)</span>. The two important issues are highlighted in that equation alone:</p>
<ol class="simple">
<li><p>We now have the same bottleneck on our parameter for <span class="math notranslate nohighlight">\(u\)</span> as we do for standard Bayesian parametric modeling.</p></li>
<li><p>The computation of <span class="math notranslate nohighlight">\(\mathcal P (y|u)\)</span> is not trivial calculation and we do not get any computational complexity gains trying to do that integral with the prior <span class="math notranslate nohighlight">\(\mathcal P (u)\)</span>.</p></li>
</ol>
</div>
<div class="section" id="variational-bound-on-mathcal-p-y-u">
<h4>Variational Bound on <span class="math notranslate nohighlight">\(\mathcal P (y|u)\)</span><a class="headerlink" href="#variational-bound-on-mathcal-p-y-u" title="Permalink to this headline">¶</a></h4>
<p>We’ve shown the difficulties of actually obtaining the probability density function of <span class="math notranslate nohighlight">\(\mathcal{P}(y)\)</span> but in this section we’re just going to show that we can obtain a lower bound for the conditional density function <span class="math notranslate nohighlight">\(\mathcal{P}(y|u)\)</span></p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y|u)=\int_f \mathcal P (y|f) \cdot \mathcal{P}(f|u) \cdot df\]</div>
<p>I’ll do the 4.5 classic steps in order to arrive at a variational lower bound:</p>
<ol class="simple">
<li><p>Given an <strong>integral problem</strong>, take the <span class="math notranslate nohighlight">\(\log\)</span> of both sides of the function.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\log \mathcal P (y|u) = \log \int_f \mathcal P (y|f) \cdot \mathcal{P}(f|u) \cdot df\]</div>
<ol class="simple">
<li><p>Introduce the variational parameter <span class="math notranslate nohighlight">\(q(f)\)</span> as a <strong>proposal</strong> with the Identity trick.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\log \mathcal P (y|u) = \log \int_f \mathcal P (y|f) \cdot \mathcal{P}(f|u) \cdot \frac{q(f)}{q(f)} \cdot df\]</div>
<ol class="simple">
<li><p>Use Jensen’s inequality for the log function to rearrange the formula to highlight the <strong>importance weight</strong> and provide a bound for <span class="math notranslate nohighlight">\(\mathcal{F}(q)\)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathcal L () = \log \mathcal P (y|u) \geq  \int_f q(f)  \cdot \log \frac{\mathcal P (y|f) \cdot \mathcal{P}(f|u)}{q(f) } \cdot df = \mathcal F (q)\]</div>
<ol class="simple">
<li><p>Rearrange to look like an expectation and KL divergence using targeted <span class="math notranslate nohighlight">\(\log\)</span> rules:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathcal F (q) = \int_f q(f) \cdot \log \mathcal P(y|f) \cdot df - \int_f q(f) \cdot \log \frac{\mathcal{P}(f|u)}{q(f)} \cdot df\]</div>
<ol class="simple">
<li><p>Simplify notation to look like every paper in ML that uses VI to profit and obtain the <strong>variational lower bound</strong>.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathcal F (q) = \mathbb E_{q(f)} \left[ \log \mathcal P(y|f) \right]  - \text{D}_{\text{KL}} \left[ q(f) || \mathcal{P}(f|u)\right]\]</div>
</div>
<div class="section" id="titsias-innovation-et-q-f-mathcal-p-f-u">
<h4>Titsias Innovation: et <span class="math notranslate nohighlight">\(q(f) = \mathcal{P}(f|u)\)</span>.<a class="headerlink" href="#titsias-innovation-et-q-f-mathcal-p-f-u" title="Permalink to this headline">¶</a></h4>
<p>According to Titsias et al. (2009) he looked at what happens if we let <span class="math notranslate nohighlight">\(q(f)=\mathcal P (f|u)\)</span>. For starters, without our criteria, the KL divergence went to zero and the integral we achieved will have one term less.</p>
<div class="math notranslate nohighlight">
\[\log \mathcal P (y|u) \geq \int_f \mathcal P (f|u) \cdot \log \mathcal P(y|f) \cdot df \]</div>
<p>As a thought experiment though, what would happen if we had thee true posterior of <span class="math notranslate nohighlight">\(\mathcal{P}(f|y,u)\)</span> and an approximating density of <span class="math notranslate nohighlight">\(\mathcal{P}(f|u)\)</span>? Well, we can take the <span class="math notranslate nohighlight">\(KL\)</span> divergence of that quantity and we get the following:</p>
<div class="math notranslate nohighlight">
\[\text{D}_{\text{KL}} \left[ q(f) || \mathcal{P}(f|y, u)\right] = \int_u \mathcal P (f|u) \cdot \log \frac{\mathcal P (f|u)}{\mathcal P (f|y,u)} \cdot du\]</div>
<p>According to Neil Lawrence, maximizing the lower bound minimizes the KL divergence between <span class="math notranslate nohighlight">\(\mathcal{P}(f|u)\)</span> and <span class="math notranslate nohighlight">\(\mathcal{P}(f|u)\)</span>. Maximizing the bound will try to find the optimal compression and looks at the information between <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(u\)</span>. He does not that there is no bound and it is an exact bound when <span class="math notranslate nohighlight">\(u=f\)</span>. I believe that’s related to the GPFlow <a class="reference external" href="https://gpflow.readthedocs.io/en/latest/notebooks/vgp_notes.html">derivation</a> of variational GPs <a class="reference external" href="https://gpflow.readthedocs.io/en/develop/_modules/gpflow/models/vgp.html#VGP">implementation</a> but I don’t have more information on this.</p>
<p><strong>Sources</strong>:</p>
<ul class="simple">
<li><p>Deep Gaussian Processes - <a class="reference external" href="http://inverseprobability.com/talks/notes/deep-gaussian-processes.html">MLSS 2019</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1309.6835.pdf">Gaussian Processes for Big Data</a> - Hensman et. al. (2013)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1412.1370.pdf">Nested Variational Compression in Deep Gaussian Processes</a> - Hensman et. al. (2014)</p></li>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v38/hensman15.pdf">Scalable Variational Gaussian Process Classification</a> - Hensman et. al. (2015)</p></li>
</ul>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="elbos">
<h2>ELBOs<a class="headerlink" href="#elbos" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathbf \Sigma = K_{ff} - K_{fu}K_{uu}^{-1}K_{fu}^{\top}\)</span></p>
<div class="section" id="lower-bound">
<h3>Lower Bound<a class="headerlink" href="#lower-bound" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[\mathcal{F}=
\log \mathcal{N} \left(y|0, \tilde{\mathbf K}_{ff} + \sigma_y^2\mathbf I \right) -
\frac{1}{2\sigma_y^2}\text{tr}\left( \mathbf \Sigma\right)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\tilde{\mathbf K}_{ff} = \mathbf{K_{fu}K_{uu}^{-1}K_{fu}^{\top}}\)</span></p>
<ul>
<li><p>Nystrom approximation</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf \Sigma = \mathbf K_{ff} - \tilde{\mathbf K}_{ff}\)</span></p>
<ul>
<li><p>Uncertainty Based Correction</p></li>
</ul>
</li>
</ul>
<div class="section" id="variational-bound-on-mathcal-p-y">
<h4>Variational Bound on <span class="math notranslate nohighlight">\(\mathcal P (y)\)</span><a class="headerlink" href="#variational-bound-on-mathcal-p-y" title="Permalink to this headline">¶</a></h4>
<p>In this scenario, we marginalize out the remaining <span class="math notranslate nohighlight">\(u\)</span>’s and we can get an error bound on the <span class="math notranslate nohighlight">\(\mathcal P(y)\)</span></p>
<div class="math notranslate nohighlight">
\[\mathcal P (y) = \int_u \mathcal P (y|u) \cdot \mathcal P (u|Z) du\]</div>
<p><strong>Source</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1412.1370.pdf">Nested Variational Compression in Deep Gaussian Processes</a> - Hensman et. al. (2014)</p></li>
<li><p>James Hensman - <a class="reference external" href="http://gpss.cc/gpss15/talks/talk_james.pdf">GPSS 2015</a> | <a class="reference external" href="http://www.approximateinference.org/schedule/Hensman2015.pdf">Aweseome Graphical Models</a></p></li>
</ul>
<p>The explicit form of the lower bound <span class="math notranslate nohighlight">\(\mathcal{P}(y)\)</span> for is gives us:</p>
<div class="math notranslate nohighlight">
\[\log \mathcal P (y) \geq \log \mathcal{N} (y|\mathbf{y|K_{fu}^{-1}m, \sigma_y^2I}) - \frac{1}{2\sigma_y^2} \text{tr}\left(  \right)\]</div>
<p><strong>Source</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1412.1370.pdf">Nested Variational Compression in Deep Gaussian Processes</a> - Hensman et. al. (2014)</p></li>
</ul>
</div>
</div>
<div class="section" id="stochastic-variational-inference">
<h3>Stochastic Variational Inference<a class="headerlink" href="#stochastic-variational-inference" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<hr class="docutils" />
<div class="section" id="supplementary-material">
<h2>Supplementary Material<a class="headerlink" href="#supplementary-material" title="Permalink to this headline">¶</a></h2>
<hr class="docutils" />
<div class="section" id="important-formulas">
<h3>Important Formulas<a class="headerlink" href="#important-formulas" title="Permalink to this headline">¶</a></h3>
<p>These formulas come up when we’re looking for clever ways to deal with sparse matrices in GPs. Typically we will have some matrix <span class="math notranslate nohighlight">\(\mathbf K\in \mathbb R^{N\times N}\)</span> which implies we need to calculate the inverse <span class="math notranslate nohighlight">\(\mathbf K^{-1}\)</span> and the determinant <span class="math notranslate nohighlight">\(|\)</span>det <span class="math notranslate nohighlight">\(\mathbf K|\)</span> which both require <span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span>. These formulas below are useful when we want to avoid those computational complexity counts.</p>
<div class="section" id="nystrom-approximation">
<h4>Nystrom Approximation<a class="headerlink" href="#nystrom-approximation" title="Permalink to this headline">¶</a></h4>
<div class="math notranslate nohighlight">
\[\mathbf K_{NN} \approx  \mathbf U_{NM} \mathbf \Lambda_{MM} \mathbf U_{NM}^{\top}\]</div>
</div>
<div class="section" id="sherman-morrison-woodbury-formula">
<h4>Sherman-Morrison-Woodbury Formula<a class="headerlink" href="#sherman-morrison-woodbury-formula" title="Permalink to this headline">¶</a></h4>
<div class="math notranslate nohighlight">
\[(\mathbf K_{NN} + \sigma_y^2 \mathbf I_N)^{-1} \approx \sigma_y^{-2}\mathbf I_N + \sigma_y^{-2} \mathbf U_{NM}\left( \sigma_y^{-2}\mathbf \Lambda_{MM}^{-1} + \mathbf U_{NM}^{\top} \mathbf U_{NM} \right)^{-1}\mathbf U_{NM}^{\top}\]</div>
</div>
<div class="section" id="sylvester-determinant-theorem">
<h4>Sylvester Determinant Theorem<a class="headerlink" href="#sylvester-determinant-theorem" title="Permalink to this headline">¶</a></h4>
<div class="math notranslate nohighlight">
\[\left|\mathbf K_{NN} + \sigma_y^2 \mathbf I_N \right| \approx |\mathbf \Lambda_{MM} | \left|\sigma_y^{2} \mathbf \Lambda_{MM}^{-1} + U_{NM}^{\top} \mathbf U_{NM} \right|\]</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="code-walk-throughs">
<h2>Code Walk-Throughs<a class="headerlink" href="#code-walk-throughs" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p><a class="reference external" href="https://bwengals.github.io/vfe-approximation-for-gaussian-processes-the-gory-details.html">VFE Approximation for GPs, the gory Details</a> | <a class="reference external" href="https://bwengals.github.io/pymc3-fitcvfe-implementation-notes.html">More Notes</a> | <a class="reference external" href="https://bwengals.github.io/inducing-point-methods-to-speed-up-gps.html">Summary of Inducing Point Methods</a></p>
<blockquote>
<div><p>A good walkthrough of the essential equations and how we can implement them from scratch</p>
</div></blockquote>
</li>
<li><p><a class="reference external" href="https://github.com/Alaya-in-Matrix/SparseGP/blob/master/VFE.py">SparseGP - Alaya-in-Matrix</a></p>
<blockquote>
<div><p>Another good walkthrough with everything well defined such that its easy to replicate.</p>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="math-walkthroughs">
<h2>Math Walkthroughs<a class="headerlink" href="#math-walkthroughs" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p><a class="reference external" href="https://www.uv.es/gonmagar/blog/2018/04/19/VariationalFreeEnergy">Gonzalo Blog</a></p>
<blockquote>
<div><p>Step-by-Step Derivations</p>
</div></blockquote>
</li>
</ul>
<div class="section" id="papers">
<h3>Papers<a class="headerlink" href="#papers" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Nystrom Approximation</p>
<ul class="simple">
<li><p><a class="reference external" href="https://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines.pdf">Using Nystrom to Speed Up Kernel Machines</a> - Williams &amp; Seeger (2001)</p></li>
</ul>
</li>
<li><p>Fully Independent Training Conditional (FITC)</p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.gatsby.ucl.ac.uk/~snelson/SPGP_up.pdf">Sparse Gaussian Processes Using Pseudo-Inputs</a> - Snelson and Ghahramani (2006)</p></li>
<li><p><a class="reference external" href="http://www.gatsby.ucl.ac.uk/~snelson/thesis.pdf">Flexible and Efficient GP Models for Machine Learning</a> - Snelson (2007)</p></li>
</ul>
</li>
<li><p>Variational Free Energy (VFE)</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pdfs.semanticscholar.org/9c13/b87b5efb4bb011acc89d90b15f637fa48593.pdf">Variational Learning of Inducing Variables in Sparse GPs</a> - Titsias (2009)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1504.07027.pdf">On Sparse Variational meethods and the KL Divergence between Stochastic Processes</a> - Matthews et. al. (2015)</p></li>
<li><p>Stochastic Variational Inference</p>
<ul>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1309.6835.pdf">Gaussian Processes for Big Data</a> - Hensman et al. (2013)</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="http://quinonero.net/Publications/lazaro-gredilla10a.pdf">Sparse Spectrum GPR</a> - Lazaro-Gredilla et al. (2010)</p>
<ul class="simple">
<li><p>SGD, SVI</p>
<ul>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v37/galb15.html">Improving the GP SS Approximation by Representing Uncertainty in Frequency Inputs</a> - Gal et al. (2015)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v70/pan17a/pan17a.pdf">Prediction under Uncertainty in SSGPs w/ Applications to Filtering and Control</a> - Pan et. al. (2017)</p></li>
<li><p><a class="reference external" href="http://www.jmlr.org/papers/volume18/16-579/16-579.pdf">Variational Fourier Features for GPs</a> - Hensman (2018)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1606.04820.pdf">Understanding Probabilistic Sparse GP Approx</a> - Bauer et. al. (2016)</p>
<blockquote>
<div><p>A good paper which highlights some import differences between the FITC, DTC and VFE. It provides a clear notational differences and also mentions how VFE is a special case of DTC.</p>
</div></blockquote>
</li>
<li><p><a class="reference external" href="http://jmlr.org/papers/volume18/16-603/16-603.pdf">A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation</a> - Bui (2017)</p>
<blockquote>
<div><p>A good summary of all of the methods under one unified framework called the Power Expectation Propagation formula.</p>
</div></blockquote>
</li>
</ul>
<div class="section" id="thesis-explain">
<h4>Thesis Explain<a class="headerlink" href="#thesis-explain" title="Permalink to this headline">¶</a></h4>
<p>Often times the papers that people publish in conferences in Journals don’t have enough information in them. Sometimes it’s really difficult to go through some of the mathematics that people put  in their articles especially with cryptic explanations like “it’s easy to show that…” or “trivially it can be shown that…”. For most of us it’s not easy nor is it trivial. So I’ve included a few thesis that help to explain some of the finer details. I’ve arranged them in order starting from the easiest to the most difficult.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/HildoBijl/GPRT">GPR Techniques</a> - Bijl (2016)</p>
<ul>
<li><p>Chapter V - Noisy Input GPR</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://lib.ugent.be/fulltxt/RUG01/002/367/115/RUG01-002367115_2017_0001_AC.pdf">Non-Stationary Surrogate Modeling with Deep Gaussian Processes</a> - Dutordoir (2016)</p>
<ul>
<li><p>Chapter IV - Finding Uncertain Patterns in GPs</p></li>
</ul>
</li>
<li><p><a class="reference external" href="http://mlg.eng.cam.ac.uk/pub/pdf/Mch14.pdf">Nonlinear Modeling and Control using GPs</a> - McHutchon (2014)</p>
<ul>
<li><p>Chapter II - GP w/ Input Noise (NIGP)</p></li>
</ul>
</li>
<li><p><a class="reference external" href="http://etheses.whiterose.ac.uk/9968/1/Damianou_Thesis.pdf">Deep GPs and Variational Propagation of Uncertainty</a> - Damianou (2015)</p>
<ul>
<li><p>Chapter IV - Uncertain Inputs in Variational GPs</p></li>
<li><p>Chapter II (2.1) - Lit Review</p></li>
</ul>
</li>
<li><p><a class="reference external" href="http://etheses.whiterose.ac.uk/18492/1/MaxZwiesseleThesis.pdf">Bringing Models to the Domain: Deploying Gaussian Processes in the Biological Sciences</a> - Zwießele (2017)</p>
<ul>
<li><p>Chapter II (2.4, 2.5) - Sparse GPs, Variational Bayesian GPLVM</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="presentations">
<h3>Presentations<a class="headerlink" href="#presentations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="http://www2.aueb.gr/users/mtitsias/papers/titsiasNipsVar14.pdf">Variational Inference for Gaussian and Determinantal Point Processes</a> - Titsias (2014)</p></li>
</ul>
</div>
<div class="section" id="notes">
<h3>Notes<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="http://mlg.eng.cam.ac.uk/thang/docs/talks/rcc_vargp.pdf">On the paper: Variational Learning of Inducing Variables in Sparse Gaussian Processees</a> - Bui and Turner (2014)</p></li>
</ul>
</div>
<div class="section" id="blogs">
<h3>Blogs<a class="headerlink" href="#blogs" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://gonzmg88.github.io/blog/2018/04/19/VariationalFreeEnergy">Variational Free Energy for Sparse GPs</a> - Gonzalo</p></li>
<li><p><a class="reference external" href="https://github.com/Alaya-in-Matrix/SparseGP">https://github.com/Alaya-in-Matrix/SparseGP</a></p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/notes/gps"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By J. Emmanuel Johnson<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>