{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M4EJ9PyKoMdb"
   },
   "source": [
    "# Kernel Derivatives\n",
    "\n",
    "**Linear Operators and Stochastic Partial Differential Equations in GPR** - Simo Särkkä - [PDF](https://users.aalto.fi/~ssarkka/pub/spde.pdf)\n",
    "\n",
    "> Expresses derivatives of GPs as operators\n",
    "\n",
    "[**Demo Colab Notebook**](https://colab.research.google.com/drive/1pbb0qlypJCqPTN_cu2GEkkKLNXCYO9F2)\n",
    "\n",
    "He looks at ths special case where we have a GP with a mean function zero and a covariance matrix $K$ defined as:\n",
    "$$\n",
    "\\mathbb{E}[f(\\mathbf{x})f^\\top(\\mathbf{x'})] = K_{ff}(\\mathbf{x,x'})\n",
    "$$\n",
    "So in GP terminology:\n",
    "$$\n",
    "f(\\mathbf(x)) \\sim \\mathcal{GP}(\\mathbf{0}, K_{ff}(\\mathbf{x,x'}))\n",
    "$$\n",
    "We use the rulse for linear transformations of GPs to obtain the different transformations of the kernel matrix. \n",
    "\n",
    "Let's define the notation for the derivative of a kernel matrix. Let $g(\\cdot)$ be the derivative operator on a function $f(\\cdot)$. So:\n",
    "$$\n",
    "g(\\mathbf{x}) = \\mathcal{L}_x f(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "So now, we want to define the cross operators between the derivative $g(\\cdot)$ and the function $f(\\cdot)$. \n",
    "\n",
    "**Example**: He draws a distinction between the two operators with an example of how this works in practice. So let's take the linear operator $\\mathcal{L}_{x}=(1, \\frac{\\partial}{\\partial x})$. This operator:\n",
    "\n",
    "* acts on a scalar GP $f(x)$\n",
    "* a scalar input $x$ \n",
    "* a covariance function $k_{ff}(x,x')$ \n",
    "* outputs a scalar value $y$\n",
    "\n",
    "\n",
    "\n",
    "We can get the following transformations:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "K_{gf}(\\mathbf{x,x'})\n",
    "&= \\mathcal{L}_x f(\\mathbf{x}) f(\\mathbf{x}) = \\mathcal{L}_xK_{ff}(\\mathbf{x,x'}) \\\\\n",
    "K_{fg}(\\mathbf{x,x'})\n",
    "&= f(\\mathbf{x}) f(\\mathbf{x'}) \\mathcal{L}_{x'} = K_{ff}(\\mathbf{x,x'})\\mathcal{L}_{x'} \\\\\n",
    "K_{gg}(\\mathbf{x,x'})\n",
    "&= \\mathcal{L}_x f(\\mathbf{x}) f(\\mathbf{x'}) \\mathcal{L}_{x'}\n",
    "= \\mathcal{L}_xK_{ff}(\\mathbf{x,x'})\\mathcal{L}_{x'}^\\top \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "kV8wRfKbYE92"
   },
   "outputs": [],
   "source": [
    "#@title Packages\n",
    "import functools\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as onp\n",
    "from sklearn.metrics.pairwise import rbf_kernel as rbf_sklearn\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(['seaborn-paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Q1X1grSrH11u"
   },
   "outputs": [],
   "source": [
    "#@title Plot Functions\n",
    "\n",
    "def plot_kernel_mat(K):\n",
    "    # plot\n",
    "    plt.figure()\n",
    "    plt.imshow(K, cmap='Reds')\n",
    "    plt.title(r'$K_{ff}$, (rbf)', fontsize=20, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "z0-afZT_YZqo",
    "outputId": "5b7ff89b-70a6-48d5-8522-233b64098fa4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:116: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "#@title Data\n",
    "\n",
    "def get_1d_data(N=30, sigma_inputs=0.15, sigma_obs=0.15, N_test=400):\n",
    "    onp.random.seed(0)\n",
    "    X = jnp.linspace(-10, 10, N)\n",
    "    # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X)\n",
    "    Y = jnp.sin(1.0 * jnp.pi / 1.6 * jnp.cos(5 + .5 * X))\n",
    "    Y += sigma_obs * onp.random.randn(N)\n",
    "    X += sigma_inputs * onp.random.randn(N)\n",
    "    Y -= jnp.mean(Y)\n",
    "    Y /= jnp.std(Y)\n",
    "\n",
    "\n",
    "\n",
    "    X_test = jnp.linspace(-11, 11, N_test) \n",
    "    X_test += sigma_inputs * onp.random.randn(N_test)\n",
    "\n",
    "    X = X[:, None]\n",
    "    X_test = X[:, None]\n",
    "\n",
    "    assert X.shape == (N,1)\n",
    "    assert Y.shape == (N,)\n",
    "\n",
    "    return X, Y, X_test\n",
    "\n",
    "def get_2d_data(N=30, sigma_obs=0.15, N_test=400):\n",
    "    onp.random.seed(0)\n",
    "    X1 = jnp.linspace(-10, 10, N)\n",
    "    X2 = jnp.linspace(-5, 2, N)\n",
    "    # Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X)\n",
    "    Y = jnp.sin(1.0 * jnp.pi / 1.6 * jnp.cos(5 + .5 * X1)) + jnp.exp(X2)\n",
    "    Y += sigma_obs * onp.random.randn(N)\n",
    "    Y -= jnp.mean(Y)\n",
    "    Y /= jnp.std(Y)\n",
    "\n",
    "\n",
    "\n",
    "    X1_test = jnp.linspace(-11, 11, N_test)\n",
    "    X2_test = jnp.linspace(-6, 4, N_test) \n",
    "\n",
    "    X = jnp.vstack((X1,X2)).T\n",
    "    X_test = jnp.vstack((X1_test,X2_test)).T\n",
    "\n",
    "    assert X.shape == (N,2)\n",
    "    assert Y.shape == (N,)\n",
    "\n",
    "    return X, Y, X_test\n",
    "\n",
    "# Get Data\n",
    "X, Y, X_test = get_1d_data(100, sigma_inputs=0.0, sigma_obs=0.1, N_test=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DbQXQEp6yWIq"
   },
   "source": [
    "## Kernel Function\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{dist} = \\sum_{i=1}^D (\\mathbf{x_i} - \\mathbf{y_i})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fz32W8cRyXNi"
   },
   "outputs": [],
   "source": [
    "#@title Kernel Functions\n",
    "\n",
    "# Squared Euclidean Distance Formula\n",
    "@jax.jit\n",
    "def sqeuclidean_distance(x, y):\n",
    "    return jnp.sum((x-y)**2)\n",
    "\n",
    "# RBF Kernel\n",
    "@jax.jit\n",
    "def rbf_kernel(params, x, y):\n",
    "    return jnp.exp( - params['gamma'] * sqeuclidean_distance(x, y))\n",
    "    \n",
    "# Covariance Matrix\n",
    "def covariance_matrix(kernel_func, x, y):\n",
    "    mapx1 = jax.vmap(lambda x, y: kernel_func(x, y), in_axes=(0, None), out_axes=0)\n",
    "    mapx2 = jax.vmap(lambda x, y: mapx1(x, y), in_axes=(None, 0), out_axes=1)\n",
    "    return mapx2(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dp1oE7ijH12C"
   },
   "source": [
    "#### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KBilt55hZA8T",
    "outputId": "fc2c9a82-521f-4545-9935-cbf3feeced73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ulrnuXvOH12E",
    "outputId": "c4c778c9-503c-4f53-a4c8-d436e795861a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) (1, 2)\n"
     ]
    }
   ],
   "source": [
    "X, Y, X_test = get_2d_data(10, sigma_obs=0.1)\n",
    "\n",
    "test_X = X[:1, :]\n",
    "test_Y = X[:1, :]\n",
    "\n",
    "rbf_x_sk = rbf_sklearn(\n",
    "    onp.array(test_X.reshape(1, -1)), \n",
    "    onp.array(test_Y.reshape(1, -1)), \n",
    "    gamma=1.0\n",
    ")\n",
    "print(rbf_x_sk.shape, test_X.shape)\n",
    "\n",
    "params = {'gamma': 1.0, 'var_f': 1.0}\n",
    "gamma = 1.0\n",
    "rbf_k_ = functools.partial(rbf_kernel, params)\n",
    "rbf_x = rbf_k_(\n",
    "    test_X.squeeze(), \n",
    "    test_Y.squeeze()\n",
    ")\n",
    "\n",
    "onp.testing.assert_array_almost_equal(onp.array(rbf_x), rbf_x_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27T3BlaXH12Q"
   },
   "source": [
    "### Kernel Matrix\n",
    "\n",
    "We defined all of our functions above with only dimensions in mind, not the number of samples or the batch size. So we need to account for that. So if we wanted to calculate the kernel matrix, we would have to loop through all of the samples and calculate the products individually, which would take a long time; especially for large amounts of data. \n",
    "\n",
    "> Avoid Loops at all cost in python...\n",
    "\n",
    "Fortunately, Jax has this incredible function `vmap` which handles batching automatically at apparently, no extra cost. So we can write our functions to account for vectors without having to care about the batch size and then use the `vmap` function to essentially \"vectorize\" our functions. It essentially allows us to take a product between a matrix and a sample or two vectors of multiple samples. Let's go through an example of how we can construct our kernel matrix.\n",
    "\n",
    "1. We need to map all points with one vector to another.\n",
    "\n",
    "We're going to take a single sample from $X'$ and take the rbf kernel between it and all of $X$. So:\n",
    "\n",
    "$$\\text{vmap}_f(\\mathbf{X}, \\mathbf{x})$$\n",
    "\n",
    "where $X\\in \\mathbb{R}^{N \\times D}$ is a matrix and $\\mathbf{x} \\in \\mathbb{R}^{D}$ is a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oz2ZhIQ3H12T"
   },
   "outputs": [],
   "source": [
    "# Gram Matrix\n",
    "def gram(func, x, y):\n",
    "    return jax.vmap(lambda x1: jax.vmap(lambda y1: func(x1, y1))(y))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uI2X1HPoH12c"
   },
   "outputs": [],
   "source": [
    "# map function 1\n",
    "mapx1 = jax.vmap(lambda x, y: rbf_kernel(params, x, y), in_axes=(0, None), out_axes=0)\n",
    "\n",
    "# test the mapping\n",
    "x1_mapped = mapx1(X, X[0, :])\n",
    "\n",
    "# Check output shapes, # of dimensions\n",
    "assert x1_mapped.shape[0] == X.shape[0]   \n",
    "assert jnp.ndim(x1_mapped) == 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z24wyM-RH12m"
   },
   "source": [
    "This that's good: we have an array of size $N$. So we've effectively mapped all points from one array to the other. \n",
    "\n",
    "So now we can do another vector mapping which allows us to take all samples of $X'$ and map them against all samples of $X$. So it'll be a `vmap` of a `vmap`. Then we'll get the $N\\times N$ kernel matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DR8A8z7nH12n"
   },
   "outputs": [],
   "source": [
    "mapx2 = jax.vmap(lambda x, y: mapx1(x, y), in_axes=(None, 0), out_axes=1)\n",
    "\n",
    "K = mapx2(X, X)\n",
    "\n",
    "# Check output shapes, # of dimensions\n",
    "assert K.shape[0] == X.shape[0], X.shape[0]   \n",
    "assert jnp.ndim(K) == 2     \n",
    "\n",
    "rbf_x_sk = rbf_sklearn(X, X, 1.0)\n",
    "\n",
    "\n",
    "onp.testing.assert_array_almost_equal(onp.array(rbf_x_sk), K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8un_tmKhH12s"
   },
   "source": [
    "So great! We now have our kernel matrix. Let's plot it and check to see if it matches the manually constructed kernel matrix.\n",
    "\n",
    "Great! We have a vectorized kernel function and we were still able to construct our functions in terms of vectors only! This is nice for me personally because I've always struggled with understanding some of the coding when trying to deal with samples/batch-sizes. Most pseudo-code is written in vector format so paper $\\rightarrow$ has always been a painful transition for me. So now, let's wrap this in a nice function so that we can finish \"wrap up\" this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "EJmi7gqwyY5U",
    "outputId": "8b367244-e5b8-4c71-fa44-25db5b9b2bb0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x316.8 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, Y, X_test = get_2d_data(10, sigma_obs=0.1)\n",
    "\n",
    "test_X = X.copy()#[:2, :]\n",
    "test_Y = X.copy() #[:2, :]\n",
    "\n",
    "rbf_x_sk = rbf_sklearn(\n",
    "    onp.array(test_X), \n",
    "    onp.array(test_Y), \n",
    "    gamma=1.0\n",
    ")\n",
    "\n",
    "params = {'gamma': 1.0, 'var_f': 1.0}\n",
    "rbf_k_ = functools.partial(rbf_kernel, params)\n",
    "rbf_x = covariance_matrix(\n",
    "    rbf_k_,\n",
    "    test_X, \n",
    "    test_Y\n",
    ")\n",
    "\n",
    "onp.testing.assert_array_almost_equal(onp.array(rbf_x), rbf_x_sk)\n",
    "\n",
    "plot_kernel_mat(rbf_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "L81uQDJmY8Ig"
   },
   "outputs": [],
   "source": [
    "#@title Tests\n",
    "\n",
    "kx = rbf_kernel(params, X[0], X[0])\n",
    "\n",
    "# check, the output should be 1.0\n",
    "assert kx == 1.0, f\"Output: {kx}\"\n",
    "\n",
    "kx = rbf_kernel(params, X[0], X[1])\n",
    "\n",
    "# check, the output should NOT be 1.0\n",
    "assert kx != 1.0, f\"Output: {kx}\"\n",
    "\n",
    "\n",
    "# dk_dx = drbf_kernel(gamma, X[0], X[0])\n",
    "\n",
    "# # check, the output should be 0.0\n",
    "# assert dk_dx == 0.0, f\"Output: {dk_dx}\"\n",
    "\n",
    "# dk_dx = drbf_kernel(gamma, X[0], X[1])\n",
    "\n",
    "# # check, the output should NOT be 0.0\n",
    "# assert dk_dx != 0.0, f\"Output: {dk_dx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1vj57Z7H123"
   },
   "outputs": [],
   "source": [
    "#@title Speed Test\n",
    "    \n",
    "# Covariance Matrix\n",
    "def covariance_matrix(kernel_func, x, y):\n",
    "    mapx1 = jax.vmap(lambda x, y: kernel_func(x, y), in_axes=(0, None), out_axes=0)\n",
    "    mapx2 = jax.vmap(lambda x, y: mapx1(x, y), in_axes=(None, 0), out_axes=1)\n",
    "    return mapx2(x, y)\n",
    "\n",
    "def gram(func, x, y):\n",
    "    return jax.vmap(lambda x1: jax.vmap(lambda y1: func(x1, y1))(x))(y)\n",
    "\n",
    "rbf_K = functools.partial(rbf_kernel, params)\n",
    "rbf_cov =  jax.jit(functools.partial(covariance_matrix, rbf_K))\n",
    "rbf_x = rbf_cov(test_X,  test_Y)\n",
    "\n",
    "\n",
    "rbf_cov2 =  jax.jit(functools.partial(gram, rbf_K))\n",
    "rbf_x2 = rbf_cov2(test_X,  test_Y)\n",
    "\n",
    "onp.testing.assert_array_almost_equal(onp.array(rbf_x), onp.array(rbf_x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "YsoavH2zH128",
    "outputId": "8e5c2758-517c-4726-9114-1486e3946114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 201 µs per loop\n",
      "10000 loops, best of 3: 203 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit _ = rbf_cov(test_X,  test_Y)\n",
    "%timeit _ = rbf_cov2(test_X,  test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9OdeHDkWFb8Z"
   },
   "source": [
    "Seems like they are comparable and there is no real difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wK14aQKmDbi"
   },
   "source": [
    "## 1. Cross-Covariance Term - 1st Derivative\n",
    "\n",
    "\n",
    "We can calculate the cross-covariance term $K_{fg}(\\mathbf{x,x})$. We apply the following operation\n",
    "\n",
    "$$\n",
    "K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})(1, \\frac{\\partial}{\\partial x'})\n",
    "$$\n",
    "If we multiply the terms across, we get:\n",
    "$$\n",
    "K_{fg}(x,x') = k_{ff}(\\mathbf{x,x'})\\frac{\\partial k_{ff}(\\mathbf{x,x'})}{\\partial x'}\n",
    "$$\n",
    "\n",
    "For the RBF Kernel, it's this:\n",
    "\n",
    "$$\\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y)$$\n",
    "\n",
    "Note: I did the derivations from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pG0PfBS2H13B"
   },
   "source": [
    "### Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsuI1c05H13C"
   },
   "outputs": [],
   "source": [
    "X, Y, X_test = get_1d_data(10, sigma_obs=0.1)\n",
    "\n",
    "test_X = X[0:1, :]\n",
    "test_Y = X[1:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EningAbiuJf-"
   },
   "source": [
    "#### From Scratch\n",
    "\n",
    "From scratch, we're going to be using loops. There are more efficient ways to implement this but it's harder to mess up loops and it's also clearer what's going on. Tricks with broadcasting are often hard to read and very hard to interpret because of the change in dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWWKg0gaH13J"
   },
   "outputs": [],
   "source": [
    "def drbf_kernel_scratch(gamma, X, Y):\n",
    "    # initialize matrix\n",
    "    dK_fg_ = onp.empty(X.shape[-1])\n",
    "    \n",
    "    constant = - 2 * gamma\n",
    "    \n",
    "    # calculate kernel matrix w. sklearn kernel\n",
    "    k_val = rbf_sklearn(onp.array(X), onp.array(Y), gamma=gamma)\n",
    "    \n",
    "    # loop through features/dimensions\n",
    "    for idim in range(X.shape[1]):\n",
    "        \n",
    "        x_val = X[:, idim] - Y[:, idim]\n",
    "\n",
    "        dK_fg_[idim] = constant * k_val *  x_val \n",
    "    return dK_fg_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B64ntGq5H13R"
   },
   "outputs": [],
   "source": [
    "dK_fg_ = drbf_kernel_scratch(gamma, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5e0gkCyH13c"
   },
   "source": [
    "#### Jax\n",
    "\n",
    "For Jax, we're going to use the built-in Jacobian function. **Note**: this function only allows us take the derivative of functions that output a scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sNY8sUfQZSC0",
    "outputId": "ef427675-be5e-493e-a4c2-56e47a38856f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(1., dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel(params, test_X[0,:], test_Y[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9C1VhDC9Zbqj",
    "outputId": "8ef798e9-686d-448d-e7a0-e7a7f5bf550c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function jax.api.jacrev.<locals>.jacfun>"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drbf_kernel_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qnn_aeQWH13d"
   },
   "outputs": [],
   "source": [
    "# define the cross operator K_fg(x, y), dK wrt x\n",
    "drbf_kernel_fg = jax.jacobian(rbf_kernel, argnums=(1))\n",
    "\n",
    "# calculate for a single sample\n",
    "dK_fg = drbf_kernel_fg(params, test_X[0,:], test_Y[0,:])\n",
    "\n",
    "# check theyre the same\n",
    "assert dK_fg.all() == dK_fg_.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPUBAZVaH13h"
   },
   "source": [
    "### Multiple Dimensions\n",
    "\n",
    "Now, we have the same problem but for a vector $\\mathbf{x}$ instead of a single sample $x$. In this example, $\\mathbf{x}$ has 2 features, $\\mathbf{x} \\in \\mathbb{R}^2$. We're still going to do it for a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bHr7t1UH13h"
   },
   "outputs": [],
   "source": [
    "# generate some data\n",
    "X, Y, X_test = get_2d_data(10, sigma_obs=0.1)\n",
    "\n",
    "# extract a single sample\n",
    "test_X = X[0:1, :]\n",
    "test_Y = X[1:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGUVnhnUH13l"
   },
   "source": [
    "#### From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8VoIRBvH13u"
   },
   "outputs": [],
   "source": [
    "dK_fg_ = drbf_kernel_scratch(gamma, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBLKyBGSH133"
   },
   "source": [
    "#### Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0kpiYQFH134"
   },
   "outputs": [],
   "source": [
    "# define the cross operator K_fg(x, y), dK wrt x\n",
    "drbf_kernel_fg = jax.jacobian(rbf_kernel, argnums=(1))\n",
    "\n",
    "# calculate for a single sample\n",
    "dK_fg = drbf_kernel_fg(params, test_X[0,:], test_Y[0,:])\n",
    "\n",
    "# check theyre the same\n",
    "assert dK_fg.all() == dK_fg_.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEeJYOqlH14Q"
   },
   "source": [
    "### Multiple Samples (Batches)\n",
    "\n",
    "Now, we're going to input a matrix $\\mathbf{X}$ which are stacked samples of multiple features. So $\\mathbf{X} \\in \\mathbb{R}^{N\\times D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dda090cfH14Q"
   },
   "outputs": [],
   "source": [
    "X, Y, X_test = get_2d_data(10, sigma_obs=0.1)\n",
    "\n",
    "test_X = X\n",
    "test_Y = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JaPsoMiOH14T"
   },
   "source": [
    "#### From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DwIP89p_H14T"
   },
   "outputs": [],
   "source": [
    "dK_fg_ = onp.empty((test_X.shape[0], test_X.shape[0], test_X.shape[1]))\n",
    "\n",
    "for i in range(test_X.shape[0]):\n",
    "    for j in range(test_Y.shape[0]):\n",
    "        \n",
    "        dK_fg_[i, j, :] = drbf_kernel_scratch(gamma, onp.array(test_X[i, :]).reshape(1,-1), onp.array(test_Y[j, :]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRGaJrXMH14a"
   },
   "source": [
    "#### Jax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIyghaoDH14a"
   },
   "outputs": [],
   "source": [
    "# define the cross operator K_fg(x, y), dK wrt x\n",
    "drbf_kernel_fg = jax.jacobian(rbf_kernel, argnums=(1))\n",
    "\n",
    "K_func = functools.partial(drbf_kernel_fg, params)\n",
    "\n",
    "# calculate kernel matrix\n",
    "dK_fg = gram(K_func, test_X, test_Y)\n",
    "\n",
    "\n",
    "# check\n",
    "onp.testing.assert_array_almost_equal(onp.array(dK_fg), dK_fg_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XafmwZfFH14d"
   },
   "source": [
    "## 2. Cross-Covariance Term - 2nd Derivative\n",
    "\n",
    "Recall the 1st derivative is:\n",
    "\n",
    "$$\\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y)$$\n",
    "\n",
    "So now we repeat. First we decompose the function using the product rule:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial^2 k(x,y)}{\\partial x^{j^2}} &=\n",
    "-2 \\gamma (x^j - y^j) \\frac{\\partial }{\\partial x^j} k(x,y) + k(x,y) \\frac{\\partial }{\\partial x^j} \\left[ -2 \\gamma (x^j - y^j) \\right]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The first term is basically the 1st Derivative squared and the 2nd term is a constant. So after applying the derivative and simplifying, we get:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial^2 k(x,y)}{\\partial x^{j^2}} &=\n",
    "4 \\gamma^2 (x^j - y^j)^2 k(x,y) -2 \\gamma k(x,y)\\\\\n",
    "&=\n",
    "\\left[ 4\\gamma^2(x^j - y^j)^2 - 2\\gamma\\right] k(\\mathbf{x}, \\mathbf{y}) \\\\\n",
    "&=\n",
    "2 \\gamma \\left[ 2\\gamma(x^j - y^j)^2 - 1\\right] k(\\mathbf{x}, \\mathbf{y}) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjBwd-E1H14e"
   },
   "source": [
    "#### From Scratch\n",
    "\n",
    "Recall, this is a Jacobian so we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ug2p0AiYH14g"
   },
   "outputs": [],
   "source": [
    "def d2rbf_kernel_scratch_jac(gamma, X, Y):\n",
    "    d2K_fg2_ = onp.empty(X.shape[-1])\n",
    "    \n",
    "    constant = 2 * gamma\n",
    "    \n",
    "    k_val = rbf_sklearn(onp.array(X), onp.array(Y), gamma=gamma)\n",
    "    \n",
    "    for idim in range(X.shape[1]):\n",
    "        \n",
    "        # compute the xterm: 2 gamma (xj - yj)^2\n",
    "        x_val = constant * (X[:, idim] - Y[:, idim]) ** 2 - 1\n",
    "\n",
    "        # compute the derivative term\n",
    "        d2K_fg2_[idim] = constant * x_val * k_val \n",
    "    return d2K_fg2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNMXJMTiH14k"
   },
   "outputs": [],
   "source": [
    "# initialize matrix\n",
    "d2K_fg2_ = onp.empty((test_X.shape[0], test_X.shape[0], test_X.shape[1]))\n",
    "\n",
    "for i in range(test_X.shape[0]):\n",
    "    for j in range(test_Y.shape[0]):\n",
    "        \n",
    "        d2K_fg2_[i, j, :] = d2rbf_kernel_scratch_jac(gamma, onp.array(test_X[i, :]).reshape(1,-1), onp.array(test_Y[j, :]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7djnpVjbH14q"
   },
   "source": [
    "#### Jax\n",
    "\n",
    "So with jax, we're computing the hessian so we'll get a matrix of size $N \\times N \\times D \\times D$. So the 2nd derivative is just the diagonal terms of $D\\times D$ part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5Alr5PfxH14r",
    "outputId": "014efb7d-b1e6-455f-861b-f3b0f8324860"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the cross operator K_fg(x, y), dK wrt x\n",
    "dK_fg_func = jax.hessian(rbf_kernel, argnums=(1))\n",
    "\n",
    "# fix params for kernel function\n",
    "K_func = functools.partial(dK_fg_func, params)\n",
    "\n",
    "# calculate kernel matrix\n",
    "d2K_fg2 = covariance_matrix(K_func, test_X, test_Y)\n",
    "\n",
    "# get the diagonal terms\n",
    "d2K_fg2 = jnp.diagonal(d2K_fg2, axis1=2, axis2=3)\n",
    "\n",
    "d2K_fg2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSqNQKkhH14x"
   },
   "outputs": [],
   "source": [
    "onp.testing.assert_array_almost_equal(onp.array(d2K_fg2), d2K_fg2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUAYrT0-Oefh"
   },
   "source": [
    "Awesome, they're the same! So that gives me hope!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NKh92cMvH142"
   },
   "source": [
    "## 3. Cross-Covariance Term - 2nd Derivative (Partial Derivatives)\n",
    "\n",
    "Recall the 1st derivative is:\n",
    "\n",
    "$$\\frac{\\partial k(x,y)}{\\partial x^j}=-2 \\gamma (x^j - y^j) k(x,y)$$\n",
    "\n",
    "So now we repeat. First we decompose the function using the product rule. But this time, we need to do the product rule first w.r.t. $x^j$ and then w.r.t. $y^k$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial^2 k(x,y)}{\\partial x^j y^k} &=\n",
    "-2 \\gamma (x^j - y^j) \\frac{\\partial }{\\partial y^k} k(x,y) + k(x,y) \\frac{\\partial }{\\partial y^k} \\left[ -2 \\gamma (x^j - y^j) \\right]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So now let's start expanding and collapsing terms:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial^2 k(x,y)}{\\partial x^j y^k} &=\n",
    "4 \\gamma^2 (x^j - y^j)(x^k - y^k) k(x,y) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The second term should go to zero and the first term is the same except it has different dimensions (w.r.t. $y$ instead of $x$).\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 k(x,y)}{\\partial x^j \\partial y^k} =\n",
    "4 \\gamma^2 (x^k - y^k)(x^j - y^j) k(\\mathbf{x}, \\mathbf{y})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Tfm_U5bH143"
   },
   "source": [
    "#### From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGkMZ5CaH145"
   },
   "outputs": [],
   "source": [
    "def d2rbf_kernel_scratch_hessian(gamma, X, Y):\n",
    "    d2K_fg2_ = onp.empty((X.shape[-1], X.shape[-1]))\n",
    "    \n",
    "    constant = 2 * gamma\n",
    "    constant_sq = constant ** 2\n",
    "    \n",
    "    k_val = rbf_sklearn(onp.array(X), onp.array(Y), gamma=gamma)\n",
    "    \n",
    "    for idim in range(X.shape[1]):\n",
    "        for jdim in range(X.shape[1]):\n",
    "        \n",
    "            # x_val = constant * (1 - constant * (X[:, idim] - Y[:, idim]) * (X[:, jdim] - Y[:, jdim]))# - constant\n",
    "            x_val = constant_sq * (X[:, idim] - Y[:, idim]) * (X[:, jdim] - Y[:, jdim])\n",
    "\n",
    "            d2K_fg2_[idim, jdim] = k_val *  x_val \n",
    "    return d2K_fg2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ootTKoE_H14-"
   },
   "outputs": [],
   "source": [
    "d2K_fg2_ = onp.empty((test_X.shape[0], test_X.shape[0], test_X.shape[1], test_X.shape[1]))\n",
    "\n",
    "for i in range(test_X.shape[0]):\n",
    "    for j in range(test_Y.shape[0]):\n",
    "        \n",
    "        d2K_fg2_[i, j, ...] = d2rbf_kernel_scratch_hessian(gamma, onp.array(test_X[i, :]).reshape(1,-1), onp.array(test_Y[j, :]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r7qr_e1yH15B"
   },
   "source": [
    "#### Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ieiIO2anH15B",
    "outputId": "29d03860-c301-4fd4-c692-87b8a1072b1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 2, 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the cross operator K_fg(x, y), dK wrt x\n",
    "dK_fg_func = jax.hessian(rbf_kernel, argnums=(1))\n",
    "\n",
    "K_func = functools.partial(dK_fg_func, params)\n",
    "d2K_fg2 = covariance_matrix(K_func, test_X, test_Y)\n",
    "\n",
    "d2K_fg2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOr9oGtoH15G"
   },
   "outputs": [],
   "source": [
    "onp.testing.assert_array_almost_equal(onp.array(onp.diagonal(d2K_fg2, axis1=2, axis2=3 )), jnp.diagonal(d2K_fg2, axis1=2, axis2=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x8ZrRh6YO5ao"
   },
   "source": [
    "So this is good. The diagonal terms are correct but the off-diagonal entries are incorrect. I'm not entirely sure how to fix this. I can't point to the part of the equation where you would actually calculate the off-diagonal entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "colab_type": "code",
    "id": "MpLTLHZ3H15K",
    "outputId": "d52483b8-2be9-4ecd-8d6c-0489ff38fc7a"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-92393160b45e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2K_fg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2K_fg2_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_almost_equal\u001b[0;34m(x, y, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n\u001b[1;32m   1046\u001b[0m              \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Arrays are not almost equal to %d decimals'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m              precision=decimal)\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    844\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 4 decimals\n\nMismatched elements: 92 / 400 (23%)\nMax absolute difference: 2.\nMax relative difference: 2.00000007\n x: array([[[[-2.0000e+00,  0.0000e+00],\n         [ 0.0000e+00, -2.0000e+00]],\n...\n y: array([[[[-0.0000e+00, -0.0000e+00],\n         [-0.0000e+00, -0.0000e+00]],\n..."
     ]
    }
   ],
   "source": [
    "onp.testing.assert_array_almost_equal(onp.array(d2K_fg2), d2K_fg2_, decimal=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tgz5kBQ1H15P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "kernel_derivatives.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-jaxkern]",
   "language": "python",
   "name": "conda-env-.conda-jaxkern-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
