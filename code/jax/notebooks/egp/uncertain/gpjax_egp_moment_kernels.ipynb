{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hecnCJD1trHy"
   },
   "source": [
    "# Numpyro Jax PlayGround\n",
    "\n",
    "My starting notebook where I install all of the necessary libraries and load some easy 1D/2D Regression data to play around with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgSMWFaNtnjB"
   },
   "outputs": [],
   "source": [
    "# @title Install\n",
    "try:\n",
    "    import sys, os\n",
    "    from pyprojroot import here\n",
    "\n",
    "    # spyder up to find the root\n",
    "    root = here(project_files=[\".root\"])\n",
    "\n",
    "    # append to path\n",
    "    sys.path.append(str(root))\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "    # #@title Install Packages\n",
    "    # %%capture\n",
    "    # !pip install pyprojroot jax jaxlib chex numpyro flax distrax gpjax numpy pandas seaborn matplotlib corner loguru nb_black sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5TKBjGbumu6"
   },
   "outputs": [],
   "source": [
    "# @title Load Packages\n",
    "# TYPE HINTS\n",
    "from typing import Tuple, Optional, Dict, Callable, Union\n",
    "\n",
    "# JAX SETTINGS\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from jax.config import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "# JAX UTILITY LIBRARIES\n",
    "import chex\n",
    "\n",
    "# NUMPYRO SETTINGS\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer.autoguide import AutoDiagonalNormal\n",
    "from numpyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "# NUMPY SETTINGS\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# MATPLOTLIB Settings\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# SEABORN SETTINGS\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(context=\"poster\", font_scale=0.8)\n",
    "\n",
    "# PANDAS SETTINGS\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "# LOGGING SETTINGS\n",
    "import loguru\n",
    "\n",
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGCkNw5OT9Mm"
   },
   "source": [
    "## Demo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfY3vhXGT-vb"
   },
   "outputs": [],
   "source": [
    "from src.data import regression_near_square\n",
    "\n",
    "(\n",
    "    Xtrain,\n",
    "    Xtrain_noise,\n",
    "    ytrain,\n",
    "    xtest,\n",
    "    xtest_noise,\n",
    "    ytest,\n",
    "    ytest_noise,\n",
    ") = regression_near_square(\n",
    "    n_train=60, n_test=1_000, x_noise=0.5, y_noise=0.01, seed=123, buffer=0.3\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(Xtrain, ytrain, color=\"tab:orange\", label=\"Data\")\n",
    "ax.plot(xtest, ytest, color=\"black\", label=\"True Function\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(Xtrain_noise, ytrain, color=\"tab:orange\", label=\"Data\")\n",
    "ax.plot(xtest, ytest, color=\"black\", label=\"True Function\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(xtest_noise, ytest_noise, color=\"tab:orange\", label=\"Test Data\")\n",
    "ax.plot(xtest, ytest, color=\"black\", label=\"True Function\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sRpFgFnhcgI"
   },
   "source": [
    "## Standard GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chex import Array\n",
    "from src.utils import identity_matrix, add_to_diagonal\n",
    "from src.kernels import RBF\n",
    "\n",
    "rbf_kernel = RBF(length_scale=1.0, variance=1.0)\n",
    "K = rbf_kernel.gram(Xtrain)\n",
    "\n",
    "# check shape\n",
    "chex.assert_shape(K, (Xtrain.shape[0], Xtrain.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzlSlTK8AU_d"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.exact import GPRModel\n",
    "from src.means import zero_mean\n",
    "\n",
    "jitter = 1e-5\n",
    "\n",
    "\n",
    "def numpyro_model(X, y):\n",
    "\n",
    "    #     # Set priors on hyperparameters.\n",
    "    #     η = numpyro.sample(\"variance\", dist.HalfCauchy(scale=5.0))\n",
    "    #     ℓ = numpyro.sample(\"length_scale\", dist.Gamma(2.0, 1.0))\n",
    "    #     σ = numpyro.sample(\"obs_noise\", dist.HalfCauchy(scale=5.0))\n",
    "\n",
    "    # set params and constraints on hyperparams\n",
    "    η = numpyro.param(\"variance\", init_value=1.0, constraints=dist.constraints.positive)\n",
    "    ℓ = numpyro.param(\n",
    "        \"length_scale\", init_value=1.0, constraints=dist.constraints.positive\n",
    "    )\n",
    "    σ = numpyro.param(\n",
    "        \"obs_noise\", init_value=0.01, onstraints=dist.constraints.positive\n",
    "    )\n",
    "\n",
    "    # Kernel Function\n",
    "    rbf_kernel = RBF(variance=η, length_scale=ℓ)\n",
    "\n",
    "    # GP Model\n",
    "    gp_model = GPRModel(\n",
    "        X=X, y=y, mean=zero_mean, kernel=rbf_kernel, obs_noise=σ, jitter=jitter\n",
    "    )\n",
    "\n",
    "    # Sample y according SGP\n",
    "    return gp_model.to_numpyro(y=y)\n",
    "\n",
    "\n",
    "def empty_guide(X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with numpyro.handlers.seed(rng_seed=123):\n",
    "    t = numpyro_model(Xtrain, ytrain)\n",
    "\n",
    "assert t.shape == Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer.autoguide import AutoDelta\n",
    "\n",
    "# ===================\n",
    "# Model\n",
    "# ===================\n",
    "# GP model\n",
    "\n",
    "# delta guide - basically deterministic\n",
    "delta_guide = AutoDelta(numpyro_model)\n",
    "mll_guide = empty_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "rng_key = random.PRNGKey(0)\n",
    "\n",
    "\n",
    "# Setup\n",
    "optimizer = numpyro.optim.Adam(step_size=0.01)\n",
    "# optimizer = numpyro.optim.Minimize()\n",
    "# optimizer = optax.adamw(learning_rate=0.1)\n",
    "svi = SVI(numpyro_model, mll_guide, optimizer, loss=Trace_ELBO())\n",
    "svi_results = svi.run(random.PRNGKey(1), 100, Xtrain, ytrain.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(svi_results.losses)\n",
    "ax.set(title=\"Loss\", xlabel=\"Iterations\", ylabel=\"Negative Log-Likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Take them directly\n",
    "learned_params = svi_results.params\n",
    "# x_u = learned_params[\"x_u\"]\n",
    "# learned_params = delta_guide.median(learned_params)\n",
    "# learned_params[\"x_u\"] = x_u\n",
    "learned_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.exact import init_gp_predictive, get_cond_params\n",
    "\n",
    "gp_pred = init_gp_predictive(RBF, learned_params, Xtrain, ytrain, jitter=1e-5)\n",
    "\n",
    "mu, var = gp_pred.predict_y(xtest)\n",
    "\n",
    "std = jnp.sqrt(var.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = 1.96\n",
    "one_stddev = ci * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(Xtrain, ytrain.squeeze(), \"o\", color=\"tab:orange\", label=\"Training Data\")\n",
    "ax.plot(xtest, ytest, color=\"black\", linestyle=\"-\", label=\"True Function\")\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    mu.ravel(),\n",
    "    color=\"Blue\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=3,\n",
    "    label=\"Predictive Mean\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    xtest.ravel(),\n",
    "    mu.ravel() - one_stddev,\n",
    "    mu.ravel() + one_stddev,\n",
    "    alpha=0.4,\n",
    "    color=\"tab:blue\",\n",
    "    label=f\" 95% Confidence Interval\",\n",
    ")\n",
    "ax.plot(xtest, mu.ravel() - one_stddev, linestyle=\"--\", color=\"tab:blue\")\n",
    "ax.plot(xtest, mu.ravel() + one_stddev, linestyle=\"--\", color=\"tab:blue\")\n",
    "plt.tight_layout()\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertainty_toolbox import viz as utviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var = gp_pred.predict_y(Xtrain)\n",
    "\n",
    "std = jnp.sqrt(var.squeeze())\n",
    "\n",
    "utviz.plot_calibration(y_pred=mu.ravel(), y_std=std.ravel(), y_true=ytrain.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var = gp_pred.predict_y(xtest)\n",
    "\n",
    "std = jnp.sqrt(var.squeeze())\n",
    "\n",
    "utviz.plot_calibration(y_pred=mu.ravel(), y_std=std.ravel(), y_true=ytest.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.exact import init_gp_predictive, get_cond_params\n",
    "\n",
    "gp_pred = init_gp_predictive(RBF, learned_params, Xtrain, ytrain, jitter=1e-8)\n",
    "\n",
    "mu, var = gp_pred.predict_y(xtest_noise)\n",
    "\n",
    "std = jnp.sqrt(var.squeeze())\n",
    "\n",
    "ci = 1.96\n",
    "one_stddev = ci * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_samples = np.random.choice(np.arange(xtest_noise.shape[0]), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(\n",
    "    xtest_noise, ytest_noise, marker=\"o\", s=30, color=\"tab:orange\", label=\"Testing Data\"\n",
    ")\n",
    "ax.plot(xtest, ytest, color=\"black\", linestyle=\"-\", label=\"True Function\")\n",
    "ax.plot(\n",
    "    xtest_noise,\n",
    "    mu.ravel(),\n",
    "    color=\"Blue\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=3,\n",
    "    label=\"Predictive Mean\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    xtest_noise.ravel(),\n",
    "    mu.ravel() - one_stddev,\n",
    "    mu.ravel() + one_stddev,\n",
    "    alpha=0.4,\n",
    "    color=\"tab:blue\",\n",
    "    label=f\" 95% Confidence Interval\",\n",
    ")\n",
    "ax.plot(xtest_noise, mu.ravel() - one_stddev, linestyle=\"--\", color=\"tab:blue\")\n",
    "ax.plot(xtest_noise, mu.ravel() + one_stddev, linestyle=\"--\", color=\"tab:blue\")\n",
    "plt.tight_layout()\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, var = gp_pred.predict_y(xtest_noise)\n",
    "\n",
    "std = jnp.sqrt(var.squeeze())\n",
    "\n",
    "utviz.plot_calibration(y_pred=mu.ravel(), y_std=std.ravel(), y_true=ytest_noise.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Sampling\n",
    "\n",
    "In this first example, we will approximate the actual posterior using Monte Carlo sampling. Using the following formula:\n",
    "\n",
    "$$\n",
    "p(f_*|\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}, \\mathcal{D}) \\approx \\frac{1}{T}\\sum_{t=1}^T \\mathcal{N}\\left(f_*|\\mu_\\mathcal{GP}(\\mathbf{x}_*^t),\\sigma^2_\\mathcal{GP}(\\mathbf{x}_*^t) \\right) \n",
    "$$\n",
    "\n",
    "This will approach the real posterior as $T$ grows. What we want to demonstrate is that this posterior is non-Gaussian when our inputs are non-Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we assume that $\\mathbf{x}_* \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\mathbf{\\Sigma_x})$. We're assuming our data points come We can reparameterize this as f\n",
    "\n",
    "\n",
    "we will be using the following form to do the sampling:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_T = \\mu_\\mathbf{x} + \\mathbf{L}\\mathbf{z}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{z}\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. So we will draw 10,000 samples and then propagate all of these through the non-linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.uncertain.monte_carlo import init_mc_transform\n",
    "\n",
    "\n",
    "egp_pred_mc = init_mc_transform(gp_pred, n_features=1, n_samples=1_000)\n",
    "x_cov = jnp.array([0.5]).reshape(-1, 1) ** 2\n",
    "key = jax.random.PRNGKey(123)\n",
    "\n",
    "mu_mc, var_mc = egp_pred_mc.predict_f(key, xtest_noise, x_cov, False)\n",
    "std_mc = jnp.sqrt(var_mc).ravel()\n",
    "mu_mc.shape, std_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_jitted = jax.jit(jax.partial(egp_pred_mc.predict_f, full_covariance=False))\n",
    "mu_mc, var_mc = pred_jitted(key, xtest_noise, x_cov)\n",
    "std_mc = jnp.sqrt(var_mc).ravel()\n",
    "mu_mc.shape, std_mc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit egp_pred_mc.predict_f(key, xtest_noise.block_until_ready(), x_cov.block_until_ready(), False)\n",
    "%timeit pred_jitted(key, xtest_noise.block_until_ready(), x_cov.block_until_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = 1.96\n",
    "one_stddev = ci * std_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(\n",
    "    xtest_noise, ytest_noise, marker=\"o\", s=30, color=\"tab:orange\", label=\"Testing Data\"\n",
    ")\n",
    "ax.plot(xtest, ytest, color=\"black\", linestyle=\"-\", label=\"True Function\")\n",
    "ax.plot(\n",
    "    xtest_noise,\n",
    "    mu.ravel(),\n",
    "    color=\"Blue\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=3,\n",
    "    label=\"Predictive Mean\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    xtest_noise.ravel(),\n",
    "    mu.ravel() - one_stddev,\n",
    "    mu.ravel() + one_stddev,\n",
    "    alpha=0.4,\n",
    "    color=\"tab:blue\",\n",
    "    label=f\" 95% Confidence Interval\",\n",
    ")\n",
    "ax.plot(xtest_noise, mu.ravel() - one_stddev, linestyle=\"--\", color=\"tab:blue\")\n",
    "ax.plot(xtest_noise, mu.ravel() + one_stddev, linestyle=\"--\", color=\"tab:blue\")\n",
    "plt.tight_layout()\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utviz.plot_calibration(\n",
    "    y_pred=mu_mc.ravel(), y_std=std_mc.ravel(), y_true=ytest_noise.ravel()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unscented Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.uncertain.unscented import init_unscented_transform\n",
    "\n",
    "\n",
    "egp_pred_unc = init_unscented_transform(gp_pred, n_features=1)\n",
    "x_cov = jnp.array([0.5]).reshape(-1, 1) ** 2\n",
    "key = jax.random.PRNGKey(123)\n",
    "mu_unc = egp_pred_unc.predict_mean(xtest_noise, x_cov)\n",
    "cov_unc = egp_pred_unc.predict_cov(xtest_noise, x_cov)\n",
    "std_unc = jnp.sqrt(jnp.diag(cov_unc.squeeze()))\n",
    "mu_unc, var_unc = egp_pred_unc.predict_f(xtest_noise, x_cov, False)\n",
    "std_unc_2 = jnp.sqrt(var_unc.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xtest_noise, mu_mc)\n",
    "plt.plot(xtest_noise, mu_unc)\n",
    "plt.plot(xtest_noise, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = 1.96\n",
    "one_stddev = ci * std_unc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(\n",
    "    xtest_noise, ytest_noise, marker=\"o\", s=30, color=\"tab:orange\", label=\"Testing Data\"\n",
    ")\n",
    "ax.plot(xtest, ytest, color=\"black\", linestyle=\"-\", label=\"True Function\")\n",
    "ax.plot(\n",
    "    xtest_noise,\n",
    "    mu.ravel(),\n",
    "    color=\"Blue\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=3,\n",
    "    label=\"Predictive Mean\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    xtest_noise.ravel(),\n",
    "    mu.ravel() - one_stddev,\n",
    "    mu.ravel() + one_stddev,\n",
    "    alpha=0.4,\n",
    "    color=\"tab:blue\",\n",
    "    label=f\" 95% Confidence Interval\",\n",
    ")\n",
    "ax.plot(xtest_noise, mu.ravel() - one_stddev, linestyle=\"--\", color=\"tab:blue\")\n",
    "ax.plot(xtest_noise, mu.ravel() + one_stddev, linestyle=\"--\", color=\"tab:blue\")\n",
    "plt.tight_layout()\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utviz.plot_calibration(\n",
    "    y_pred=mu_unc.ravel(), y_std=std_unc.ravel(), y_true=ytest_noise.ravel()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG97NTv4hmK-"
   },
   "outputs": [],
   "source": [
    "from gpjax.gps import Prior\n",
    "from gpjax.mean_functions import Zero\n",
    "from gpjax.kernels import RBF\n",
    "from gpjax.likelihoods import Gaussian\n",
    "from gpjax.types import Dataset\n",
    "\n",
    "\n",
    "# GP Prior\n",
    "mean_function = Zero()\n",
    "kernel = RBF()\n",
    "prior = Prior(mean_function=mean_function, kernel=kernel)\n",
    "\n",
    "# GP Likelihood\n",
    "lik = Gaussian()\n",
    "\n",
    "# GP Posterior\n",
    "posterior = prior * lik\n",
    "\n",
    "# initialize training dataset\n",
    "training_ds = Dataset(X=X, y=y_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKkIpyjciN3s"
   },
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqayQrwyiOhj"
   },
   "outputs": [],
   "source": [
    "from gpjax.parameters import initialise\n",
    "import numpyro.distributions as dist\n",
    "from gpjax.interfaces.numpyro import numpyro_dict_params, add_constraints\n",
    "\n",
    "\n",
    "# initialize parameters\n",
    "params = initialise(posterior)\n",
    "\n",
    "hyperpriors = {\n",
    "    \"lengthscale\": dist.LogNormal(0.0, 10.0),\n",
    "    \"variance\": dist.LogNormal(0.0, 10.0),\n",
    "    \"obs_noise\": dist.LogNormal(0.0, 10.0),\n",
    "}\n",
    "\n",
    "hyperpriors = {\n",
    "    \"lengthscale\": 1.0,\n",
    "    \"variance\": 1.0,\n",
    "    \"obs_noise\": 0.01,\n",
    "}\n",
    "\n",
    "\n",
    "# convert to numpyro-style params\n",
    "numpyro_params = numpyro_dict_params(hyperpriors)\n",
    "\n",
    "# convert to numpyro-style params\n",
    "numpyro_params = add_constraints(numpyro_params, dist.constraints.softplus_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayhRflSLhmuw"
   },
   "source": [
    "### Inference (SVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPz8HG3chdmG"
   },
   "outputs": [],
   "source": [
    "from numpyro.infer.autoguide import AutoDelta\n",
    "from gpjax.interfaces.numpyro import numpyro_marginal_ll, numpyro_dict_params\n",
    "\n",
    "\n",
    "# initialize numpyro-style GP model\n",
    "npy_model = numpyro_marginal_ll(posterior, numpyro_params)\n",
    "\n",
    "# approximate posterior\n",
    "guide = AutoDelta(npy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIibgYw1h15o"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_kfQeiLh-at",
    "outputId": "232b8e72-a63a-40d2-f873-cb5ee9f6dc31"
   },
   "outputs": [],
   "source": [
    "from numpyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "# reproducibility\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "n_iterations = 2_000\n",
    "\n",
    "\n",
    "# numpyro specific optimizer\n",
    "optimizer = numpyro.optim.Adam(step_size=0.005)\n",
    "\n",
    "# stochastic variational inference (pseudo)\n",
    "svi = SVI(npy_model, guide, optimizer, loss=Trace_ELBO())\n",
    "svi_results = svi.run(jax.random.PRNGKey(1), n_iterations, training_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YITQAlHbiBv4"
   },
   "source": [
    "#### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "ZIHmoN4DiCkM",
    "outputId": "7bc90343-9b56-493c-95be-acfbf91e15e4"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(svi_results.losses)\n",
    "ax.set(title=\"Loss\", xlabel=\"Iterations\", ylabel=\"ELBO\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-whVeaxBieN0"
   },
   "source": [
    "# Take them directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1yssV0ykQUt",
    "outputId": "eab37bd6-cfee-4644-a9a5-03a60c310995"
   },
   "outputs": [],
   "source": [
    "learned_params = svi_results.params\n",
    "learned_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDi0cQcrie3D"
   },
   "outputs": [],
   "source": [
    "from gpjax import mean, variance\n",
    "\n",
    "meanf = mean(posterior, learned_params, training_ds)\n",
    "covarf = variance(posterior, learned_params, training_ds)\n",
    "varf = lambda x: jnp.atleast_1d(jnp.diag(covarf(x)))\n",
    "\n",
    "\n",
    "mu = meanf(Xtest).squeeze()\n",
    "cov = covarf(Xtest).squeeze()\n",
    "var = varf(Xtest).squeeze()\n",
    "\n",
    "one_stddev = 1.96 * jnp.sqrt(jnp.diag(cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtHRc7Bip2bS"
   },
   "outputs": [],
   "source": [
    "def get_sorted_indices(X):\n",
    "    return jnp.argsort(X, axis=0).squeeze()\n",
    "\n",
    "\n",
    "def apply_sorted_indices(X, sorted_idx):\n",
    "    return X[(sorted_idx,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrE9ddknC8-W"
   },
   "outputs": [],
   "source": [
    "def plot_1D_GP_clean(Xtest, y_mu, y_var):\n",
    "\n",
    "    one_stddev = 1.96 * jnp.sqrt(y_var)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(X.ravel(), y.squeeze(), \"o\", color=\"tab:orange\")\n",
    "\n",
    "    ax.plot(Xtest.ravel(), y_mu, color=\"tab:blue\")\n",
    "    ax.fill_between(\n",
    "        Xtest.ravel(),\n",
    "        y_mu.ravel() - one_stddev.ravel(),\n",
    "        y_mu.ravel() + one_stddev.ravel(),\n",
    "        alpha=0.4,\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "    ax.plot(Xtest.ravel(), y_mu.ravel() - one_stddev, linestyle=\"--\", color=\"tab:blue\")\n",
    "    ax.plot(Xtest.ravel(), y_mu.ravel() + one_stddev, linestyle=\"--\", color=\"tab:blue\")\n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "id": "nPq_VbWbDSXD",
    "outputId": "d30a0ce6-15b7-4cc1-c020-a66d5b0cf160"
   },
   "outputs": [],
   "source": [
    "plot_1D_GP_clean(Xtest, mu, jnp.diag(cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvZxHBSflWd8"
   },
   "source": [
    "### Predictions on Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "LAifTf0MN-jl",
    "outputId": "b8283c92-389a-472d-e924-f15d45f7a55b"
   },
   "outputs": [],
   "source": [
    "x_noise = 0.5\n",
    "input_cov = jnp.array([x_noise]).reshape(-1, 1) ** 2\n",
    "\n",
    "ntest = 100\n",
    "\n",
    "\n",
    "Xtest = np.linspace(-10.1, 10.1, ntest)[:, None]\n",
    "ytest = f(Xtest)\n",
    "\n",
    "demo_sample_idx = 47\n",
    "\n",
    "key, xt_rng = jax.random.split(key, 2)\n",
    "\n",
    "Xtest_noisy = Xtest + x_noise * jax.random.normal(xt_rng, shape=Xtest.shape)\n",
    "\n",
    "\n",
    "idx_sorted = jnp.argsort(Xtest_noisy, axis=0)\n",
    "\n",
    "# Xtest = Xtest[(idx_sorted,)]\n",
    "Xtest_noisy = Xtest_noisy[(idx_sorted,)][..., 0]\n",
    "ytest_noisy = ytest[(idx_sorted,)]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "ax[1].scatter(Xtest_noisy, ytest_noisy, color=\"red\")\n",
    "ax[0].scatter(Xtest, ytest, color=\"red\")\n",
    "ax[1].scatter(\n",
    "    Xtest_noisy[demo_sample_idx],\n",
    "    ytest_noisy[demo_sample_idx],\n",
    "    marker=\".\",\n",
    "    s=300,\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJLVcur0FSd_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Lxqu2_4DfBs"
   },
   "outputs": [],
   "source": [
    "def plot_1D_GP_noisy(Xtest_noisy, y_mu, y_var):\n",
    "\n",
    "    one_stddev = 1.96 * jnp.sqrt(y_var)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(Xtest_noisy.squeeze(), ytest_noisy.squeeze(), \"o\", color=\"tab:orange\")\n",
    "\n",
    "    ax.plot(Xtest_noisy.squeeze(), y_mu.squeeze(), color=\"tab:blue\")\n",
    "    ax.fill_between(\n",
    "        Xtest_noisy.squeeze(),\n",
    "        y_mu.squeeze() - one_stddev.squeeze(),\n",
    "        y_mu.ravel() + one_stddev.squeeze(),\n",
    "        alpha=0.4,\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        Xtest_noisy.squeeze(),\n",
    "        y_mu.squeeze() - one_stddev.squeeze(),\n",
    "        linestyle=\"--\",\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        Xtest_noisy.squeeze(),\n",
    "        y_mu.squeeze() + one_stddev.squeeze(),\n",
    "        linestyle=\"--\",\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNSRFJ3EINxG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVO_hYbblYoU"
   },
   "outputs": [],
   "source": [
    "mu = meanf(Xtest_noisy).squeeze()\n",
    "var = varf(Xtest_noisy).squeeze()\n",
    "\n",
    "\n",
    "one_stddev = 1.96 * jnp.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "id": "kGE4GHKoDwZt",
    "outputId": "c5c92b28-a4c8-4d4b-a8e7-71da2f79e498"
   },
   "outputs": [],
   "source": [
    "plot_1D_GP_noisy(Xtest_noisy, mu, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo9YBI4jmjZp"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpsOZ8Z3VGsZ"
   },
   "source": [
    "## Noisy Test Samples\n",
    "\n",
    "In Gaussian processes, the original formulation dictates that we assume there is some noise in the observations, $y$ and that we observe the real inputs $\\mathbf{x}$. So we'll see that this it is not trivial to modify this formulation to account for uncertain inputs. Let's assume that we have a data set $\\mathcal{D}=\\{\\mathbf{X}, \\boldsymbol{y} \\}$. In this case we assume the following relationship between our inputs, $\\mathbf{x}$, and outputs, $y$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_n &= f(\\mathbf{x}_n)+ \\epsilon_y \\\\\n",
    "\\epsilon_y &\\sim \\mathcal{N}(0,\\sigma_y^2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let's also assume that we have a standard GP model optimized and fitted to this data set. We're not assuming noisy inputs during the training phase so we will use the standard log-likelihood maximization procedure. However, during the testing phase, we will assume that our inputs are noisy. For simplicity, we can assume our test data set is normally distributed with a mean $\\mu_\\mathbf{x}$ and variance $\\Sigma_\\mathbf{x}$. So we will have:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_* \\sim \\mathcal{N}(\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}) $$\n",
    "\n",
    "or equivalently we can reparameterize it like so:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{x}_* &=\\mu_\\mathbf{x}+ \\epsilon_\\mathbf{x} \\\\\n",
    "\\epsilon_\\mathbf{x} &\\sim \\mathcal{N}(0, \\Sigma_\\mathbf{x})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we consider the predictive distribution given by $p(f_*|\\mathbf{x}_*, \\mathcal{D})$,  we need to marginalize out the input distribution. So the full integral appears as follows.\n",
    "\n",
    "$$p(f_*|\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x},\\mathcal{D}) = \\int p(f_*|\\mathbf{x}_*,\\mathcal{D})\\;\\mathcal{N}(\\mathbf{x}_*|\\mu_\\mathbf{x},\\Sigma_\\mathbf{x})\\; d\\mathbf{x}_*$$\n",
    "\n",
    "If we use the GP formulation, we have a closed-form deterministic predictive distribution for $p(f_*|\\mathbf{x}_*,\\mathcal{D})$. Plugging this into the above equation gives us:\n",
    "\n",
    "$$p(f_*|\\mu_\\mathbf{x},\\Sigma_\\mathbf{x},\\mathcal{D}) = \\int \\mathcal{N}\\left(f_*|\\mu_\\mathcal{GP}(\\mathbf{x}_*),\\sigma^2_\\mathcal{GP}(\\mathbf{x}_*) \\right) \\; \\mathcal{N}(\\mathbf{x}_*|\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x})\\; d\\mathbf{x}_*$$\n",
    "\n",
    "So this integral is intractable because if we consider the terms within the GP predictive mean and predictive variance, we will need to calculate the integral of an inverse kernel function, $\\mathbf{K}_\\mathcal{GP}^{-1}$. Below we outline some of the most popular methods found in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9meHDrP6xoTA"
   },
   "source": [
    "### Demo - Single Point\n",
    "\n",
    "For to demonstrate the function, we're going to do the steps assuming no batch dimension. In other words, we're working with a test vector $\\mathbf{x}_* \\in \\mathbb{R}^{D}$, **not** a matrix of samples $\\mathbf{X}_* \\in \\mathbb{R}^{N\\times D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RNU493zIyOz"
   },
   "outputs": [],
   "source": [
    "xtest_noisy = Xtest_noisy[demo_sample_idx]\n",
    "mu_demo = {}\n",
    "\n",
    "mu_demo = {\"standard\": meanf(xtest_noisy)}\n",
    "covar_demo = {\"standard\": varf(xtest_noisy)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "lJ1YzNyFzT7W",
    "outputId": "95df640e-d19f-474b-afc5-95ad325d25ad"
   },
   "outputs": [],
   "source": [
    "plt.scatter(Xtest_noisy, ytest_noisy, s=10, color=\"tab:orange\")\n",
    "plt.scatter(xtest_noisy, mu_demo[\"standard\"], marker=\".\", s=300, color=\"black\")\n",
    "plt.axhline(mu_demo[\"standard\"], linestyle=\"--\", color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ppp8TM_4WmRv"
   },
   "outputs": [],
   "source": [
    "# x_mu = np.array([-0.2])\n",
    "# x_cov = np.tile(np.diag(np.array([3.0])), (1, 1))\n",
    "\n",
    "# # get function\n",
    "# f = jax.vmap(jax.partial(predictive_mean, model))\n",
    "# df = jax.vmap(jax.grad(jax.partial(predictive_mean, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd-d6STpWsSG"
   },
   "outputs": [],
   "source": [
    "# mu = f(Xtest)\n",
    "\n",
    "# # plt.hist(x_mc_samples.squeeze(), bins=100, density=True);\n",
    "# fig, ax = plt.subplots(figsize=(7, 5))\n",
    "# ax.plot(Xtest, mu, color='red', linewidth=4, label=r\"$f(x)$\")\n",
    "# ax.axvline(x_sample, linestyle=\"--\", color='black', linewidth=4)\n",
    "# ax.set(xticklabels=\"\", xlabel=\"\", yticklabels=\"\")\n",
    "# plt.fill_between(Xtest.squeeze(), lb, ub, label='CI', color='darkorange', alpha=0.2)\n",
    "# ax.set(xlim=(-10, 10), ylim=(-2.1, 2.1))\n",
    "# plt.scatter(X, y, label='Training', color='blue', s=60, zorder=3)\n",
    "# plt.legend(fontsize=15)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZOz9jXlVKgH"
   },
   "source": [
    "### Monte Carlo Sampling\n",
    "\n",
    "In this first example, we will approximate the actual posterior using Monte Carlo sampling. Using the following formula:\n",
    "\n",
    "$$\n",
    "p(f_*|\\mu_\\mathbf{x}, \\Sigma_\\mathbf{x}, \\mathcal{D}) \\approx \\frac{1}{T}\\sum_{t=1}^T \\mathcal{N}\\left(f_*|\\mu_\\mathcal{GP}(\\mathbf{x}_*^t),\\sigma^2_\\mathcal{GP}(\\mathbf{x}_*^t) \\right) \n",
    "$$\n",
    "\n",
    "This will approach the real posterior as $T$ grows. What we want to demonstrate is that this posterior is non-Gaussian when our inputs are non-Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slBWyndkWNV4"
   },
   "source": [
    "In this example, we assume that $\\mathbf{x}_* \\sim \\mathcal{N}(\\mathbf{\\mu_x}, \\mathbf{\\Sigma_x})$. We're assuming our data points come We can reparameterize this as f\n",
    "\n",
    "\n",
    "we will be using the following form to do the sampling:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_T = \\mu_\\mathbf{x} + \\mathbf{L}\\mathbf{z}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{z}\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. So we will draw 10,000 samples and then propagate all of these through the non-linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9GHLiNer5Iq"
   },
   "outputs": [],
   "source": [
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "dist = tfp.distributions\n",
    "\n",
    "n_features = 1\n",
    "\n",
    "z = dist.Normal(loc=np.zeros((n_features,)), scale=np.ones(n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6do6rLF9sx0K"
   },
   "outputs": [],
   "source": [
    "# input covariance\n",
    "L = jnp.linalg.cholesky(input_cov)\n",
    "\n",
    "chex.assert_shape(\n",
    "    L,\n",
    "    (\n",
    "        n_features,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# generate sigma points\n",
    "n_mc_points = 100\n",
    "sigma_pts = z.sample((n_mc_points,), key).T\n",
    "\n",
    "chex.assert_shape(\n",
    "    sigma_pts,\n",
    "    (\n",
    "        n_features,\n",
    "        n_mc_points,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# calculate sigma points\n",
    "x_mc_samples = xtest_noisy + L @ sigma_pts\n",
    "\n",
    "chex.assert_shape(x_mc_samples, (n_features, n_mc_points))\n",
    "\n",
    "# mean function predictions\n",
    "\n",
    "f = lambda x: jnp.atleast_1d(meanf(x).squeeze())\n",
    "\n",
    "y_mu_mc = jax.vmap(f, in_axes=2)(x_mc_samples[:, None, :])\n",
    "\n",
    "y_mu_mc = jnp.atleast_1d(y_mu_mc)\n",
    "\n",
    "chex.assert_shape(\n",
    "    y_mu_mc,\n",
    "    (\n",
    "        n_mc_points,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# calculate mean of MC samples\n",
    "wm = 1.0 / n_mc_points\n",
    "y_mu = jnp.mean(y_mu_mc, axis=0)\n",
    "\n",
    "chex.assert_shape(y_mu, (n_features,))\n",
    "\n",
    "# calculate covariance of MC samples\n",
    "wc = 1.0 / (n_mc_points - 1.0)\n",
    "dfx = y_mu_mc - y_mu\n",
    "\n",
    "chex.assert_shape(\n",
    "    dfx,\n",
    "    (\n",
    "        n_mc_points,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "x_cov = wc * dfx.T @ dfx\n",
    "\n",
    "chex.assert_shape(\n",
    "    x_cov,\n",
    "    (\n",
    "        n_features,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# input-output covariance\n",
    "x_covx = wc * (x_mc_samples - xtest_noisy) @ dfx\n",
    "\n",
    "chex.assert_shape(\n",
    "    x_cov,\n",
    "    (\n",
    "        n_features,\n",
    "        n_features,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UT_CYCrHJrDp"
   },
   "outputs": [],
   "source": [
    "mu_demo[\"mc\"] = y_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "id": "5GFHSY340nmj",
    "outputId": "362169a8-eb39-475d-bfb3-ee0b1658ac29"
   },
   "outputs": [],
   "source": [
    "# plt.hist(x_mc_samples.squeeze(), bins=100, density=True);\n",
    "fig, ax = plt.subplots(figsize=(7, 1))\n",
    "sns.kdeplot(\n",
    "    ax=ax,\n",
    "    x=x_mc_samples.squeeze(),\n",
    "    bw_adjust=3.0,\n",
    "    color=\"black\",\n",
    "    linewidth=4,\n",
    "    fill=True,\n",
    "    label=r\"$p(x)$\",\n",
    ")\n",
    "ax.axvline(xtest_noisy, linestyle=\"--\", color=\"black\", linewidth=4)\n",
    "ax.set(yticklabels=\"\", ylabel=\"\", xticklabels=\"\", xlim=(-10, 10))\n",
    "# ax.set_ylim(-1.5, 1.1)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "6CHLNOIfBvOg",
    "outputId": "9a5cd3ef-ceb7-43a9-8eac-75d4f1f61b20"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 5))\n",
    "sns.kdeplot(ax=ax, y=y_mu_mc.ravel(), bw_adjust=3.0, linewidth=4, label=\"MC Samples\")\n",
    "ax.set(xlabel=\"\", xticklabels=\"\", ylim=(-2.1, 2.1), yticklabels=\"\")\n",
    "ax.axhline(mu_demo[\"standard\"], color=\"black\", linestyle=\"--\", label=\"Mean Prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C56AGC7X4DD9"
   },
   "source": [
    "#### Vectorized Over All Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJkXl1IV4F1a"
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def init_mc_moment_transform(\n",
    "    meanf: Callable, n_features: int, mc_samples: int = 100, covariance: bool = False\n",
    ") -> Callable:\n",
    "\n",
    "    f = lambda x: jnp.atleast_1d(meanf(x).squeeze())\n",
    "    z = dist.Normal(loc=np.zeros((n_features,)), scale=np.ones(n_features))\n",
    "    wm = 1.0 / mc_samples\n",
    "    wc = 1.0 / (mc_samples - 1.0)\n",
    "\n",
    "    def apply_transform(rng_key, x, x_cov):\n",
    "\n",
    "        # sigma points\n",
    "        sigma_pts = z.sample((mc_samples,), key)\n",
    "\n",
    "        # cholesky for input covariance\n",
    "        L = jnp.linalg.cholesky(input_cov)\n",
    "\n",
    "        # calculate sigma points\n",
    "        x_mc_samples = x + L @ sigma_pts.T\n",
    "\n",
    "        # ===================\n",
    "        # Mean\n",
    "        # ===================\n",
    "\n",
    "        # function predictions over mc samples\n",
    "        y_mu_mc = jax.vmap(f, in_axes=1)(x_mc_samples).reshape(\n",
    "            (\n",
    "                mc_samples,\n",
    "                n_features,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # mean of mc samples\n",
    "        y_mu = jnp.mean(y_mu_mc, axis=0)\n",
    "\n",
    "        y_mu = jnp.atleast_1d(y_mu)\n",
    "\n",
    "        # ===================\n",
    "        # Covariance\n",
    "        # ===================\n",
    "        dfydx = y_mu_mc - y_mu\n",
    "\n",
    "        y_cov = wc * dfydx.T @ dfydx\n",
    "\n",
    "        if not covariance:\n",
    "            y_cov = jnp.diag(y_cov)\n",
    "\n",
    "            y_cov = jnp.atleast_1d(y_cov)\n",
    "\n",
    "        return y_mu, y_cov\n",
    "\n",
    "    return apply_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCW9pSxK6EzB"
   },
   "outputs": [],
   "source": [
    "# init function\n",
    "n_features = 1\n",
    "mc_samples = 1_000\n",
    "mc_transform = init_mc_moment_transform(\n",
    "    meanf, n_features=n_features, mc_samples=mc_samples\n",
    ")\n",
    "\n",
    "# apply function\n",
    "xtest_noisy = Xtest_noisy[demo_sample_idx][..., 0]\n",
    "input_cov = jnp.array([x_noise]).reshape(-1, 1)\n",
    "\n",
    "x_mu, x_var = mc_transform(key, xtest_noisy, input_cov)\n",
    "\n",
    "chex.assert_equal_shape([x_mu, x_var])\n",
    "chex.assert_shape(x_mu, (n_features,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zl4MRLNgAsW-"
   },
   "outputs": [],
   "source": [
    "mc_transform_vectorized = jax.jit(jax.vmap(mc_transform, in_axes=(None, 0, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Say06XOb6t7w"
   },
   "outputs": [],
   "source": [
    "y_mu, y_var = mc_transform_vectorized(key, Xtest_noisy, input_cov)\n",
    "\n",
    "chex.assert_equal_shape([y_mu, y_var])\n",
    "chex.assert_shape(\n",
    "    y_mu,\n",
    "    (\n",
    "        ntest,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "y_std = 1.96 * jnp.sqrt(y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "ytHAWsqgEoQX",
    "outputId": "6b1d64d3-9e3a-4c45-cc4a-552e17aa72f6"
   },
   "outputs": [],
   "source": [
    "plot_1D_GP_noisy(Xtest_noisy, y_mu, y_var);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cvz8m2MWXKEr"
   },
   "source": [
    "As you can see, this is a non-Gaussian distribution. Below we will see how we can approximate this as a Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvId3r6AXi39"
   },
   "source": [
    "## Taylor Approximation\n",
    "\n",
    "The \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgjyNYhxCrch"
   },
   "source": [
    "### 1st Order Approximation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{\\mathbf{\\mu}}_\\text{LinGP}(\\mathbf{x_*})\n",
    "&= \\mathbf{\\mu}_\\text{GP}(\\mathbf{\\mu}_\\mathbf{x_*})\\\\\n",
    "\\tilde{\\mathbf{\\Sigma}}^2_\\text{LinGP} (\\mathbf{x_*}) &= \n",
    "\\mathbf{\\Sigma}^2_\\text{GP}(\\mathbf{\\mu}_\\mathbf{x_*}) + \n",
    "\\underbrace{\\frac{\\partial \\mathbf{\\mu}_\\text{GP}(\\mathbf{\\mu}_\\mathbf{x_*})}{\\partial \\mathbf{x_*}}^\\top\n",
    "\\mathbf{\\Sigma}_\\mathbf{x_*}\n",
    "\\frac{\\partial \\mathbf{\\mu}_\\text{GP}(\\mathbf{\\mu}_\\mathbf{x_*})}{\\partial \\mathbf{x_*}}}_\\text{1st Order} \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99tQFQmfGE0X"
   },
   "outputs": [],
   "source": [
    "mu = meanf(Xtest_noisy).squeeze()\n",
    "var = varf(Xtest_noisy).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Tc7O_Nfm_UW"
   },
   "outputs": [],
   "source": [
    "# demo sample\n",
    "xtest_noisy = Xtest_noisy[demo_sample_idx]\n",
    "\n",
    "# input covariance\n",
    "input_cov = jnp.array([x_noise]).reshape(-1, 1)\n",
    "\n",
    "# gradient predictive mean\n",
    "f = lambda x: meanf(x).squeeze()\n",
    "\n",
    "y_mu = f(xtest_noisy)\n",
    "\n",
    "df = jax.grad(f)\n",
    "\n",
    "# predictive mean\n",
    "dy_mu = df(xtest_noisy)\n",
    "chex.assert_equal_shape([dy_mu, xtest_noisy])\n",
    "\n",
    "# predictive covariance\n",
    "covar_to1 = dy_mu[None, :] @ input_cov @ dy_mu[:, None]\n",
    "chex.assert_shape(\n",
    "    covar_to1,\n",
    "    (\n",
    "        n_features,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# predictive variance\n",
    "var_to1 = jnp.diag(covar_to1).squeeze()\n",
    "\n",
    "one_stddev_to1 = 1.96 * jnp.sqrt(var_to1.squeeze()) + one_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ec9v4G5uHlUN"
   },
   "outputs": [],
   "source": [
    "mu_demo[\"taylor\"] = y_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZxJHNnuG10C"
   },
   "outputs": [],
   "source": [
    "# generate samples from the distribution\n",
    "lin_samples = jax.random.multivariate_normal(\n",
    "    key, jnp.atleast_1d(y_mu), covar_to1, (n_mc_points,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "sz08y2JEHDLP",
    "outputId": "fd909c48-9822-4206-f800-38e1d3a943c5"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 5))\n",
    "sns.kdeplot(ax=ax, y=y_mu_mc.ravel(), bw_adjust=3.0, linewidth=4, label=\"MC Samples\")\n",
    "sns.kdeplot(\n",
    "    ax=ax, y=lin_samples.ravel(), bw_adjust=3.0, linewidth=4, label=\"Taylor Approx\"\n",
    ")\n",
    "# sns.kdeplot(ax=ax, y=mm_samples.squeeze(), bw_adjust=3., linewidth=4, label='Moment Matching',)\n",
    "ax.set(xlabel=\"\", xticklabels=\"\", ylim=(-2.1, 2.1), yticklabels=\"\")\n",
    "ax.axhline(mu_demo[\"standard\"], color=\"black\", linestyle=\"--\", label=\"Mean Prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymfD-_koGf_1"
   },
   "outputs": [],
   "source": [
    "def init_taylor_o1_transform(\n",
    "    meanf: Callable, covarf: Callable, covariance: bool = False\n",
    ") -> Callable:\n",
    "\n",
    "    f = lambda x: meanf(x).squeeze()\n",
    "\n",
    "    gradf = jax.grad(f)\n",
    "\n",
    "    def apply_transform(x, x_cov):\n",
    "\n",
    "        # ===================\n",
    "        # Mean\n",
    "        # ===================\n",
    "\n",
    "        y_mu = f(x)\n",
    "        y_mu = jnp.atleast_1d(y_mu)\n",
    "\n",
    "        # ===================\n",
    "        # Co/Variance\n",
    "        # ===================\n",
    "        y_cov = covarf(x)\n",
    "        y_cov = jnp.atleast_2d(y_cov)\n",
    "\n",
    "        # gradient of the\n",
    "        dy_mu = gradf(x)\n",
    "\n",
    "        # gradient\n",
    "        x_cov = jnp.atleast_2d(x_cov)\n",
    "\n",
    "        y_cov = dy_mu[None, :] @ x_cov @ dy_mu[:, None]\n",
    "\n",
    "        if not covariance:\n",
    "            y_cov = jnp.diag(y_cov).squeeze()\n",
    "            y_cov = jnp.atleast_1d(y_cov)\n",
    "\n",
    "        return y_mu, y_cov\n",
    "\n",
    "    return apply_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpgXnd_LHTTv"
   },
   "outputs": [],
   "source": [
    "# init function\n",
    "taylor_o1_transform = init_taylor_o1_transform(meanf, covarf, False)\n",
    "\n",
    "# apply function\n",
    "xtest_noisy = Xtest_noisy[demo_sample_idx]\n",
    "input_cov = jnp.array([x_noise]).reshape(-1, 1)\n",
    "\n",
    "y_mu, y_var = taylor_o1_transform(xtest_noisy, input_cov)\n",
    "\n",
    "chex.assert_equal_shape([y_mu, y_var])\n",
    "chex.assert_shape(y_mu, (n_features,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEQHmq-vIAuh"
   },
   "outputs": [],
   "source": [
    "taylor_o1_transform_vectorized = jax.jit(\n",
    "    jax.vmap(taylor_o1_transform, in_axes=(0, None))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0x4YvoeIH99n"
   },
   "outputs": [],
   "source": [
    "y_mu, y_var_o1 = taylor_o1_transform_vectorized(Xtest_noisy, input_cov)\n",
    "\n",
    "\n",
    "chex.assert_equal_shape([y_mu, y_var_o1])\n",
    "chex.assert_shape(\n",
    "    y_mu,\n",
    "    (\n",
    "        ntest,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# y_std = 1.96 * jnp.sqrt(y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "tOsq2hf4JF44",
    "outputId": "5e69d99a-6c8a-4264-d9b7-a03b83cf5cb3"
   },
   "outputs": [],
   "source": [
    "plot_1D_GP_noisy(Xtest_noisy, y_mu, y_var_o1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgVInvFgBK5-"
   },
   "source": [
    "### Linearization (2nd Order)\n",
    "\n",
    "Here we do the same Taylor expansion except we can do the second order approximation.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{\\mathbf{\\mu}}_\\text{LinGP}(\\mathbf{x_*})\n",
    "&= \\mathbf{\\mu}_\\text{GP}(\\mathbf{\\mu}_\\mathbf{x_*}) +\n",
    "\\underbrace{\\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\mathbf{\\mu}_\\text{GP}(\\mathbf{\\mu}_\\mathbf{x_*})}{\\partial \\mathbf{x_*} \\partial \\mathbf{x_*}^\\top}  \\mathbf{\\Sigma}_\\mathbf{x_*}\\right\\}}_\\text{second Order} \\\\\n",
    "\\tilde{\\mathbf{\\Sigma}}^2_\\text{LinGP} (\\mathbf{x_*}) &= \n",
    "\\mathbf{\\Sigma}^2_\\text{GP}(\\mathbf{\\mu}_\\mathbf{x_*}) + \n",
    "\\underbrace{\\frac{\\partial \\mathbf{\\mu}_\\text{GP}(\\mathbf{\\mu}_\\mathbf{x_*})}{\\partial \\mathbf{x_*}}^\\top\n",
    "\\mathbf{\\Sigma}_\\mathbf{x_*}\n",
    "\\frac{\\partial \\mathbf{\\mu}_\\text{GP}(\\mathbf{\\mu}_\\mathbf{x_*})}{\\partial \\mathbf{x_*}}}_\\text{1st Order} +\n",
    "\\underbrace{\\frac{1}{2} \\text{Tr}\\left\\{ \\frac{\\partial^2 \\mathbf{\\Sigma}^2_\\text{GP}(\\mathbf{\\mu}_\\mathbf{x_*})}{\\partial \\mathbf{x_*} \\partial \\mathbf{x_*}^\\top}  \\mathbf{\\Sigma}_\\mathbf{x_*}\\right\\}}_\\text{second Order}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oqCNcmYLYRb"
   },
   "source": [
    "**Mean Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OksggwUnLZko"
   },
   "outputs": [],
   "source": [
    "# mean function\n",
    "f = lambda x: meanf(x).squeeze()\n",
    "\n",
    "y_mu = f(xtest_noisy)\n",
    "y_mu = jnp.atleast_1d(y_mu)\n",
    "\n",
    "chex.assert_shape(y_mu, (n_features,))\n",
    "\n",
    "# correction\n",
    "# 2nd derivative of mean function\n",
    "d2f = lambda x: jnp.atleast_2d(jax.hessian(f)(x).squeeze())\n",
    "\n",
    "d2y_mu = d2f(xtest_noisy)\n",
    "\n",
    "chex.assert_shape(d2y_mu, (n_features, n_features))\n",
    "\n",
    "term_o2 = 0.5 * jnp.trace(d2y_mu @ input_cov)\n",
    "\n",
    "y_mu += term_o2\n",
    "\n",
    "chex.assert_equal_shape([y_mu, xtest_noisy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pY7b07FPiGp"
   },
   "outputs": [],
   "source": [
    "mu_demo[\"taylor_o2\"] = y_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_HafFcPMXvD"
   },
   "source": [
    "**Covariance Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKQglAezEmol"
   },
   "outputs": [],
   "source": [
    "# 0th Order Term\n",
    "vf = lambda x: jnp.atleast_1d(varf(x).squeeze())\n",
    "cvf = lambda x: jnp.atleast_2d(covarf(x).squeeze())\n",
    "\n",
    "y_cov = cvf(xtest_noisy)\n",
    "\n",
    "chex.assert_shape(y_cov, (n_features, n_features))\n",
    "\n",
    "# 1st Order Term\n",
    "\n",
    "# 1st derivative of mean function\n",
    "df = lambda x: jnp.atleast_1d(jax.grad(f)(x).squeeze())\n",
    "dy_mu = df(xtest_noisy)\n",
    "\n",
    "chex.assert_equal_shape([dy_mu, xtest_noisy])\n",
    "\n",
    "# covariance\n",
    "term_o1 = dy_mu[None, :] @ x_cov @ dy_mu[:, None].T\n",
    "\n",
    "chex.assert_shape(term_o1, (n_features, n_features))\n",
    "\n",
    "# 2nd Order Term\n",
    "\n",
    "# correction\n",
    "# 2nd derivative of variance function\n",
    "vf = lambda x: varf(x).squeeze()\n",
    "dv2f = jax.hessian(vf)\n",
    "\n",
    "d2y_var = dv2f(xtest_noisy)\n",
    "\n",
    "chex.assert_shape(d2y_var, (n_features, n_features))\n",
    "\n",
    "\n",
    "term_o2 = 0.5 * jnp.trace(d2y_var @ input_cov)\n",
    "\n",
    "\n",
    "y_cov += term_o1 + term_o2\n",
    "\n",
    "chex.assert_shape(y_cov, (n_features, n_features))\n",
    "\n",
    "y_var = jnp.diag(y_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAbFSqFtPZuA"
   },
   "outputs": [],
   "source": [
    "# mu_demo[\"taylor_o2\"] = y_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dd69aUyGPca7"
   },
   "outputs": [],
   "source": [
    "# generate samples from the distribution\n",
    "lin_o2_samples = jax.random.multivariate_normal(\n",
    "    key, jnp.atleast_1d(y_mu), y_cov, (n_mc_points,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "jVr238cePWnX",
    "outputId": "c1ac13ea-1a5f-4ff8-d5ac-0eddde012c1c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 5))\n",
    "sns.kdeplot(ax=ax, y=y_mu_mc.ravel(), bw_adjust=3.0, linewidth=4, label=\"MC Samples\")\n",
    "sns.kdeplot(\n",
    "    ax=ax,\n",
    "    y=lin_samples.ravel(),\n",
    "    bw_adjust=3.0,\n",
    "    linewidth=4,\n",
    "    label=r\"Taylor Approx ($\\mathcal{O}(1)$)\",\n",
    ")\n",
    "sns.kdeplot(\n",
    "    ax=ax,\n",
    "    y=lin_o2_samples.ravel(),\n",
    "    bw_adjust=3.0,\n",
    "    linewidth=4,\n",
    "    label=r\"Taylor Approx ($\\mathcal{O}(2)$)\",\n",
    ")\n",
    "# sns.kdeplot(ax=ax, y=mm_samples.squeeze(), bw_adjust=3., linewidth=4, label='Moment Matching',)\n",
    "ax.set(xlabel=\"\", xticklabels=\"\", ylim=(-2.1, 2.1), yticklabels=\"\")\n",
    "ax.axhline(mu_demo[\"standard\"], color=\"black\", linestyle=\"--\", label=\"Mean Prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAHo6og5BUFu"
   },
   "outputs": [],
   "source": [
    "def init_taylor_o2_transform(\n",
    "    meanf: Callable, covarf: Callable, covariance: bool = False\n",
    ") -> Callable:\n",
    "\n",
    "    # mean function\n",
    "    f = lambda x: meanf(x).squeeze()\n",
    "    # derivative of Mean function\n",
    "    df = lambda x: jnp.atleast_1d(jax.grad(f)(x).squeeze())\n",
    "    # 2nd derivative of mean function\n",
    "    d2f = lambda x: jnp.atleast_2d(jax.hessian(f)(x).squeeze())\n",
    "    # 2nd derivative of variance function\n",
    "    cvf = lambda x: jnp.diag(covarf(x)).squeeze()\n",
    "    dv2f = jax.hessian(vf)\n",
    "\n",
    "    def apply_transform(x, x_cov):\n",
    "\n",
    "        # ===================\n",
    "        # Mean\n",
    "        # ===================\n",
    "\n",
    "        y_mu = f(x)\n",
    "        y_mu = jnp.atleast_1d(y_mu)\n",
    "\n",
    "        # apply 2nd order correction\n",
    "        d2y_mu = d2f(x)\n",
    "\n",
    "        y_mu += 0.5 * jnp.trace(d2y_mu @ x_cov)\n",
    "\n",
    "        # ===================\n",
    "        # Co/Variance\n",
    "        # ===================\n",
    "        y_cov = covarf(x)\n",
    "\n",
    "        # 1st order correction\n",
    "        # gradient of the\n",
    "        dy_mu = df(x)\n",
    "\n",
    "        # gradient\n",
    "        x_cov = jnp.atleast_2d(x_cov)\n",
    "\n",
    "        y_cov += dy_mu[None, :] @ x_cov @ dy_mu[:, None]\n",
    "\n",
    "        # 2nd order correction\n",
    "        d2y_var = dv2f(x)\n",
    "        y_cov += 0.5 * jnp.trace(d2y_var @ x_cov)\n",
    "\n",
    "        if not covariance:\n",
    "            y_cov = jnp.diag(y_cov).squeeze()\n",
    "            y_cov = jnp.atleast_1d(y_cov)\n",
    "\n",
    "        return y_mu, y_cov\n",
    "\n",
    "    return apply_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QM5Jf7uvEaJ2"
   },
   "outputs": [],
   "source": [
    "# init function\n",
    "taylor_o2_transform = init_taylor_o2_transform(meanf, varf, False)\n",
    "\n",
    "# apply function\n",
    "xtest_noisy = Xtest_noisy[demo_sample_idx]\n",
    "input_cov = jnp.array([x_noise]).reshape(-1, 1)\n",
    "\n",
    "y_mu, y_var = taylor_o2_transform(xtest_noisy, input_cov)\n",
    "\n",
    "chex.assert_equal_shape([y_mu, y_var])\n",
    "chex.assert_shape(y_mu, (n_features,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYte0jdEX63W"
   },
   "outputs": [],
   "source": [
    "taylor_o2_transform_vectorized = jax.jit(\n",
    "    jax.vmap(taylor_o2_transform, in_axes=(0, None))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVSbxt60X9-0"
   },
   "outputs": [],
   "source": [
    "y_mu, y_var_o2 = taylor_o2_transform_vectorized(Xtest_noisy, input_cov)\n",
    "\n",
    "\n",
    "chex.assert_equal_shape([y_mu, y_var_o2])\n",
    "chex.assert_shape(\n",
    "    y_mu,\n",
    "    (\n",
    "        ntest,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# y_var = var.squeeze() + y_var_o2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "7DPbZa0SYDIo",
    "outputId": "a6204ba7-e425-4dac-bfb5-9901f045be93"
   },
   "outputs": [],
   "source": [
    "plot_1D_GP_noisy(Xtest_noisy, y_mu, y_var_o2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9o0dchFha3Ey"
   },
   "source": [
    "## Unscented Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbSvjiaKdGEC"
   },
   "source": [
    "#### Unscented Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3v-QhdtrcivO"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "def get_unscented_weights(\n",
    "    n_features: int,\n",
    "    kappa: Optional[float] = None,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 2.0,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Generate normalizers for MCMC samples\"\"\"\n",
    "\n",
    "    # calculate kappa value\n",
    "    if kappa is None:\n",
    "        kappa = jnp.maximum(3.0 - n_features, 0.0)\n",
    "\n",
    "    lam = alpha**2 * (n_features + kappa) - n_features\n",
    "    wm = 1.0 / (2.0 * (n_features + lam)) * np.ones(2 * n_features + 1)\n",
    "    wc = wm.copy()\n",
    "    wm = jax.ops.index_update(wm, 0, lam / (n_features + lam))\n",
    "    wc = jax.ops.index_update(wc, 0, wm[0] + (1 - alpha**2 + beta))\n",
    "    return wm, wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LORz4HFGcqcY"
   },
   "outputs": [],
   "source": [
    "n_features = 1\n",
    "alpha = 1.0\n",
    "beta = 2.0\n",
    "kappa = None\n",
    "n_sigma_points = 3\n",
    "\n",
    "wm, wc = get_unscented_weights(n_features, kappa, alpha, beta)\n",
    "\n",
    "chex.assert_equal_shape([wm, wc])\n",
    "chex.assert_shape(wm, (n_sigma_points,))\n",
    "\n",
    "# Wm, Wc = jnp.diag(wm), jnp.diag(wc)\n",
    "\n",
    "# chex.assert_equal_shape([Wm, Wc])\n",
    "# chex.assert_shape(Wm, (n_sigma_points,n_sigma_points))\n",
    "\n",
    "Wm, Wc = wm, jnp.diag(wc)\n",
    "\n",
    "chex.assert_shape(Wc, (n_sigma_points, n_sigma_points))\n",
    "chex.assert_shape(Wm, (n_sigma_points,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_z96ANudLDg"
   },
   "source": [
    "#### Sigma Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9B86XRUdFKD"
   },
   "outputs": [],
   "source": [
    "def get_unscented_sigma_points(\n",
    "    n_features: int, kappa: Optional[float] = None, alpha: float = 1.0\n",
    ") -> Tuple[chex.Array, chex.Array]:\n",
    "    \"\"\"Generate Unscented samples\"\"\"\n",
    "\n",
    "    # calculate kappa value\n",
    "    if kappa is None:\n",
    "        kappa = np.maximum(3.0 - n_features, 0.0)\n",
    "\n",
    "    lam = alpha**2 * (n_features + kappa) - n_features\n",
    "    c = np.sqrt(n_features + lam)\n",
    "    return np.hstack(\n",
    "        (np.zeros((n_features, 1)), c * np.eye(n_features), -c * np.eye(n_features))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aD1ALhejdp62"
   },
   "outputs": [],
   "source": [
    "kappa = None\n",
    "alpha = 1.0\n",
    "n_features = 1\n",
    "n_sigma_points = 3\n",
    "\n",
    "\n",
    "# generate sigma points\n",
    "sigma_pts = get_unscented_sigma_points(n_features, kappa, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUBLLbukbYQ6"
   },
   "outputs": [],
   "source": [
    "# input covariance\n",
    "L = jnp.linalg.cholesky(input_cov)\n",
    "\n",
    "chex.assert_shape(\n",
    "    L,\n",
    "    (\n",
    "        n_features,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# calculate sigma points\n",
    "x_sigma_samples = xtest_noisy + L @ sigma_pts\n",
    "\n",
    "chex.assert_shape(x_sigma_samples, (n_features, n_sigma_points))\n",
    "\n",
    "# mean function predictions\n",
    "\n",
    "f = lambda x: jnp.atleast_1d(meanf(x).squeeze())\n",
    "\n",
    "y_mu_unc = jax.vmap(f, in_axes=2)(x_sigma_samples[:, None, :])\n",
    "\n",
    "y_mu_unc = jnp.atleast_1d(y_mu_unc)\n",
    "\n",
    "chex.assert_shape(\n",
    "    y_mu_unc,\n",
    "    (\n",
    "        n_sigma_points,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# calculate weights of unscented points\n",
    "y_mu = y_mu_unc.T @ Wm\n",
    "\n",
    "\n",
    "chex.assert_shape(y_mu, (n_features,))\n",
    "\n",
    "# calculate covariance of MC samples\n",
    "dfx = y_mu_unc - y_mu\n",
    "\n",
    "chex.assert_shape(\n",
    "    dfx,\n",
    "    (\n",
    "        n_sigma_points,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "y_cov = dfx.T @ Wc @ dfx\n",
    "\n",
    "chex.assert_shape(\n",
    "    y_cov,\n",
    "    (\n",
    "        n_features,\n",
    "        n_features,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# input-output covariance\n",
    "y_covx = wc * (x_sigma_samples - xtest_noisy) @ dfx\n",
    "\n",
    "chex.assert_shape(\n",
    "    y_cov,\n",
    "    (\n",
    "        n_features,\n",
    "        n_features,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i77i9sFebcEw"
   },
   "outputs": [],
   "source": [
    "mu_demo[\"sigma\"] = y_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "id": "pl_PwRgYgZJn",
    "outputId": "fc462438-237a-40e4-b5fc-e7c3a4f50685"
   },
   "outputs": [],
   "source": [
    "# plt.hist(x_mc_samples.squeeze(), bins=100, density=True);\n",
    "fig, ax = plt.subplots(figsize=(7, 1))\n",
    "sns.kdeplot(\n",
    "    ax=ax,\n",
    "    x=x_mc_samples.squeeze(),\n",
    "    bw_adjust=3.0,\n",
    "    color=\"black\",\n",
    "    linewidth=4,\n",
    "    fill=True,\n",
    "    label=r\"$p(x)$\",\n",
    ")\n",
    "ax.axvline(xtest_noisy, linestyle=\"--\", color=\"black\", linewidth=4)\n",
    "ax.axvline(sigma_pts[:, 0], linestyle=\"--\", color=\"red\", linewidth=4)\n",
    "ax.axvline(sigma_pts[:, 1], linestyle=\"--\", color=\"red\", linewidth=2)\n",
    "ax.axvline(sigma_pts[:, 2], linestyle=\"--\", color=\"red\", linewidth=2)\n",
    "ax.set(yticklabels=\"\", ylabel=\"\", xticklabels=\"\", xlim=(-10, 10))\n",
    "# ax.set_ylim(-1.5, 1.1)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFU5x2yghltr"
   },
   "outputs": [],
   "source": [
    "# generate samples from the distribution\n",
    "unscented_samples = jax.random.multivariate_normal(\n",
    "    key, jnp.atleast_1d(y_mu), y_cov, (n_mc_points,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "aT1oW8zcg4Vy",
    "outputId": "f1ba63d4-046a-4b32-b636-e585e98c2411"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 5))\n",
    "sns.kdeplot(\n",
    "    ax=ax,\n",
    "    y=unscented_samples.ravel(),\n",
    "    bw_adjust=3.0,\n",
    "    linewidth=4,\n",
    "    color=\"red\",\n",
    "    label=\"Unscented\",\n",
    ")\n",
    "ax.set(xlabel=\"\", xticklabels=\"\", ylim=(-2.1, 2.1), yticklabels=\"\")\n",
    "ax.axhline(y_mu_unc[0], linestyle=\"--\", color=\"red\", linewidth=4)\n",
    "ax.axhline(y_mu_unc[1], linestyle=\"--\", color=\"red\", linewidth=4)\n",
    "ax.axhline(y_mu_unc[2], linestyle=\"--\", color=\"red\", linewidth=4)\n",
    "ax.axhline(mu_demo[\"standard\"], color=\"black\", linestyle=\"--\", label=\"Mean Prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "yYPu6d4XbiCF",
    "outputId": "dcf99b2c-b4c1-4605-d69f-14a6f8c4eb0d"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 5))\n",
    "sns.kdeplot(ax=ax, y=y_mu_mc.ravel(), bw_adjust=3.0, linewidth=4, label=\"MC Samples\")\n",
    "sns.kdeplot(\n",
    "    ax=ax,\n",
    "    y=lin_samples.ravel(),\n",
    "    bw_adjust=3.0,\n",
    "    linewidth=4,\n",
    "    label=r\"Taylor Approx ($\\mathcal{O}(1)$)\",\n",
    ")\n",
    "sns.kdeplot(\n",
    "    ax=ax,\n",
    "    y=lin_o2_samples.ravel(),\n",
    "    bw_adjust=3.0,\n",
    "    linewidth=4,\n",
    "    label=r\"Taylor Approx ($\\mathcal{O}(2)$)\",\n",
    ")\n",
    "sns.kdeplot(\n",
    "    ax=ax,\n",
    "    y=unscented_samples.ravel(),\n",
    "    bw_adjust=3.0,\n",
    "    linewidth=4,\n",
    "    color=\"red\",\n",
    "    label=\"Unscented\",\n",
    ")\n",
    "ax.set(xlabel=\"\", xticklabels=\"\", ylim=(-2.1, 2.1), yticklabels=\"\")\n",
    "ax.axhline(mu_demo[\"standard\"], color=\"black\", linestyle=\"--\", label=\"Mean Prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iedjuwNiw7l"
   },
   "source": [
    "### Vectorized Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fsLlRlhiyDn"
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def init_unscented_transform(\n",
    "    meanf: Callable,\n",
    "    n_features: int,\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 2.0,\n",
    "    kappa: Optional[float] = None,\n",
    "    covariance: bool = False,\n",
    ") -> Callable:\n",
    "\n",
    "    f = lambda x: jnp.atleast_1d(meanf(x).squeeze())\n",
    "\n",
    "    # get weights\n",
    "    wm, wc = get_unscented_weights(n_features, kappa, alpha, beta)\n",
    "\n",
    "    Wm, Wc = wm, jnp.diag(wc)\n",
    "\n",
    "    # generate sigma points\n",
    "    sigma_pts = get_unscented_sigma_points(n_features, kappa, alpha)\n",
    "\n",
    "    def apply_transform(x, x_cov):\n",
    "\n",
    "        # cholesky decomposition\n",
    "        L = jnp.linalg.cholesky(x_cov)\n",
    "\n",
    "        # calculate sigma points\n",
    "        x_sigma_samples = x + L @ sigma_pts\n",
    "\n",
    "        # propagate samples through function\n",
    "        y_mu_unc = jax.vmap(f, in_axes=2)(x_sigma_samples[:, None, :])\n",
    "\n",
    "        y_mu_unc = jnp.atleast_1d(y_mu_unc)\n",
    "\n",
    "        # ===================\n",
    "        # Mean\n",
    "        # ===================\n",
    "\n",
    "        # calculate weights of unscented points\n",
    "        y_mu = y_mu_unc.T @ Wm\n",
    "\n",
    "        # ===================\n",
    "        # Covariance\n",
    "        # ===================\n",
    "\n",
    "        # calculate covariance of MC samples\n",
    "        dfx = y_mu_unc - y_mu\n",
    "\n",
    "        y_cov = dfx.T @ Wc @ dfx\n",
    "\n",
    "        if not covariance:\n",
    "            y_cov = jnp.diag(y_cov)\n",
    "\n",
    "            y_cov = jnp.atleast_1d(y_cov)\n",
    "\n",
    "        return y_mu, y_cov\n",
    "\n",
    "    return apply_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eulPgO73kAKG"
   },
   "outputs": [],
   "source": [
    "# init function\n",
    "unscented_transform = init_unscented_transform(meanf, 1)\n",
    "\n",
    "# apply function\n",
    "xtest_noisy = Xtest_noisy[demo_sample_idx]\n",
    "input_cov = jnp.array([x_noise]).reshape(-1, 1)\n",
    "\n",
    "y_mu, y_var = unscented_transform(xtest_noisy, input_cov)\n",
    "\n",
    "chex.assert_equal_shape([y_mu, y_var])\n",
    "chex.assert_shape(y_mu, (n_features,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNjmk_L-h1Fj"
   },
   "outputs": [],
   "source": [
    "unscented_transform_vectorized = jax.jit(\n",
    "    jax.vmap(unscented_transform, in_axes=(0, None))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNayztFMkZ4i"
   },
   "outputs": [],
   "source": [
    "y_mu, y_var = unscented_transform_vectorized(Xtest_noisy, input_cov)\n",
    "\n",
    "chex.assert_equal_shape([y_mu, y_var])\n",
    "chex.assert_shape(\n",
    "    y_mu,\n",
    "    (\n",
    "        ntest,\n",
    "        n_features,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "itmBS-6Xkin3",
    "outputId": "2381b935-8fa2-4254-ad1b-9718e2002418"
   },
   "outputs": [],
   "source": [
    "# y_var = var.squeeze() + y_var_o2.squeeze()\n",
    "\n",
    "plot_1D_GP_noisy(Xtest_noisy, y_mu, y_var);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIIG6no-XkXk"
   },
   "outputs": [],
   "source": [
    "from jaxkern.gp.uncertain.linear import TaylorFirstOrder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Qc7Sun9XiRi"
   },
   "outputs": [],
   "source": [
    "# calculate the mean\n",
    "lin_mu = f(x_mu[:, None])\n",
    "\n",
    "# calculate the covariance\n",
    "lin_cov = df(x_mu[:, None]) @ x_cov @ df(x_mu[:, None]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evQ-nQMYXJU0",
    "outputId": "e85a27a3-1c92-47bc-939e-0a35d575e5e2"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean: {lin_mu}\")\n",
    "print(f\"Covariance: {lin_cov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TziaPctUVX6"
   },
   "outputs": [],
   "source": [
    "# generate samples from the distribution\n",
    "lin_samples = jax.random.multivariate_normal(key, lin_mu, lin_cov, (n_mc_points,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "FQ1HjuxeYQtL",
    "outputId": "54ae0997-67e9-45d6-9e10-2cb9be041546"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 5))\n",
    "sns.kdeplot(ax=ax, y=y_mc_samples, bw_adjust=3.0, linewidth=4, label=\"MC Samples\")\n",
    "sns.kdeplot(\n",
    "    ax=ax, y=lin_samples.squeeze(), bw_adjust=3.0, linewidth=4, label=\"Taylor Approx\"\n",
    ")\n",
    "# sns.kdeplot(ax=ax, y=mm_samples.squeeze(), bw_adjust=3., linewidth=4, label='Moment Matching',)\n",
    "ax.set(xlabel=\"\", xticklabels=\"\", ylim=(-2.1, 2.1), yticklabels=\"\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCI6y74RYdIK"
   },
   "source": [
    "### Moment Matching Transformation\n",
    "\n",
    "So explicitly, we need to take expectations (integrals) of the GP predictive mean and variance w.r.t. our distribution for $\\mathbf{x}_*$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{\\mu}_{GP}(\\mathbf{x_*}) &= \\int \\mu_{GP}(\\mathbf{x_*}) p(\\mathbf{x_*})d\\mathbf{x_*} \\\\\n",
    "\\tilde{\\sigma}^2_{GP}(\\mathbf{x}_*) &= \\int \\sigma^2_{GP}(\\mathbf{x_*}) p(\\mathbf{x_*}) d\\mathbf{x}_* + \\int  \\mu_{GP}^2(\\mathbf{x_*})p(\\mathbf{x_*})d\\mathbf{x_*}  - \\left[ \\int \\mu_{GP}(\\mathbf{x_*}) p(\\mathbf{x_*})d\\mathbf{x_*}\\right]^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "After some manipulation, this results in the follow equations for the predictive mean and variance (**cite**).\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{\\mu}_{GP}(\\mathbf{x_*}) &= \\Psi_1^\\top\\alpha \\\\\n",
    "\\tilde{\\sigma}^2_{GP}(\\mathbf{x}_*)\n",
    "&=\n",
    "\\psi_0 - \\text{Tr}\\left( \\left(\\mathbf{K}_{GP}^{-1}  - \\alpha\\alpha^\\top\\right) \\Psi_2\\right) - \\text{Tr}\\left( \\Psi_1\\Psi_1^\\top\\alpha\\alpha^\\top \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where we have $\\Psi_i$ quantities called kernel expectations denoted by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "[\\Psi_0][\\Psi_0]_{i}  &= \\int k(\\mathbf{x}_i, \\mathbf{x}_i)p(\\mathbf{x}_i)d\\mathbf{x}_i \\\\\n",
    "[\\Psi_1]_{ij} &= \\int k(\\mathbf{x}_i, \\mathbf{y}_i)p(\\mathbf{x}_i)d\\mathbf{x}_i \\\\\n",
    "[\\Psi_2]_{ijk} &= \\int k(\\mathbf{x}_i, \\mathbf{y}_j)k(\\mathbf{x}_i, \\mathbf{z}_k) d\\mathbf{x}_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice how we have expectations where we need to use quadrature methods to calculate these quantities. In this example, we will use the GaussHermite method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Io-HxGdFYelx"
   },
   "outputs": [],
   "source": [
    "from jaxkern.gp.uncertain.moment import (\n",
    "    MomentMatchingTransform,\n",
    "    GaussHermite,\n",
    "    get_quadrature_sigma_points,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yO7kDw2pUVor"
   },
   "outputs": [],
   "source": [
    "degree = 20\n",
    "\n",
    "# initialize model\n",
    "mm_clf = MomentMatchingTransform(model, GaussHermite, degree=degree, jitted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzdialhcUV-y"
   },
   "outputs": [],
   "source": [
    "# calculate kernel expectations\n",
    "psi0 = mm_clf.kernel_expectations.expectation_xkx(x_mu[:, None], x_cov[None, :])\n",
    "psi1 = mm_clf.kernel_expectations.expectation_xkxy(\n",
    "    x_mu[:, None], x_cov[None, :], mm_clf.model.X_train_\n",
    ")\n",
    "psi2 = mm_clf.kernel_expectations.expectation_xkxyz(\n",
    "    x_mu[:, None], x_cov[None, :], mm_clf.model.X_train_, mm_clf.model.X_train_\n",
    ")\n",
    "\n",
    "# calculate mean function\n",
    "mm_mu = psi1 @ mm_clf.model.weights\n",
    "\n",
    "# calculate covariance function\n",
    "t1 = psi0\n",
    "t2 = psi2 @ (mm_clf.K_inv_ - mm_clf.model.weights @ mm_clf.model.weights.T)\n",
    "t3 = psi1.T @ psi1 @ mm_clf.model.weights @ mm_clf.model.weights.T\n",
    "\n",
    "mm_cov = t1 - np.trace(t2, axis1=1, axis2=2) - np.trace(mm_mu**2)\n",
    "mm_cov = mm_cov.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NoQhoMGPZB_X",
    "outputId": "d6517c8f-9b45-4251-9992-4267afee04e2"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean: {mm_mu}\")\n",
    "print(f\"Covariance: {mm_cov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FBBDYuBUWJ9"
   },
   "outputs": [],
   "source": [
    "# generate samples from distribution\n",
    "mm_samples = jax.random.multivariate_normal(key, mm_mu, mm_cov, (n_mc_points,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "FZCdT6iKUWV0",
    "outputId": "464a8b02-2f83-41f6-89a2-0ea3b5bae71d"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3, 5))\n",
    "sns.kdeplot(ax=ax, y=y_mc_samples, bw_adjust=3.0, linewidth=4, label=\"MC Samples\")\n",
    "sns.kdeplot(\n",
    "    ax=ax, y=lin_samples.squeeze(), bw_adjust=3.0, linewidth=4, label=\"Taylor Approx\"\n",
    ")\n",
    "sns.kdeplot(\n",
    "    ax=ax,\n",
    "    y=mm_samples.squeeze(),\n",
    "    bw_adjust=3.0,\n",
    "    linewidth=4,\n",
    "    label=\"Moment Matching\",\n",
    ")\n",
    "# ax.set_ylim(-1.5, 1.1)\n",
    "ax.set(xlabel=\"\", xticklabels=\"\", ylim=(-2.1, 2.1), yticklabels=\"\")\n",
    "# ax.set_xticklabels(\"\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "Lypq4oK1ZbP-",
    "outputId": "6144d328-0066-407b-f04b-a9b734d51e8f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.kdeplot(ax=ax, x=y_mc_samples, bw_adjust=3.0, linewidth=4, label=\"MC Samples\")\n",
    "\n",
    "sns.kdeplot(\n",
    "    ax=ax,\n",
    "    x=lin_samples.squeeze(),\n",
    "    bw_adjust=3.0,\n",
    "    linewidth=4,\n",
    "    label=\"Taylor Approx\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "ax.axvline(\n",
    "    lin_mu, linestyle=\"--\", label=\"Mean (Taylor Approx.)\", linewidth=2, color=\"orange\"\n",
    ")\n",
    "sns.kdeplot(\n",
    "    ax=ax,\n",
    "    x=mm_samples.squeeze(),\n",
    "    bw_adjust=3.0,\n",
    "    linewidth=4,\n",
    "    label=\"Moment Matching\",\n",
    "    color=\"green\",\n",
    ")\n",
    "ax.axvline(\n",
    "    mm_mu, linestyle=\"--\", label=\"Mean (Moment Matching)\", linewidth=2, color=\"green\"\n",
    ")\n",
    "# ax.set_ylim(-1.5, 1.1)\n",
    "ax.set(xlabel=\"\", xticklabels=\"\", yticklabels=\"\", ylim=(0, 2.0), ylabel=\"\")\n",
    "# ax.set_xticklabels(\"\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTlbItB8jcdS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpjax_egp_moment_kernels.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-jax_py38]",
   "language": "python",
   "name": "conda-env-.conda-jax_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
