---
---
@article{perez2011python,
  title     = {Python: an ecosystem for scientific computing},
  author    = {Perez, Fernando and Granger, Brian E and Hunter, John D},
  journal   = {Computing in Science \\& Engineering},
  volume    = {13},
  number    = {2},
  pages     = {13--21},
  year      = {2011},
  publisher = {AIP Publishing}
}

@article{holdgraf_encoding_2017,
  title    = {Encoding and decoding models in cognitive electrophysiology},
  volume   = {11},
  issn     = {16625137},
  doi      = {10.3389/fnsys.2017.00061},
  abstract = {© 2017 Holdgraf, Rieger, Micheli, Martin, Knight and Theunissen. Cognitive neuroscience has seen rapid growth in the size and complexity of data recorded from the human brain as well as in the computational tools available to analyze this data. This data explosion has resulted in an increased use of multivariate, model-based methods for asking neuroscience questions, allowing scientists to investigate multiple hypotheses with a single dataset, to use complex, time-varying stimuli, and to study the human brain under more naturalistic conditions. These tools come in the form of “Encoding” models, in which stimulus features are used to model brain activity, and “Decoding” models, in which neural features are used to generated a stimulus output. Here we review the current state of encoding and decoding models in cognitive electrophysiology and provide a practical guide toward conducting experiments and analyses in this emerging field. Our examples focus on using linear models in the study of human language and audition. We show how to calculate auditory receptive fields from natural sounds as well as how to decode neural recordings to predict speech. The paper aims to be a useful tutorial to these approaches, and a practical introduction to using machine learning and applied statistics to build models of neural activity. The data analytic approaches we discuss may also be applied to other sensory modalities, motor systems, and cognitive systems, and we cover some examples in these areas. In addition, a collection of Jupyter notebooks is publicly available as a complement to the material covered in this paper, providing code examples and tutorials for predictive modeling in python. The aimis to provide a practical understanding of predictivemodeling of human brain data and to propose best-practices in conducting these analyses.},
  journal  = {Frontiers in Systems Neuroscience},
  author   = {Holdgraf, Christopher Ramsay and Rieger, J.W. and Micheli, C. and Martin, S. and Knight, R.T. and Theunissen, F.E.},
  year     = {2017},
  keywords = {Decoding models, Encoding models, Electrocorticography (ECoG), Electrophysiology/evoked potentials, Machine learning applied to neuroscience, Natural stimuli, Predictive modeling, Tutorials}
}


@inproceedings{holdgraf_evidence_2014,
  address   = {Brisbane, Australia, Australia},
  title     = {Evidence for {Predictive} {Coding} in {Human} {Auditory} {Cortex}},
  booktitle = {International {Conference} on {Cognitive} {Neuroscience}},
  publisher = {Frontiers in Neuroscience},
  author    = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Knight, Robert T.},
  year      = {2014}
}

@inproceedings{holdgraf_portable_2017,
  title    = {Portable learning environments for hands-on computational instruction using container-and cloud-based technology to teach data science},
  volume   = {Part F1287},
  isbn     = {978-1-4503-5272-7},
  doi      = {10.1145/3093338.3093370},
  abstract = {© 2017 ACM. There is an increasing interest in learning outside of the traditional classroom setting. This is especially true for topics covering computational tools and data science, as both are challenging to incorporate in the standard curriculum. These atypical learning environments offer new opportunities for teaching, particularly when it comes to combining conceptual knowledge with hands-on experience/expertise with methods and skills. Advances in cloud computing and containerized environments provide an attractive opportunity to improve the effciency and ease with which students can learn. This manuscript details recent advances towards using commonly-Available cloud computing services and advanced cyberinfrastructure support for improving the learning experience in bootcamp-style events. We cover the benets (and challenges) of using a server hosted remotely instead of relying on student laptops, discuss the technology that was used in order to make this possible, and give suggestions for how others could implement and improve upon this model for pedagogy and reproducibility.},
  author   = {Holdgraf, Christopher Ramsay and Culich, A. and Rokem, A. and Deniz, F. and Alegro, M. and Ushizima, D.},
  year     = {2017},
  keywords = {Teaching, Bootcamps, Cloud computing, Data science, Docker, Pedagogy}
}

@article{holdgraf_rapid_2016,
  title   = {Rapid tuning shifts in human auditory cortex enhance speech intelligibility},
  volume  = {7},
  issn    = {2041-1723},
  url     = {http://www.nature.com/doifinder/10.1038/ncomms13654},
  doi     = {10.1038/ncomms13654},
  number  = {May},
  journal = {Nature Communications},
  author  = {Holdgraf, Christopher Ramsay and de Heer, Wendy and Pasley, Brian N. and Rieger, Jochem W. and Crone, Nathan and Lin, Jack J. and Knight, Robert T. and Theunissen, Frédéric E.},
  year    = {2016},
  pages   = {13654},
  file    = {Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:C\:\\Users\\chold\\Zotero\\storage\\MDQP3JWE\\Holdgraf et al. - 2016 - Rapid tuning shifts in human auditory cortex enhance speech intelligibility.pdf:application/pdf}
}

@misc{kornblith2019similarity,
  title         = {Similarity of Neural Network Representations Revisited},
  author        = {Simon Kornblith and Mohammad Norouzi and Honglak Lee and Geoffrey Hinton},
  year          = {2019},
  eprint        = {1905.00414},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@book{ruby,
  title     = {The Ruby Programming Language},
  author    = {Flanagan, David and Matsumoto, Yukihiro},
  year      = {2008},
  publisher = {O'Reilly Media}
}
-----------------------------
# KERNEL METHODS
-----------------------------
@article{kmfe2020,
  author         = {Pilario, Karl Ezra and Shafiee, Mahmood and Cao, Yi and Lao, Liyun and Yang, Shuang-Hua},
  title          = {A Review of Kernel Methods for Feature Extraction in Nonlinear Process Monitoring},
  journal        = {Processes},
  volume         = {8},
  year           = {2020},
  number         = {1},
  article-number = {24},
  url            = {https://www.mdpi.com/2227-9717/8/1/24},
  issn           = {2227-9717},
  abstract       = {Kernel methods are a class of learning machines for the fast recognition of nonlinear patterns in any data set. In this paper, the applications of kernel methods for feature extraction in industrial process monitoring are systematically reviewed. First, we describe the reasons for using kernel methods and contextualize them among other machine learning tools. Second, by reviewing a total of 230 papers, this work has identified 12 major issues surrounding the use of kernel methods for nonlinear feature extraction. Each issue was discussed as to why they are important and how they were addressed through the years by many researchers. We also present a breakdown of the commonly used kernel functions, parameter selection routes, and case studies. Lastly, this review provides an outlook into the future of kernel-based process monitoring, which can hopefully instigate more advanced yet practical solutions in the process industries.},
  doi            = {10.3390/pr8010024}
}
@misc{belkin2018understand,
  author    = {Mikhail Belkin and
               Siyuan Ma and
               Soumik Mandal},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {To Understand Deep Learning We Need to Understand Kernel Learning},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {540--548},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/belkin18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/BelkinMM18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@book{Rojo18dspkm,
  title     = { Digital Signal Processing with Kernel Methods },
  author    = { Rojo-Álvarez, J.L. and Martínez-Ramón, M. and Muñoz-Marí, J. and Camps-Valls, G. },
  publisher = { Wiley \& Sons },
  year      = { 2018 },
  address   = { UK },
  month     = { Apr },
  code      = { https://github.com/DSPKM },
  isbn      = { 978-1118611791 },
  url       = { https://www.wiley.com/en-es/Digital+Signal+Processing+with+Kernel+Methods-p-9781118611791 }
}
@book{CampsValls09wiley,
  author    = { Camps-Valls, G. and Bruzzone, L. },
  title     = { Kernel methods for Remote Sensing Data Analysis },
  publisher = { Wiley & Sons },
  year      = { 2009 },
  address   = { UK },
  month     = { Dec },
  isbn      = { 978-0-470-72211-4 }
}
@misc{lopezpaz2016dependence,
  title         = {From Dependence to Causation},
  author        = {David Lopez-Paz},
  year          = {2016},
  eprint        = {1607.03300},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}
  
  

---------------
# COEFFICIENTS
---------------
@article{burt1948,
  title   = {Factor analysis and canonical correlations},
  author  = {Cyril Burt},
  year    = {1948},
  journal = {British Journal of Psychology},
  section = {Statistical Section},
  volume  = {1},
  pages   = {95–106}
}
@article{tucker1951,
  title   = {A method for the synthesis of factor analysis
studies},
  author  = {L.R. Tucker},
  year    = {1951},
  journal = {Washington: Department of the Army},
  volume  = {984}
}
@book{harman1976,
  title     = {Modern Factor Analysis},
  author    = {H.H. Harman},
  year      = {1976},
  publisher = {Chicago: Chicago University Press}
}
@book{borg1997modern,
  title     = {Modern Multidimensional Scaling: Theory and Applications},
  author    = {Borg, I. and Groenen, P.J.F.},
  isbn      = {9780387948454},
  lccn      = {96030269},
  series    = {Springer series in statistics},
  url       = {https://books.google.es/books?id=KZ8koJkSVEcC},
  year      = {1997},
  publisher = {Springer}
}
@article{josse2016,
  author    = {Josse, Julie and Holmes, Susan},
  doi       = {10.1214/16-SS116},
  fjournal  = {Statistics Surveys},
  journal   = {Statistics Survey},
  pages     = {132--167},
  publisher = {The American Statistical Association, the Bernoulli Society, the Institute of Mathematical Statistics, and the Statistical Society of Canada},
  title     = {Measuring multivariate association and beyond},
  url       = {https://doi.org/10.1214/16-SS116},
  volume    = {10},
  year      = {2016}
}
@book{purdom2006multivariate,
  title     = {Multivariate Kernel Methods in the Analysis of Graphical Structures},
  author    = {Purdom, E. and Stanford University. Department of Statistics},
  url       = {https://books.google.es/books?id=xQbbAQAACAAJ},
  year      = {2006},
  publisher = {Stanford University}
}
@incollection{kta2002,
  title     = {On Kernel-Target Alignment},
  author    = {Nello Cristianini and John Shawe-Taylor and Andr\'{e} Elisseeff and Jaz S. Kandola},
  booktitle = {Advances in Neural Information Processing Systems 14},
  editor    = {T. G. Dietterich and S. Becker and Z. Ghahramani},
  pages     = {367--373},
  year      = {2002},
  publisher = {MIT Press},
  url       = {http://papers.nips.cc/paper/1946-on-kernel-target-alignment.pdf}
}
@article{Sejdinovic_2013,
  title     = {Equivalence of distance-based and RKHS-based statistics in hypothesis testing},
  volume    = {41},
  issn      = {0090-5364},
  url       = {http://dx.doi.org/10.1214/13-AOS1140},
  doi       = {10.1214/13-aos1140},
  number    = {5},
  journal   = {The Annals of Statistics},
  publisher = {Institute of Mathematical Statistics},
  author    = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
  year      = {2013},
  month     = {Oct},
  pages     = {2263–2291}
}
@article{Holmes_2008,
  title     = {Multivariate data analysis: The French way},
  isbn      = {0940600749},
  url       = {http://dx.doi.org/10.1214/193940307000000455},
  doi       = {10.1214/193940307000000455},
  journal   = {Probability and Statistics: Essays in Honor of David A. Freedman},
  publisher = {Institute of Mathematical Statistics},
  author    = {Holmes, Susan},
  year      = {2008},
  pages     = {219–233}
}
@misc{cortes2014,
  author     = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
  title      = {Algorithms for Learning Kernels Based on Centered Alignment},
  year       = {2012},
  issue_date = {January 2012},
  publisher  = {JMLR.org},
  volume     = {13},
  number     = {1},
  issn       = {1532-4435},
  abstract   = {This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difficult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classification and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efficient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classification and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classification and regression.},
  journal    = {J. Mach. Learn. Res.},
  month      = mar,
  pages      = {795–828},
  numpages   = {34},
  keywords   = {learning kernels, feature selection, kernel methods}
}
@article{szekely2007,
  author    = {Székely, Gábor J. and Rizzo, Maria L. and Bakirov, Nail K.},
  doi       = {10.1214/009053607000000505},
  fjournal  = {Annals of Statistics},
  journal   = {Ann. Statist.},
  month     = {12},
  number    = {6},
  pages     = {2769--2794},
  publisher = {The Institute of Mathematical Statistics},
  title     = {Measuring and testing dependence by correlation of distances},
  url       = {https://doi.org/10.1214/009053607000000505},
  volume    = {35},
  year      = {2007}
}
@article{Robert1976,
  issn      = {00359254, 14679876},
  url       = {http://www.jstor.org/stable/2347233},
  abstract  = {Consider two data matrices on the same sample of n individuals, X(p × n), Y(q × n). From these matrices, geometrical representations of the sample are obtained as two configurations of n points, in Rp and Rq. It is shown that the RV-coefficient (Escoufier, 1970, 1973) can be used as a measure of similarity of the two configurations, taking into account the possibly distinct metrics to be used on them to measure the distances between points. The purpose of this paper is to show that most classical methods of linear multivariate statistical analysis can be interpreted as the search for optimal linear transformations or, equivalently, the search for optimal metrics to apply on two data matrices on the same sample; the optimality is defined in terms of the similarity of the corresponing configurations of points. which, in turn, calls for the maximization of the associated RV-coefficient. The methods studied are principal components, principal components of instrumental variables, multivariate regression, canonical variables, discriminant analysis; they are differentiated by the possible relationships existing between the two data matrices involved and by additional constraints under which the maximum of RV is to be obtained. It is also shown that the RV-coefficient can be used as a measure of goodness of a solution to the problem of discarding variables.},
  author    = {P. Robert and Y. Escoufier},
  journal   = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  number    = {3},
  pages     = {257--265},
  publisher = {[Wiley, Royal Statistical Society]},
  title     = {A Unifying Tool for Linear Multivariate Statistical Methods: The RV- Coefficient},
  volume    = {25},
  year      = {1976}
}
@article{Escoufier1973,
  issn      = {0006341X, 15410420},
  url       = {http://www.jstor.org/stable/2529140},
  abstract  = {Multivariate statistical techniques are based upon the Hilbert space Structure of the set of random variables with finite variance, and are therefore generalizable to any set having such a structure. In Particular, since certain applications lead to families of vector random variables we are led to the problem of finding the Hilbert space which permits the above generalization to families of vector variables. The inner product which is used possesses theoretical and practical properties which are discussed. An illustrative example is given.},
  author    = {Yves Escoufier},
  journal   = {Biometrics},
  number    = {4},
  pages     = {751--760},
  publisher = {[Wiley, International Biometric Society]},
  title     = {Le Traitement des Variables Vectorielles},
  volume    = {29},
  year      = {1973}
}
@article{De_la_Cruz_2011,
  title     = {The duality diagram in data analysis: Examples of modern applications},
  volume    = {5},
  issn      = {1932-6157},
  url       = {http://dx.doi.org/10.1214/10-AOAS408},
  doi       = {10.1214/10-aoas408},
  number    = {4},
  journal   = {The Annals of Applied Statistics},
  publisher = {Institute of Mathematical Statistics},
  author    = {De la Cruz, Omar and Holmes, Susan},
  year      = {2011},
  month     = {Dec},
  pages     = {2266–2277}
}

--------------------------
# INFORMATION THEORY
--------------------------
@article{vivinh10a,
  author  = {Nguyen Xuan Vinh and Julien Epps and James Bailey},
  title   = {Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {95},
  pages   = {2837-2854},
  url     = {http://jmlr.org/papers/v11/vinh10a.html}
}
@article{MeilaVI,
  title    = {Comparing clusterings—an information based distance},
  journal  = {Journal of Multivariate Analysis},
  volume   = {98},
  number   = {5},
  pages    = {873 - 895},
  year     = {2007},
  issn     = {0047-259X},
  doi      = {https://doi.org/10.1016/j.jmva.2006.11.013},
  url      = {http://www.sciencedirect.com/science/article/pii/S0047259X06002016},
  author   = {Marina Meilă},
  keywords = {Agreement measures, Clustering, Comparing partitions, Information theory, Mutual information, Similarity measures},
  abstract = {This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information (VI), measures the amount of information lost and gained in changing from clustering C to clustering C′. The basic properties of VI are presented and discussed. We focus on two kinds of properties: (1) those that help one build intuition about the new criterion (in particular, it is shown the VI is a true metric on the space of clusterings), and (2) those that pertain to the comparability of VI values over different experimental conditions. As the latter properties have rarely been discussed explicitly before, other existing comparison criteria are also examined in their light. Finally we present the VI from an axiomatic point of view, showing that it is the only “sensible” criterion for comparing partitions that is both aligned to the lattice and convexely additive. As a consequence, we prove an impossibility result for comparing partitions: there is no criterion for comparing partitions that simultaneously satisfies the above two desirable properties and is bounded.}
}

--------------------------
# SENSITIVITY ANALYSIS
--------------------------
@book{Saltelli2000,
  title     = {Sensitivity Analysis},
  author    = {Andrea Saltelli and K. Chan and E. M. Scott},
  year      = {2000},
  publisher = { John Wiley & Sons}
}
@book{faivre2016,
  title     = {Analyse de sensibilit{\'e} et exploration de mod{\`e}les: Application aux sciences de la nature et de l'environnement},
  author    = {Faivre, R. and Iooss, B. and Mah{\'e}vas, S. and Makowski, D. and Monod, H.},
  isbn      = {9782759219063},
  series    = {Collection Savoir-faire},
  url       = {https://books.google.es/books?id=6JZP7Uy4fD4C},
  year      = {2016},
  publisher = {Quae {\'e}ditions}
}

@article{French2003,
  author   = {Simon French},
  title    = {{Modelling, making inferences and making decisions: The roles of sensitivity analysis}},
  journal  = {TOP: An Official Journal of the Spanish Society of Statistics and Operations Research},
  year     = 2003,
  volume   = {11},
  number   = {2},
  pages    = {229-251},
  month    = {December},
  keywords = {Bayesian methods; decision analysis; elicitation; expert judgement; inference; Monte Carlo; risk ana},
  doi      = {10.1007/BF02579043},
  abstract = {No abstract is available for this item.},
  url      = {https://ideas.repec.org/a/spr/topjnl/v11y2003i2p229-251.html}
}
@article{Borgonovo2016,
  title    = {Sensitivity analysis: A review of recent advances},
  journal  = {European Journal of Operational Research},
  volume   = {248},
  number   = {3},
  pages    = {869 - 887},
  year     = {2016},
  issn     = {0377-2217},
  doi      = {https://doi.org/10.1016/j.ejor.2015.06.032},
  url      = {http://www.sciencedirect.com/science/article/pii/S0377221715005469},
  author   = {Emanuele Borgonovo and Elmar Plischke},
  keywords = {Sensitivity analysis, Simulation, Computer experiments},
  abstract = {The solution of several operations research problems requires the creation of a quantitative model. Sensitivity analysis is a crucial step in the model building and result communication process. Through sensitivity analysis we gain essential insights on model behavior, on its structure and on its response to changes in the model inputs. Several interrogations are possible and several sensitivity analysis methods have been developed, giving rise to a vast and growing literature. We present an overview of available methods, structuring them into local and global methods. For local methods, we discuss Tornado diagrams, one way sensitivity functions, differentiation-based methods and scenario decomposition through finite change sensitivity indices, providing a unified view of the associated sensitivity measures. We then analyze global sensitivity methods, first discussing screening methods such as sequential bifurcation and the Morris method. We then address variance-based, moment-independent and value of information-based sensitivity methods. We discuss their formalization in a common rationale and present recent results that permit the estimation of global sensitivity measures by post-processing the sample generated by a traditional Monte Carlo simulation. We then investigate in detail the methodological issues concerning the crucial step of correctly interpreting the results of a sensitivity analysis. A classical example is worked out to illustrate some of the approaches.}
}
@inbook{Zhou2008,
  author    = {Zhou, Xiaobo and Lin, Henry},
  editor    = {Shekhar, Shashi and Xiong, Hui},
  title     = {Global Sensitivity Analysis},
  booktitle = {Encyclopedia of GIS},
  year      = {2008},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {408--409},
  isbn      = {978-0-387-35973-1},
  doi       = {10.1007/978-0-387-35973-1_538},
  url       = {https://doi.org/10.1007/978-0-387-35973-1_538}
}
@article{DaVeiga2015,
  author    = {Sebastien Da Veiga},
  title     = {Global sensitivity analysis with dependence measures},
  journal   = {Journal of Statistical Computation and Simulation},
  volume    = {85},
  number    = {7},
  pages     = {1283-1305},
  year      = {2015},
  publisher = {Taylor & Francis},
  doi       = {10.1080/00949655.2014.945932},
  url       = { https://doi.org/10.1080/00949655.2014.945932},
  eprint    = { https://doi.org/10.1080/00949655.2014.945932}
}
@article{Razavi2015,
  author   = {Razavi, Saman and Gupta, Hoshin V.},
  title    = {What do we mean by sensitivity analysis? The need for comprehensive characterization of “global” sensitivity in Earth and Environmental systems models},
  journal  = {Water Resources Research},
  volume   = {51},
  number   = {5},
  pages    = {3070-3092},
  keywords = {sensitivity analysis, Sobol, Morris, interaction effect, response surface, Earth and Environmental system models},
  doi      = {10.1002/2014WR016527},
  url      = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2014WR016527},
  eprint   = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/2014WR016527},
  abstract = {Abstract Sensitivity analysis is an essential paradigm in Earth and Environmental Systems modeling. However, the term “sensitivity” has a clear definition, based in partial derivatives, only when specified locally around a particular point (e.g., optimal solution) in the problem space. Accordingly, no unique definition exists for “global sensitivity” across the problem space, when considering one or more model responses to different factors such as model parameters or forcings. A variety of approaches have been proposed for global sensitivity analysis, based on different philosophies and theories, and each of these formally characterizes a different “intuitive” understanding of sensitivity. These approaches focus on different properties of the model response at a fundamental level and may therefore lead to different (even conflicting) conclusions about the underlying sensitivities. Here we revisit the theoretical basis for sensitivity analysis, summarize and critically evaluate existing approaches in the literature, and demonstrate their flaws and shortcomings through conceptual examples. We also demonstrate the difficulty involved in interpreting “global” interaction effects, which may undermine the value of existing interpretive approaches. With this background, we identify several important properties of response surfaces that are associated with the understanding and interpretation of sensitivities in the context of Earth and Environmental System models. Finally, we highlight the need for a new, comprehensive framework for sensitivity analysis that effectively characterizes all of the important sensitivity-related properties of model response surfaces.},
  year     = {2015}
}
@misc{meynaoui2019,
  title         = {New statistical methodology for second level global sensitivity analysis},
  author        = {Anouar Meynaoui and Amandine Marrel and Béatrice Laurent},
  year          = {2019},
  eprint        = {1902.07030},
  archiveprefix = {arXiv},
  primaryclass  = {math.ST}
}

--------------------------
# ESDC
--------------------------
@article{esdc2020,
  author  = {Mahecha, M. D. and Gans, F. and Brandt, G. and Christiansen, R. and Cornell, S. E. and Fomferra, N. and Kraemer, G. and Peters, J. and Bodesheim, P. and Camps-Valls, G. and Donges, J. F. and Dorigo, W. and Estupinan-Suarez, L. M. and Gutierrez-Velez, V. H. and Gutwin, M. and Jung, M. and Londo\~no, M. C. and Miralles, D. G. and Papastefanou, P. and Reichstein, M.},
  title   = {Earth system data cubes unravel global multivariate dynamics},
  journal = {Earth System Dynamics},
  volume  = {11},
  year    = {2020},
  number  = {1},
  pages   = {201--234},
  url     = {https://esd.copernicus.org/articles/11/201/2020/},
  doi     = {10.5194/esd-11-201-2020}
}
@article{dropout_2016,
  author    = {Yarin Gal and
               Zoubin Ghahramani},
  editor    = {Maria{-}Florina Balcan and
               Kilian Q. Weinberger},
  title     = {Dropout as a Bayesian Approximation: Representing Model Uncertainty
               in Deep Learning},
  booktitle = {Proceedings of the 33nd International Conference on Machine Learning,
               {ICML} 2016, New York City, NY, USA, June 19-24, 2016},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {48},
  pages     = {1050--1059},
  publisher = {JMLR.org},
  year      = {2016},
  url       = {http://proceedings.mlr.press/v48/gal16.html},
  timestamp = {Wed, 29 May 2019 08:41:46 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/GalG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}