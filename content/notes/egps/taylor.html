
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Taylor Approximation &#8212; Research Notebook</title>
    
  <link rel="stylesheet" href="../../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Moment Matching" href="moment_matching.html" />
    <link rel="prev" title="Monte Carlo Sampling" href="mcmc.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/book.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Research Notebook</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../intro.html">
   Overview
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../resources/python/overview.html">
   Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/ides.html">
     Integraded Development Environment (IDE)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/stack.html">
     Standard Python Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/earthsci_stack.html">
     Earth Science Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/dl_stack.html">
     Deep Learning Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/scale_stack.html">
     Scaling Stack
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../resources/python/good_code.html">
     Good Code
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../tutorials/jax_journey/overview.html">
   My JAX Journey
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/ecosystem.html">
     Ecosystem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/vmap.html">
     vmap
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/jit.html">
     Jit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/jax_journey/classes.html">
     Classes
    </a>
   </li>
   <li class="toctree-l2 collapsible-parent">
    <a class="reference internal" href="../../tutorials/jax_journey/algorithms/overview.html">
     Algorithms
    </a>
    <ul class="collapse-ul">
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/algorithms/bisection.html">
       Bisection search
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/algorithms/kernel_derivatives.html">
       Kernel Derivatives
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/jax_journey/algorithms/gpr.html">
       GP from Scratch
      </a>
     </li>
    </ul>
    <i class="fas fa-chevron-down">
    </i>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../tutorials/remote/overview.html">
   Remote Computing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/remote/vscode_jlab.html">
     JupyterLab + VSCode
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../bayesian/overview.html">
   Bayesian
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/intro.html">
     Bayesian: Language of Uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/gaussian.html">
     Gaussian Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/gaussian.html#gaussian-distribution">
     Gaussian Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/regression.html">
     Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/inference/inference.html">
     Solving Hard Integral Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/inference/inference.html#inference">
     Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/inference/variational_inference.html">
     Variational Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayesian/confidence_intervals.html">
     Confidence Intervals
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../concepts/overview.html">
   Sleeper Concepts
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/change_of_variables.html">
     Change of Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/identity_trick.html">
     Identity Trick
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/inverse_function.html">
     Inverse Function Theorem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/jensens.html">
     Jensens Inequality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../concepts/lin_alg.html">
     Linear Algebra Tricks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../kernels/overview.html">
   Kernel Methods
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../kernels/kernel_derivatives.html">
     Kernel Derivatives
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="overview.html">
   Uncertain Gaussian Processes
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="predictions.html">
     Uncertain Predictions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gauss_approx.html">
     Gaussian Approximation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="mcmc.html">
     Monte Carlo Sampling
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Taylor Approximation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="moment_matching.html">
     Moment Matching
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../bib.html">
   Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/content/notes/egps/taylor.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proof">
   Proof
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mean-function">
     Mean Function
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#zeroth-term">
       Zeroth Term
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#st-order-term">
       1st Order Term
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#nd-order-term">
       2nd Order Term
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linearized-gp-mean">
       Linearized GP Mean
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variance-function">
     Variance Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linearized-predictive-mean-and-variance">
     Linearized Predictive Mean and Variance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sparse-gps">
   Sparse GPs
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="taylor-approximation">
<h1>Taylor Approximation<a class="headerlink" href="#taylor-approximation" title="Permalink to this headline">¶</a></h1>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\tilde{\mathbf{\mu}}_\text{LinGP}(\mathbf{x_*}) &amp;= \mathbf{\mu}_\text{GP}(\mathbf{\mu}_\mathbf{x_*}) +
\underbrace{\frac{1}{2} \text{Tr}\left\{ \frac{\partial^2 \mathbf{\mu}_\text{GP}(\mathbf{\mu}_\mathbf{x_*})}{\partial \mathbf{x_*} \partial \mathbf{x_*}^\top}  \mathbf{\Sigma}_\mathbf{x_*}\right\}}_\text{second Order}\\
\tilde{\mathbf{\Sigma}}^2_\text{LinGP} (\mathbf{x_*}) &amp;= 
\mathbf{\Sigma}^2_\text{GP}(\mathbf{\mu}_\mathbf{x_*}) + 
\underbrace{\frac{\partial \mathbf{\mu}_\text{GP}(\mathbf{\mu}_\mathbf{x_*})}{\partial \mathbf{x_*}}^\top
\mathbf{\Sigma}_\mathbf{x_*}
\frac{\partial \mathbf{\mu}_\text{GP}(\mathbf{\mu}_\mathbf{x_*})}{\partial \mathbf{x_*}}}_\text{1st Order} +
\underbrace{\frac{1}{2} \text{Tr}\left\{ \frac{\partial^2 \mathbf{\Sigma}^2_\text{GP}(\mathbf{\mu}_\mathbf{x_*})}{\partial \mathbf{x_*} \partial \mathbf{x_*}^\top}  \mathbf{\Sigma}_\mathbf{x_*}\right\}}_\text{second Order}
\end{aligned}
\end{split}\]</div>
<hr class="docutils" />
<div class="section" id="proof">
<h2>Proof<a class="headerlink" href="#proof" title="Permalink to this headline">¶</a></h2>
<p>We will approximate our mean and variance function via a Taylor Expansion. First let’s take a step back and look at the taylor expansion of a single function <span class="math notranslate nohighlight">\(f\)</span> w.r.t. to <span class="math notranslate nohighlight">\(\mathbf{x}_*\)</span> which is characterized by its mean <span class="math notranslate nohighlight">\(\mu_\mathbf{x_*}\)</span> and variance function <span class="math notranslate nohighlight">\(\Sigma_\mathbf{x_*}\)</span>. Taking the first two orders, we get</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbf{z}_\mu = f(\mathbf{x_*}) &amp;\approx
\underbrace{f(\mu_\mathbf{x_*})}_\text{Zeroth Order} +
\underbrace{\nabla_\mathbf{x_*} f\bigg\vert_{\mathbf{x}_* = \mu_\mathbf{x}}
(\mathbf{x}_* - \mu_\mathbf{x_*})}_\text{1st Order} \\
&amp;+ \underbrace{\nabla^2_\mathbf{x_*} f \bigg\vert_{\mathbf{x}_* = \mu_\mathbf{x}}
(\mathbf{x}_* - \mu_\mathbf{x_*})^\top (\mathbf{x}_* - \mu_\mathbf{x_*})}_\text{2nd Order} +
\underbrace{\mathcal{O} (\mathbf{x_*}^3)}_\text{Higher Order}
\end{aligned}\end{split}\]</div>
<div class="section" id="mean-function">
<h3>Mean Function<a class="headerlink" href="#mean-function" title="Permalink to this headline">¶</a></h3>
<p>Now we need to take the expectation of our approximation, <span class="math notranslate nohighlight">\(\mathbb{E}[\mathbf{z}_\mu]\)</span>. We tackle each of the terms individually below.</p>
<div class="section" id="zeroth-term">
<h4>Zeroth Term<a class="headerlink" href="#zeroth-term" title="Permalink to this headline">¶</a></h4>
<p>For the first term, we take the expectation.</p>
<div class="math notranslate nohighlight">
\[
 \mathbb{E}_\mathbf{x_*}\left[ f(\mu_\mathbf{x_*})\right] =  f(\mu_\mathbf{x_*}) 
\]</div>
<p>This is the same because the expectation of the mean of a function <span class="math notranslate nohighlight">\(f\)</span> is simply the function <span class="math notranslate nohighlight">\(f\)</span> evaluated at the mean.</p>
</div>
<div class="section" id="st-order-term">
<h4>1st Order Term<a class="headerlink" href="#st-order-term" title="Permalink to this headline">¶</a></h4>
<p>The 1st order term is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
 \mathbb{E}_\mathbf{x_*}\left[ \nabla_\mathbf{x_*} f \bigg\vert_{\mathbf{x}_* = \mu_\mathbf{x}}
(\mathbf{x}_* - \mu_\mathbf{x_*})\right] 
&amp;=  \nabla_\mathbf{x_*} f(\mu_\mathbf{x_*}) \mathbb{E}_\mathbf{x_*}\left[ (\mathbf{x}_* - \mu_\mathbf{x_*}) \right] = 0
\end{aligned}\]</div>
<p><span class="math notranslate nohighlight">\(\mathbb{E}[(\mathbf{x}_* - \mu_\mathbf{x_*})]=0\)</span> because the terms cancel each other out.</p>
</div>
<div class="section" id="nd-order-term">
<h4>2nd Order Term<a class="headerlink" href="#nd-order-term" title="Permalink to this headline">¶</a></h4>
<p>The 2nd order term is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\mathbb{E}_\mathbf{x_*} \left[ \nabla^2_\mathbf{x_*} f \bigg\vert_{\mathbf{x}_* = \mu_\mathbf{x}}
(\mathbf{x}_* - \mu_\mathbf{x_*})^\top (\mathbf{x}_* - \mu_\mathbf{x_*}) \right] &amp;= \nabla_\mathbf{x_*}^2 f(\mu_\mathbf{x_*})  \mathbb{E}_\mathbf{x_*}\left[ (\mathbf{x}_* - \mu_\mathbf{x_*})^\top (\mathbf{x}_* - \mu_\mathbf{x_*}) \right]
\end{aligned}\]</div>
<p>We have the covariance term so we can simplify this:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_\mathbf{x_*} \left[ \nabla^2_\mathbf{x_*} f \bigg\vert_{\mathbf{x}_* = \mu_\mathbf{x}}
(\mathbf{x}_* - \mu_\mathbf{x_*})^\top (\mathbf{x}_* - \mu_\mathbf{x_*}) \right] = \nabla_\mathbf{x_*}^2 f(\mu_\mathbf{x_*}) \Sigma_\mathbf{x_*}
\]</div>
<p>So we’re left with:</p>
<div class="math notranslate nohighlight">
\[
 \mathbb{E}_\mathbf{x_*}\left[ f(\mathbf{x_*})\right]  \approx  f(\mu_\mathbf{x_*}) + \nabla^2_\mathbf{x_*} f(\mu_\mathbf{x})^\top\;\Sigma_{\mathbf{x_*}} + \mathcal{O} (\mathbf{x_*}^3) 
\]</div>
<p>which we can simplify to:</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x_*}) \approx f(\mu_\mathbf{x_*}) +
\underbrace{\frac{1}{2} \text{Tr}\left\{ \nabla^2_\mathbf{x_*} f(\mu_\mathbf{x_*})^\top\;\Sigma_\mathbf{x_*}\right\}}_\text{2nd Order}
\]</div>
</div>
<div class="section" id="linearized-gp-mean">
<h4>Linearized GP Mean<a class="headerlink" href="#linearized-gp-mean" title="Permalink to this headline">¶</a></h4>
<p>So now instead of a simple function <span class="math notranslate nohighlight">\(f\)</span>, we have our GP predictive mean equation <span class="math notranslate nohighlight">\(\mu_\text{GP}\)</span> which we can simply plug into the approximation.</p>
<div class="math notranslate nohighlight">
\[
\tilde{\mu}_\text{LinGP}(\mathbf{x_*}) = \mu_\text{GP}(\mu_\mathbf{x_*}) +
\underbrace{\frac{1}{2} \text{Tr}\left\{ \nabla^2_\mathbf{x_*} \mu_\text{GP}(\mu_\mathbf{x_*})^\top\;\Sigma_\mathbf{x_*}\right\}}_\text{2nd Order}
\]</div>
</div>
</div>
<div class="section" id="variance-function">
<h3>Variance Function<a class="headerlink" href="#variance-function" title="Permalink to this headline">¶</a></h3>
<p>So the variance term is a bit more difficult to calculate due to the <span class="math notranslate nohighlight">\(\mathbb{V}[\cdot]\)</span> operator.</p>
<div class="math notranslate nohighlight">
\[
\tilde{\sigma}^2_\text{LinGP}(\mathbf{x_*}) =\mathbb{E}_\mathbf{x_*} \left[  \sigma_\text{GP}^2(\mathbf{x}_*) \right] +
\mathbb{V}_\mathbf{x_*} \left[\mu_\text{GP}(\mathbf{x}_*) \right]
\]</div>
<p><strong>Term I</strong>: The formulation for the expectation of the Taylor expanded predictive variance function is similar to the equation above. So we can replace <span class="math notranslate nohighlight">\(\mu_\text{GP}\)</span> with <span class="math notranslate nohighlight">\(\sigma^2_\text{GP}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_\mathbf{x_*} \left[  \sigma_\text{GP}^2(\mathbf{x}_*) \right] = \sigma^2_\text{GP}(\mu_\mathbf{x_*}) +
\underbrace{\frac{1}{2} \text{Tr}\left\{ \nabla^2_\mathbf{x_*} \sigma^2_\text{GP}(\mu_\mathbf{x_*})^\top\;\Sigma_\mathbf{x_*}\right\}}_\text{2nd Order}
\]</div>
<p><strong>Term II</strong>: This term is more difficult to calculate. Again, we’ll take a step back and see how this is for a function <span class="math notranslate nohighlight">\(f\)</span>. If we want a first order approximation, we will have the following:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{V}_\mathbf{x_*}\left[f(\mathbf{x_*}) \right] = \nabla_\mathbf{x_*}f(\mu_\mathbf{x_*})^\top \Sigma_\mathbf{x_*}\nabla_\mathbf{x_*}f(\mu_\mathbf{x_*})
\]</div>
<p>This is a sufficient approximation when <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> is approximately linear and/or when <span class="math notranslate nohighlight">\(\Sigma_\mathbf{x_*}\)</span> is relatively small compared to <span class="math notranslate nohighlight">\(f(\mu_\mathbf{x_*})\)</span>. Alternatively, we can add a second order approximation which would add the following terms:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{V}_\mathbf{x_*}\left[f(\mathbf{x_*}) \right] &amp;\approx
\underbrace{\left(\nabla_\mathbf{x_*}f(\mu_\mathbf{x_*})\right)^2 \Sigma_\mathbf{x_*}}_\text{1st Order} \\
&amp;- \underbrace{\frac{1}{4}\left(\nabla^2_\mathbf{x_*}f(\mu_\mathbf{x_*})\right)^2\Sigma_\mathbf{x_*} + \mathbb{E}_\mathbf{x_*}\left[ \mathbf{x_*}-\mu_\mathbf{x_*} \right]\nabla^3_\mathbf{x_*}f(\mu_\mathbf{x_*}) + \frac{1}{4}\mathbb{E}_\mathbf{x_*}\left[\mathbf{x_*}-\mu_\mathbf{x_*} \right](\nabla^2_\mathbf{x_*}f(\mu_\mathbf{x_*}))^2}_\text{2nd Order}
\end{aligned}\end{split}\]</div>
<p>This expression has 3rd and 4th central moments with respect to the mean. These terms are often negligible according to the conditions mentioned above. In addition, it is a very expensive calculation for some functions. So a practical compromise is to use the 2nd order approximation for the mean and the first order approximation for the variance. This is the approach given here as 3rd and 4th central moments of kernel functions is very expensive. So combining the terms together, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\mathbb{V}_\mathbf{x_*}\left[f(\mathbf{x_*}) \right] &amp;\approx f(\mu_\mathbf{x_*}) + \frac{1}{2} \text{Tr}\left\{ \nabla^2_\mathbf{x_*} f(\mu_\mathbf{x_*})^\top\;\Sigma_\mathbf{x_*}\right\} +
\left(\nabla_\mathbf{x_*}f(\mu_\mathbf{x_*})\right)^2 \Sigma_\mathbf{x_*}
\end{aligned}\]</div>
<p>So then subsituting all of the portions for the appropriate GP function, we get the following:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
\tilde{\sigma}^2_\text{LinGP}(\mathbf{x_*}) = \sigma^2_\text{GP}(\mu_\mathbf{x_*})+
\left(\nabla_\mathbf{x_*}\mu_\text{GP}(\mu_\mathbf{x_*})\right)^2 \Sigma_\mathbf{x_*}+
\frac{1}{2} \text{Tr}\left\{ \nabla^2_\mathbf{x_*} \sigma^2_\text{GP}(\mu_\mathbf{x_*})^\top\;\Sigma_\mathbf{x_*}\right\} 
\end{aligned}\]</div>
</div>
<div class="section" id="linearized-predictive-mean-and-variance">
<h3>Linearized Predictive Mean and Variance<a class="headerlink" href="#linearized-predictive-mean-and-variance" title="Permalink to this headline">¶</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\tilde{\mu}_\text{LinGP}(\mathbf{x_*}) &amp;= \underbrace{\mu_\text{GP}(\mu_\mathbf{x_*})}_\text{1st Order} +
\underbrace{\frac{1}{2} \text{Tr}\left\{ \nabla^2_\mathbf{x_*} \mu_\text{GP}(\mu_\mathbf{x_*})^\top\;\Sigma_\mathbf{x_*}\right\}}_\text{2nd Order}\\
\tilde{\sigma}^2_\text{LinGP}(\mathbf{x_*}) &amp;= \underbrace{\sigma^2_\text{GP}(\mu_\mathbf{x_*})+
\nabla_\mathbf{x_*}\mu_\text{GP}(\mu_\mathbf{x_*})^\top \Sigma_\mathbf{x_*} \nabla_\mathbf{x_*}\mu_\text{GP}(\mu_\mathbf{x_*})}_\text{1st Order} \\
&amp;+
\underbrace{\frac{1}{2} \text{Tr}\left\{ \nabla^2_\mathbf{x_*} \sigma^2_\text{GP}(\mu_\mathbf{x_*})^\top\;\Sigma_\mathbf{x_*}\right\}}_\text{2nd Order}
\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\nabla_\mathbf{x_*}\)</span> is the gradient of the function w.r.t. <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\nabla_\mathbf{x_*}^2 \)</span> is the second derivative (the Hessian) of the function w.r.t. <span class="math notranslate nohighlight">\(\mathbf{x_*}\)</span>. This is a second-order approximation which has that expensive Hessian term. There have have been studies that have shown that that term tends to be neglible in practice and a first-order approximation is typically enough.</p>
<p>Practically speaking, this leaves us with the following predictive mean and variance functions:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mu_\text{GP}(\mathbf{x_*}) &amp;= k(\mathbf{x_*}) \, \mathbf{K}_{GP}^{-1}y=k(\mathbf{x_*}) \, \alpha  \\
\nu_{GP}^2(\mathbf{x_*}) &amp;= \sigma_y^2 + {\color{red}{\nabla_{\mu_\text{GP}}\,\Sigma_\mathbf{x_*} \,\nabla_{\mu_\text{GP}}^\top} }+ k_{**}- {\mathbf k}_* ({\mathbf K}+\sigma_y^2 \mathbf{I}_N )^{-1} {\mathbf k}_{*}^{\top}
\end{aligned}\end{split}\]</div>
<p>As seen above, the only extra term we need to include is the derivative of the mean function that is present in the predictive variance term.</p>
</div>
</div>
<div class="section" id="sparse-gps">
<h2>Sparse GPs<a class="headerlink" href="#sparse-gps" title="Permalink to this headline">¶</a></h2>
<p>We can extend this method to other GP algorithms including sparse GP models. The only thing that changes are the original <span class="math notranslate nohighlight">\(\mu_{GP}\)</span> and <span class="math notranslate nohighlight">\(\nu^2_{GP}\)</span> equations. In a sparse GP we have the following predictive functions</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \mu_{SGP} &amp;= K_{*z}K_{zz}^{-1}m \\
    \nu^2_{SGP} &amp;= K_{**} 
    - K_{*z}\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \right]K_{*z}^{\top}
\end{aligned}\end{split}\]</div>
<p>So the new predictive functions will be:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \mu_{SGP} &amp;= k_{*z}K_{zz}^{-1}m \\
    \nu^2_{SGP} &amp;= K_{**} 
    - K_{*z}\left[ K_{zz}^{-1} - K_{zz}^{-1}SK_{zz}^{-1} \right]K_{*z}^{\top} 
    + \tilde{\Sigma}_x
\end{aligned}\end{split}\]</div>
<p>As shown above, this is a fairly extensible method that offers a cheap improved predictive variance estimates on an already trained GP model. Some future work could be evaluating how other GP models, e.g. Sparse Spectrum GP, Multi-Output GPs, e.t.c.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/notes/egps"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="mcmc.html" title="previous page">Monte Carlo Sampling</a>
    <a class='right-next' id="next-link" href="moment_matching.html" title="next page">Moment Matching</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By J. Emmanuel Johnson<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>