{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(0)\n",
    "output_model_name = \"E2E_KF_SST_1Point_mdr95\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SST data at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/sanssauvegarde/homes/s17ouala/Complement/Koopman reduction/LearningKoopman/Identification/SST_Data_Points/SST_data_points_processed.pickle\",\n",
    "    \"rb\",\n",
    ") as handle:\n",
    "    dict_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesOfLocations = dict_data[\"namesOfLocations\"]\n",
    "LonOfLocations = dict_data[\"LonOfLocations\"]\n",
    "LatOfLocations = dict_data[\"LatOfLocations\"]\n",
    "idx_points = dict_data[\"idx_points\"]\n",
    "\n",
    "dim = 1\n",
    "\n",
    "Y_Train = dict_data[\"data_points_sat\"][::5][1 : int(10000 / 5) + 1, :dim]\n",
    "Y_Test = dict_data[\"data_points_sat\"][::5][int(10000 / 5) + 1 :, :dim]\n",
    "\n",
    "X_Train = np.copy(Y_Train)\n",
    "X_Test = np.copy(Y_Test)\n",
    "\n",
    "X_Train_Dyn = dict_data[\"data_points_sat_Fil\"][::5][: int(10000 / 5) + 1, :dim]\n",
    "X_Test_Dyn = dict_data[\"data_points_sat_Fil\"][::5][int(10000 / 5) + 1 :, :dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesOfLocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LonOfLocations[0], LatOfLocations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_rate = 0.95\n",
    "\n",
    "idx_to_nan_train = np.random.choice(\n",
    "    np.arange(X_Train.shape[0]),\n",
    "    size=int(missing_data_rate * X_Train.shape[0]),\n",
    "    replace=False,\n",
    ")\n",
    "idx_to_nan_test = np.random.choice(\n",
    "    np.arange(X_Test.shape[0]),\n",
    "    size=int(missing_data_rate * X_Test.shape[0]),\n",
    "    replace=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train[idx_to_nan_train, :] = np.nan\n",
    "X_Test[idx_to_nan_test, :] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(Y_Train, lw=3, alpha=0.1, label=\"True state\")\n",
    "plt.plot(X_Train, \"o\", markersize=16, label=\"Training observations\")\n",
    "plt.xlabel(\"Time step (adimensional)\")\n",
    "plt.ylabel(\"SST Â°C\")\n",
    "plt.legend(loc=9, bbox_to_anchor=(0.5, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_data_sst.png\")\n",
    "plt.savefig(\"training_data_sst.pdf\")\n",
    "plt.savefig(\"training_data_sst.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = X_Train_Dyn.shape[0]\n",
    "test_len = X_Test_Dyn.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Init KF model with a linear model of dim 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_aug = 6\n",
    "Batch_size = 500\n",
    "size_int_new = X_Train_Dyn.shape[0]\n",
    "dim = X_Train_Dyn.shape[-1]\n",
    "nb_batch = int(size_int_new / Batch_size)\n",
    "\n",
    "params = {}\n",
    "params[\"transition_layers\"] = 1\n",
    "params[\"bi_linear_layers\"] = dim_aug\n",
    "params[\"dim_hidden_linear\"] = dim_aug\n",
    "params[\"dim_input\"] = dim\n",
    "params[\"dim_output\"] = dim_aug\n",
    "params[\"dim_latent\"] = dim_aug - dim\n",
    "params[\"dim_observations\"] = dim_aug\n",
    "params[\"dim_hidden\"] = dim_aug\n",
    "params[\"Nb_parts\"] = 100\n",
    "params[\"dt_integration\"] = 1.0\n",
    "params[\"n_train\"] = 1000\n",
    "params[\"transition_layers_B\"] = 10\n",
    "params[\"dim_hidden_B\"] = 100\n",
    "params[\"Batch_size\"] = Batch_size\n",
    "dt = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2E_EM_KS(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(E2E_EM_KS, self).__init__()\n",
    "        # defining DA params\n",
    "        A = np.random.uniform(\n",
    "            size=(params[\"dim_observations\"], params[\"dim_observations\"])\n",
    "        )\n",
    "        self.A = torch.nn.Parameter(\n",
    "            data=torch.from_numpy(A).float(), requires_grad=True\n",
    "        )\n",
    "\n",
    "        B = 0.1 + 0 * np.random.uniform(\n",
    "            size=(params[\"dim_observations\"], params[\"dim_observations\"])\n",
    "        )\n",
    "        self.B = torch.nn.Parameter(torch.from_numpy(B).float(), requires_grad=False)\n",
    "\n",
    "        R = 0.1 + 0 * np.random.uniform(size=(dim, dim))\n",
    "        self.R = torch.nn.Parameter(torch.from_numpy(R).float(), requires_grad=False)\n",
    "\n",
    "        Q = 0.1 * np.eye((params[\"dim_observations\"]))\n",
    "        self.Q = torch.nn.Parameter(torch.from_numpy(Q).float(), requires_grad=True)\n",
    "\n",
    "        X0 = 0 * np.random.uniform(size=(params[\"dim_observations\"]))\n",
    "        self.X0 = torch.nn.Parameter(torch.from_numpy(X0).float(), requires_grad=False)\n",
    "\n",
    "        weight_R = 1 + 0 * np.random.uniform(size=(1))\n",
    "        self.weight_R = torch.nn.Parameter(\n",
    "            torch.from_numpy(weight_R).float(), requires_grad=True\n",
    "        )\n",
    "\n",
    "    def Koopman_Propagator(self, dt=1.0):\n",
    "        A = (self.A[:, :] - self.A[:, :].T) / 2\n",
    "        Phi = torch.matrix_exp(A * dt)\n",
    "        return Phi\n",
    "\n",
    "    def RMSE(self, E):\n",
    "        \"Returns the Root Mean Squared Error\"\n",
    "        return torch.sqrt(torch.mean(E**2))\n",
    "\n",
    "    def climat_background(self, X_true):\n",
    "        \"Returns a climatology (mean and covariance)\"\n",
    "        X_true = X_true.detach().numpy()\n",
    "        xb = np.mean(X_true, 1)\n",
    "        B = np.cov(X_true)\n",
    "        xb = torch.from_numpy(xb).float()\n",
    "        B = torch.from_numpy(B).float()\n",
    "        return xb, B\n",
    "\n",
    "    def gaspari_cohn(self, r):\n",
    "        corr = 0\n",
    "        if 0 <= r and r < 1:\n",
    "            corr = 1 - 5 / 3 * r**2 + 5 / 8 * r**3 + 1 / 2 * r**4 - 1 / 4 * r**5\n",
    "        elif 1 <= r and r < 2:\n",
    "            corr = (\n",
    "                4\n",
    "                - 5 * r\n",
    "                + 5 / 3 * r**2\n",
    "                + 5 / 8 * r**3\n",
    "                - 1 / 2 * r**4\n",
    "                + 1 / 12 * r**5\n",
    "                - 2 / (3 * r)\n",
    "            )\n",
    "        return corr\n",
    "\n",
    "    def cov_prob(self, Xs, Ps, X_true):\n",
    "        \"Returns the number of true state in the 95% confidence interval\"\n",
    "        X_true = X_true.detach().numpy()\n",
    "        Ps = Ps.detach().numpy()\n",
    "        Xs = Xs.detach().numpy()\n",
    "        n, T = np.shape(X_true)\n",
    "        cov_prob = 0\n",
    "        for i_n in range(n):\n",
    "            cov_prob += (\n",
    "                sum(\n",
    "                    (\n",
    "                        np.squeeze(Xs[i_n, :])\n",
    "                        - 1.96 * np.sqrt(np.squeeze(Ps[i_n, i_n, :]))\n",
    "                        <= X_true[i_n, :]\n",
    "                    )\n",
    "                    & (\n",
    "                        np.squeeze(Xs[i_n, :])\n",
    "                        + 1.96 * np.sqrt(np.squeeze(Ps[i_n, i_n, :]))\n",
    "                        >= X_true[i_n, :]\n",
    "                    )\n",
    "                )\n",
    "                / T\n",
    "            )\n",
    "        return cov_prob\n",
    "\n",
    "    def _likelihood0(self, Xf, Pf, Yo, H, get_innovation_statistics):\n",
    "        T = Xf.shape[1]\n",
    "        l = 0\n",
    "        # print(Yo.shape)\n",
    "        sig_all = np.zeros((T, Yo.shape[0], Yo.shape[0])) * np.nan\n",
    "        inno_all = np.zeros((T, Yo.shape[0])) * np.nan\n",
    "        for t in range(T):\n",
    "            i_var_obs = np.where(~np.isnan(Yo.detach().cpu().data.numpy()[:, t]))[0]\n",
    "            if len(i_var_obs) > 0:\n",
    "                sig = torch.mm(\n",
    "                    torch.mm(H[i_var_obs, :, t], Pf[:, :, t]), H[i_var_obs, :, t].T\n",
    "                ) + torch.abs(self.weight_R) * (self.R[np.ix_(i_var_obs, i_var_obs)])\n",
    "                innov = Yo[i_var_obs, t] - torch.mv(H[:, :][i_var_obs, :, t], Xf[:, t])\n",
    "                # l -= .5 * np.log(np.linalg.det(sig))\n",
    "                # print(torch.slogdet(sig))\n",
    "                # print(torch.det((sig).inverse()))\n",
    "                # print(innov.shape)\n",
    "                # sign, l_tmp = torch.slogdet(sig)# to print\n",
    "                tmppp = 0.5 * torch.logdet(sig)  # to print\n",
    "                # print('logdet',tmppp)\n",
    "                # print('positive value',  0.5*torch.mm(torch.mm(innov.unsqueeze(0),(sig).inverse()),innov.unsqueeze(0).T))\n",
    "                # print('sum log eig',0.5*sum(torch.log(torch.eig(sig)[0][:,0])))\n",
    "                # print('eigs : ',sig.shape, torch.eig(sig)[0][:,0])\n",
    "                # tmppp2 = .5 * sign * l_tmp\n",
    "                # tmppp = 0.5*sum(torch.log(torch.eig(sig,eigenvectors=True)[0][:,0]))#   .5 * sign * l_tmp\n",
    "                l += -tmppp - 0.5 * torch.mm(\n",
    "                    torch.mm(innov.unsqueeze(0), (sig).inverse()), innov.unsqueeze(0).T\n",
    "                )\n",
    "                # sign, l_tmp = torch.slogdet(sig)# to print\n",
    "                # l -= .5 * sign * l_tmp\n",
    "                # l -=  .5 * torch.mm(innov.unsqueeze(0),torch.solve(innov.unsqueeze(1),sig)[0])[0,0]# to print\n",
    "                # l -= tmpp\n",
    "                # print('innov :', innov.shape)\n",
    "                # print('inno_all : ',inno_all.shape)\n",
    "                # print('inno_all[i,i_var_obs] : ',inno_all[t,i_var_obs].shape)\n",
    "                if get_innovation_statistics:\n",
    "                    inno_all[t, i_var_obs] = innov.detach().numpy()\n",
    "                    sig_all[t, :, :][\n",
    "                        np.ix_(i_var_obs, i_var_obs)\n",
    "                    ] = sig.detach().numpy()\n",
    "        return l[0, 0], inno_all, sig_all\n",
    "\n",
    "    def _likelihood(self, Xf, Pf, Yo, H, get_innovation_statistics):\n",
    "        T = Xf.shape[1]\n",
    "        l = 0\n",
    "        sig_all = np.zeros((T, Yo.shape[0], Yo.shape[0])) * np.nan\n",
    "        inno_all = np.zeros((T, Yo.shape[0])) * np.nan\n",
    "        for t in range(T):\n",
    "            i_var_obs = np.where(~np.isnan(Yo.detach().cpu().data.numpy()[:, t]))[0]\n",
    "            if len(i_var_obs) > 0:\n",
    "                sig = torch.mm(\n",
    "                    torch.mm(H[i_var_obs, :, t], Pf[:, :, t]), H[i_var_obs, :, t].T\n",
    "                ) + torch.abs(self.weight_R) * (self.R[np.ix_(i_var_obs, i_var_obs)])\n",
    "                innov = Yo[i_var_obs, t] - torch.mv(H[i_var_obs, :, t], Xf[:, t])\n",
    "                # l -= .5 * np.log(np.linalg.det(sig))\n",
    "                # sign, l_tmp = torch.slogdet(sig)# to print\n",
    "                l -= 0.5 * torch.log(torch.det(sig))  # .5 * sign * l_tmp\n",
    "                # print('inov : ',innov.shape)\n",
    "                # print('sig : ',sig.shape)\n",
    "                tmpp = (\n",
    "                    0.5\n",
    "                    * torch.mm(\n",
    "                        innov.unsqueeze(0).T, torch.solve(innov.unsqueeze(0), sig)[0]\n",
    "                    )[0, 0]\n",
    "                )  # to print\n",
    "                l -= tmpp\n",
    "                inno_all[t, i_var_obs] = innov.detach().numpy()\n",
    "                sig_all[t, :, :][np.ix_(i_var_obs, i_var_obs)] = sig.detach().numpy()\n",
    "        return l, inno_all, sig_all\n",
    "\n",
    "    def _EKF(self, Nx, No, T, Yo, H, alpha):\n",
    "        Xa = torch.zeros((Nx, T + 1))\n",
    "        Xf = torch.zeros((Nx, T))\n",
    "        Pa = torch.zeros((Nx, Nx, T + 1))\n",
    "        Pf = torch.zeros((Nx, Nx, T))\n",
    "        F_all = torch.zeros((Nx, Nx, T))\n",
    "        H_all = torch.zeros((No, Nx, T))\n",
    "        Kf_all = torch.zeros((Nx, No, T))\n",
    "        K = torch.zeros((Nx, No))\n",
    "\n",
    "        x = self.X0\n",
    "        Xa[:, 0] = x\n",
    "        P = torch.abs(self.B)\n",
    "        Pa[:, :, 0] = P\n",
    "        for t in range(T):\n",
    "            # Forecast\n",
    "            # print(F.shape)\n",
    "            # print(x.shape)\n",
    "            F = self.Koopman_Propagator()\n",
    "            x = torch.mv(F, x)\n",
    "            # diag_Q = self.Q\n",
    "            # Q = Q*Q' with Q = reshape(n,n) of output of model_Q for full covariance support\n",
    "            # Q      = torch.diag(diag_Q)#self.Q_const#\n",
    "            P = torch.mm(torch.mm(F, P), F.T) + torch.mm(\n",
    "                self.Q, self.Q.T\n",
    "            )  # self.Q# + Q#\n",
    "            P = 0.5 * (P + P.T)\n",
    "            Pf[:, :, t] = P\n",
    "            Xf[:, t] = x\n",
    "            F_all[:, :, t] = F\n",
    "\n",
    "            # Update\n",
    "            i_var_obs = np.where(~np.isnan(Yo.detach().cpu().data.numpy()[:, t]))[0]\n",
    "            # print(t,i_var_obs,T)\n",
    "            if len(i_var_obs) > 0:\n",
    "                d = Yo[i_var_obs, t] - torch.mv(H[i_var_obs, :], x)\n",
    "                S = torch.mm(\n",
    "                    torch.mm(H[i_var_obs, :], P), H[i_var_obs, :].T\n",
    "                ) + torch.abs(self.weight_R) * (\n",
    "                    self.R[np.ix_(i_var_obs, i_var_obs)]\n",
    "                )  # torch.abs(self.R)[np.ix_(i_var_obs,i_var_obs)]/alpha\n",
    "                K = torch.mm(torch.mm(P, H[i_var_obs, :].T), (S.inverse()))\n",
    "                P = torch.mm((torch.eye(Nx) - torch.mm(K, H[i_var_obs, :])), P)\n",
    "                x = x + torch.mv(K, d)\n",
    "                Kf_all[:, i_var_obs, t] = K\n",
    "            Pa[:, :, t + 1] = P\n",
    "            Xa[:, t + 1] = x\n",
    "            H_all[:, :, t] = H\n",
    "\n",
    "        return Xa, Pa, Xf, Pf, F_all, H_all, Kf_all\n",
    "\n",
    "    def _EKS(self, Nx, No, T, Yo, H, alpha):\n",
    "        Xa, Pa, Xf, Pf, F, H, Kf = self._EKF(Nx, No, T, Yo, H, alpha)\n",
    "        Xs = torch.zeros((Nx, T + 1))\n",
    "        Ps = torch.zeros((Nx, Nx, T + 1))\n",
    "        K_all = torch.zeros((Nx, Nx, T))\n",
    "\n",
    "        x = Xa[:, -1]\n",
    "        Xs[:, -1] = x\n",
    "        P = Pa[:, :, -1]\n",
    "        Ps[:, :, -1] = P\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            K = torch.mm(torch.mm(Pa[:, :, t], F[:, :, t].T), (Pf[:, :, t].inverse()))\n",
    "            x = Xa[:, t] + torch.mv(K, x - Xf[:, t])\n",
    "            P = Pa[:, :, t] - torch.mm(torch.mm(K, Pf[:, :, t] - P), K.T)\n",
    "\n",
    "            Ps[:, :, t] = P\n",
    "            Xs[:, t] = x\n",
    "            K_all[:, :, t] = K\n",
    "\n",
    "        # pykalman\n",
    "        Ps_lag = torch.zeros((Nx, Nx, T))\n",
    "        # Ps_lag[:,:,-1] = ((np.eye(Nx)-Kf[:,:,-1].dot(H[:,:,-1])).dot(F[:,:,-1]).dot(Pa[:,:,-2]))\n",
    "        for t in range(1, T):\n",
    "            Ps_lag[:, :, t] = torch.mm(Ps[:, :, t], K_all[:, :, t - 1].T)\n",
    "        return Xs, Ps, Ps_lag, Xa, Pa, Xf, Pf, H\n",
    "\n",
    "    def EKS(self, params):\n",
    "        Yo = params[\"observations\"]\n",
    "        xb = params[\"background_state\"]\n",
    "        B = params[\"background_covariance\"]\n",
    "        Q = params[\"model_noise_covariance\"]\n",
    "        R = params[\"observation_noise_covariance\"]\n",
    "        F = params[\"model_dynamics\"]\n",
    "        # jacF = params['model_jacobian']\n",
    "        H = params[\"observation_operator\"]\n",
    "        # jacH = params['observation_jacobian']\n",
    "        Nx = params[\"state_size\"]\n",
    "        No = params[\"observation_size\"]\n",
    "        T = params[\"temporal_window_size\"]\n",
    "        Xt = params[\"true_state\"]\n",
    "        alpha = params[\"inflation_factor\"]\n",
    "\n",
    "        Xs, Ps, Ps_lag, Xa, Pa, Xf, Pf, H = _EKS(Nx, No, T, Yo, H, alpha)\n",
    "        l, inno_all, sig_all = _likelihood(Xf, Pf, Yo, R, H)\n",
    "        cov_p = cov_prob(Xs, Ps, Xt)\n",
    "\n",
    "        res = {\n",
    "            \"smoothed_states\": Xs,\n",
    "            \"smoothed_covariances\": Ps,\n",
    "            \"smoothed_lagged_covariances\": Ps_lag,\n",
    "            \"analysis_states\": Xa,\n",
    "            \"analysis_covariance\": Pa,\n",
    "            \"forecast_states\": Xf,\n",
    "            \"forecast_covariance\": Pf,\n",
    "            \"RMSE\": RMSE(Xs - Xt),\n",
    "            \"params\": params,\n",
    "            \"loglikelihood\": l,\n",
    "            \"cov_prob\": cov_p,\n",
    "        }\n",
    "        return res\n",
    "\n",
    "    def _ML_crit_numpy(self, Xs, Ps, Ps_lag, Yo, H):\n",
    "        Xs = Xs.detach().numpy()\n",
    "        Ps = Ps.detach().numpy()\n",
    "\n",
    "        Ps_lag = Ps_lag.detach().numpy()\n",
    "        Yo = Yo.detach().numpy()\n",
    "        H = H.detach().numpy()\n",
    "        F = self.Koopman_Propagator().detach().numpy()\n",
    "\n",
    "        No = Yo.shape[0]\n",
    "        T = Yo.shape[1]\n",
    "        Nx = Xs.shape[0]\n",
    "\n",
    "        No = Yo.shape[0]\n",
    "        T = Yo.shape[1]\n",
    "        Nx = Xs.shape[0]\n",
    "\n",
    "        xb = Xs[:, 0]\n",
    "        B = Ps[:, :, 0]\n",
    "        R = 0\n",
    "        nobs = 0\n",
    "        sumSig = 0\n",
    "\n",
    "        # Dreano et al. 2017, Eq. (34)\n",
    "        for t in range(T):\n",
    "            if not np.isnan(Yo[0, t]):\n",
    "                nobs += 1\n",
    "                R += np.outer(\n",
    "                    Yo[:, t] - np.dot(H, Xs[:, t + 1]),\n",
    "                    Yo[:, t] - np.dot(H, Xs[:, t + 1]),\n",
    "                )\n",
    "                R += H.dot(Ps[:, :, t + 1]).dot(H.T)\n",
    "        R = 0.5 * (R + R.T)\n",
    "        sigma1 = 0.5 * (R + R.T)\n",
    "        # R /= nobs\n",
    "\n",
    "        # for Shumway 1982\n",
    "        mat_A = 0\n",
    "        mat_B = 0\n",
    "        mat_C = 0\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            # Dreano et al. 2017, Eq. (33)\n",
    "            sumSig += Ps[:, :, t + 1]\n",
    "            sumSig += np.outer(\n",
    "                Xs[:, t + 1] - np.dot(F, Xs[:, t]), Xs[:, t + 1] - np.dot(F, Xs[:, t])\n",
    "            )  # CAUTION: error in Dreano equations\n",
    "            sumSig += F.dot(Ps[:, :, t]).dot(F.T)\n",
    "            sumSig -= Ps_lag[:, :, t].dot(F.T) + F.dot(\n",
    "                Ps_lag[:, :, t].T\n",
    "            )  # CAUTION: transpose at the end (error in Dreano equations)\n",
    "            sumSig = 0.5 * (sumSig + sumSig.T)\n",
    "\n",
    "        sigma2 = sumSig  # /T\n",
    "        sigma3 = Ps[:, :, 0]\n",
    "\n",
    "        R = self.R.detach().numpy()\n",
    "        Q = self.Q.detach().numpy()\n",
    "        B = self.B.detach().numpy()\n",
    "\n",
    "        ll = (\n",
    "            -0.5 * np.log(np.linalg.norm(np.abs(B)))\n",
    "            - 0.5 * np.trace((np.dot(np.linalg.inv(np.abs(B)), sigma3)))\n",
    "            - 0.5 * T * np.log(np.linalg.norm(np.abs(Q)))\n",
    "            - 0.5 * np.trace((np.dot(np.linalg.inv(np.abs(Q)), sigma2)))\n",
    "            - 0.5 * T * np.log(np.linalg.norm(np.abs(R)))\n",
    "            - 0.5 * np.trace((np.dot(np.linalg.inv(np.abs(R)), sigma1)))\n",
    "        )\n",
    "\n",
    "        return ll\n",
    "\n",
    "    def _Get_3_sigmas(self, Xs, Ps, Ps_lag, Yo, H):\n",
    "        No = Yo.shape[0]\n",
    "        T = Yo.shape[1]\n",
    "        Nx = Xs.shape[0]\n",
    "\n",
    "        xb = Xs[:, 0]\n",
    "        B = Ps[:, :, 0]\n",
    "        nobs = 0\n",
    "        tmp = 0\n",
    "        sumSig = 0\n",
    "        # Dreano et al. 2017, Eq. (34)\n",
    "        for t in range(T):\n",
    "            i_var_obs = np.where(~np.isnan(Yo.detach().cpu().data.numpy()[:, t]))[0]\n",
    "            if len(i_var_obs) > 0:\n",
    "                nobs += 1\n",
    "                # H = jacH(Xs[:,t+1])\n",
    "                tmp += torch.outer(\n",
    "                    Yo[i_var_obs, t] - torch.mv(H[i_var_obs, :], Xs[:, t + 1]),\n",
    "                    Yo[i_var_obs, t] - torch.mv(H[i_var_obs, :], Xs[:, t + 1]),\n",
    "                )\n",
    "                tmp += torch.mm(torch.mm(H, Ps[:, :, t + 1]), H.T)\n",
    "        sigma1 = 0.5 * (tmp + tmp.T)\n",
    "        # tmp /= nobs\n",
    "\n",
    "        # for Shumway 1982\n",
    "        mat_A = 0\n",
    "        mat_B = 0\n",
    "        mat_C = 0\n",
    "\n",
    "        for t in range(T):\n",
    "            # Dreano et al. 2017, Eq. (33)\n",
    "            # F = jacF(Xs[:,t+1])\n",
    "            sumSig += Ps[:, :, t + 1]\n",
    "\n",
    "            sumSig += torch.outer(\n",
    "                Xs[:, t + 1] - torch.mv(self.Koopman_Propagator(), Xs[:, t]),\n",
    "                Xs[:, t + 1] - torch.mv(self.Koopman_Propagator(), Xs[:, t]),\n",
    "            )  # CAUTION: error in Dreano equations\n",
    "            sumSig += torch.mm(\n",
    "                torch.mm(self.Koopman_Propagator(), Ps[:, :, t]),\n",
    "                self.Koopman_Propagator().T,\n",
    "            )\n",
    "            sumSig -= torch.mm(Ps_lag[:, :, t], self.Koopman_Propagator().T) + torch.mm(\n",
    "                self.Koopman_Propagator(), Ps_lag[:, :, t].T\n",
    "            )  # CAUTION: transpose at the end (error in Dreano equations)\n",
    "            sumSig = 0.5 * (sumSig + sumSig.T)\n",
    "\n",
    "        sigma2 = sumSig  # /T\n",
    "        sigma3 = Ps[:, :, 0]\n",
    "        self.B.data = Ps[:, :, 0]\n",
    "        self.X0.data = xb\n",
    "        ll = (\n",
    "            -0.5 * torch.log(torch.norm(torch.abs(self.B)))\n",
    "            - 0.5 * torch.trace((torch.mm(torch.abs(self.B).inverse(), sigma3)))\n",
    "            - 0.5 * T * torch.log(torch.norm(torch.abs(self.Q)))\n",
    "            - 0.5 * torch.trace((torch.mm(torch.abs(self.Q).inverse(), sigma2)))\n",
    "            - 0.5 * T * torch.log(torch.norm(torch.abs(self.R)))\n",
    "            - 0.5 * torch.trace((torch.mm(torch.abs(self.R).inverse(), sigma1)))\n",
    "        )\n",
    "        return ll\n",
    "        # return sigma1, sigma2, sigma3\n",
    "\n",
    "    def _ML_crit(self, sigma1, sigma2, sigma3, T):\n",
    "        ll = (\n",
    "            -0.5 * torch.log(torch.norm(torch.abs(self.B)))\n",
    "            - 0.5 * torch.trace((torch.mm(torch.abs(self.B).inverse(), sigma3)))\n",
    "            - 0.5 * T * torch.log(torch.norm(torch.abs(self.Q)))\n",
    "            - 0.5 * torch.trace((torch.mm(torch.abs(self.Q).inverse(), sigma2)))\n",
    "            - 0.5 * T * torch.log(torch.norm(torch.abs(self.R)))\n",
    "            - 0.5 * torch.trace((torch.mm(torch.abs(self.R).inverse(), sigma1)))\n",
    "        )\n",
    "        return ll\n",
    "\n",
    "    def forward(self, params, empty=False):\n",
    "        if empty:\n",
    "            return 0\n",
    "        else:\n",
    "            # jacF  = params['model_jacobian']\n",
    "            H = params[\"observation_operator\"]\n",
    "            # jacH  = params['observation_jacobian']\n",
    "            Yo = params[\"observations\"]\n",
    "            nIter = params[\"nb_EM_iterations\"]\n",
    "            Xt = params[\"true_state\"]\n",
    "            Nx = params[\"state_size\"]\n",
    "            No = params[\"observation_size\"]\n",
    "            T = params[\"temporal_window_size\"]\n",
    "            alpha = params[\"inflation_factor\"]\n",
    "            estimateQ = params[\"is_model_noise_covariance_estimated\"]\n",
    "            estimateR = params[\"is_observation_noise_covariance_estimated\"]\n",
    "            estimateX0 = params[\"is_background_estimated\"]\n",
    "            Xs, Ps, Ps_lag, Xa, Pa, Xf, Pf, H_all = self._EKS(Nx, No, T, Yo, H, alpha)\n",
    "            # ll = self._ML_crit_numpy(Xs, Ps, Ps_lag, Yo, H)\n",
    "\n",
    "            # LL_TORCH = self._Get_3_sigmas(Xs, Ps, Ps_lag, Yo, H)\n",
    "            # LL_TORCH = self._ML_crit(S1,S2,S3,Yo.shape[1])\n",
    "\n",
    "            loglik, inno_all, sig_all = self._likelihood(Xf, Pf, Yo, H_all, True)\n",
    "            rmse_em = self.RMSE(torch.mm(H, Xs[:, 1:]) - Xt)\n",
    "            cov_prob_em = self.cov_prob(Xs[:, 1:], Ps[:, :, 1:], Xt)\n",
    "\n",
    "            res = {\n",
    "                \"forecast\": Xf,\n",
    "                \"smoothed_states\": Xs,\n",
    "                \"background_state\": self.X0,\n",
    "                \"background_covariance\": torch.abs(self.B),\n",
    "                \"model_noise_covariance\": torch.mm(self.Q, self.Q.T),\n",
    "                \"observation_noise_covariance\": torch.abs(self.weight_R) * (self.R),\n",
    "                #'loglikelihood_crit'             : ll,\n",
    "                \"loglikelihood\": loglik,\n",
    "                \"RMSE\": rmse_em,\n",
    "                \"cov_prob\": cov_prob_em,\n",
    "                \"cov_smooth\": Ps,\n",
    "                \"cov_lag\": Ps_lag,\n",
    "                \"innoAll\": inno_all,\n",
    "                \"sigAll\": sig_all,\n",
    "                \"params\": params,\n",
    "            }\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_E2E_EM_KS = E2E_EM_KS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.NAdam(model_E2E_EM_KS.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group[\"lr\"] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb_all, B_all, R_all, Q_all, F_all = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSM parameters\n",
    "\n",
    "# EM parameters\n",
    "Nx = dim_aug\n",
    "No = dim\n",
    "# N_iter = 100\n",
    "Q_init = torch.eye(Nx)\n",
    "R_init = torch.eye(No)\n",
    "T = Batch_size\n",
    "# parameters\n",
    "params = {\n",
    "    \"initial_background_state\": torch.zeros((dim_aug)),\n",
    "    \"initial_background_covariance\": torch.eye(Nx),\n",
    "    \"initial_model_noise_covariance\": Q_init,\n",
    "    \"initial_observation_noise_covariance\": R_init,\n",
    "    \"state_size\": Nx,\n",
    "    \"observation_size\": No,\n",
    "    \"temporal_window_size\": T,\n",
    "    \"model_noise_covariance_structure\": \"full\",\n",
    "    \"is_model_noise_covariance_estimated\": True,\n",
    "    \"is_observation_noise_covariance_estimated\": True,\n",
    "    \"is_background_estimated\": True,\n",
    "    \"inflation_factor\": 1,\n",
    "}\n",
    "params[\"observation_operator\"] = torch.zeros((dim, dim_aug))\n",
    "params[\"observation_operator\"][:dim, :dim] = torch.eye(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_batched = torch.from_numpy(X_Train.reshape(nb_batch, Batch_size, dim)).float()\n",
    "Y_train_batched = torch.from_numpy(Y_Train.reshape(nb_batch, Batch_size, dim)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group[\"lr\"] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train_batched[3, :, 0], \"*\")\n",
    "plt.plot(\n",
    "    Y_train_batched[3, :, 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 3\n",
    "params[\"observations\"] = torch.from_numpy(X_Test.T).float()\n",
    "params[\"nb_EM_iterations\"] = 1\n",
    "params[\"temporal_window_size\"] = X_Test.shape[0]\n",
    "params[\"true_state\"] = torch.from_numpy(Y_Test.T).float()\n",
    "res_EM_EKS_init = model_E2E_EM_KS(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_E2E_EM_KS.Q.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_E2E_EM_KS.load_state_dict(torch.load(output_model_name + \".pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"ntrain\"] = [2000, 10000]\n",
    "for t in range(params[\"ntrain\"][0]):\n",
    "    for b in range(nb_batch):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        params[\"observations\"] = X_train_batched[b, :, :].T\n",
    "        params[\"nb_EM_iterations\"] = 1\n",
    "        params[\"temporal_window_size\"] = Batch_size\n",
    "        params[\"true_state\"] = Y_train_batched[b, :, :].T\n",
    "        res_EM_EKS = model_E2E_EM_KS(params)\n",
    "        loss = -res_EM_EKS[\"loglikelihood\"]\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        Xb_all.append(model_E2E_EM_KS.X0.detach().clone().numpy())\n",
    "        # model_E2E_EM_KS.R.data = model_E2E_EM_KS.R.data-0.000001*model_E2E_EM_KS.R.grad.data\n",
    "        # model_E2E_EM_KS.Q.data = model_E2E_EM_KS.Q.data-0.000001*model_E2E_EM_KS.Q.grad.data\n",
    "        B_all.append(torch.abs(model_E2E_EM_KS.B).detach().clone().numpy())\n",
    "        R_all.append(\n",
    "            (torch.abs(model_E2E_EM_KS.weight_R) * (model_E2E_EM_KS.R))\n",
    "            .detach()\n",
    "            .clone()\n",
    "            .numpy()\n",
    "        )\n",
    "        Q_all.append(torch.abs(model_E2E_EM_KS.Q).detach().clone().numpy())\n",
    "        F_all.append(\n",
    "            ((model_E2E_EM_KS.A - model_E2E_EM_KS.A.T) / 2).detach().clone().numpy()\n",
    "        )\n",
    "    print(t, b, loss)\n",
    "    if t > 400:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_E2E_EM_KS.state_dict(), output_model_name + \".pt\")\n",
    "Dyn_Mat = ((model_E2E_EM_KS.A - model_E2E_EM_KS.A.T) / 2).detach().clone()\n",
    "torch.linalg.eig(Dyn_Mat)[0].imag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 3\n",
    "params[\"observations\"] = X_train_batched[b, :, :].T\n",
    "params[\"nb_EM_iterations\"] = 1\n",
    "params[\"temporal_window_size\"] = Batch_size\n",
    "params[\"true_state\"] = Y_train_batched[b, :, :].T\n",
    "res_EM_EKS = model_E2E_EM_KS(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_EM_EKS[\"smoothed_states\"][:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res_EM_EKS[\"smoothed_states\"][0, 1:].detach())\n",
    "plt.plot(params[\"true_state\"][0, :])\n",
    "plt.plot(params[\"observations\"][0, :], \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = res_EM_EKS[\"smoothed_states\"].detach()\n",
    "# ll=res_EM_EKS['loglikelihood_crit']\n",
    "cov_prob = res_EM_EKS[\"cov_prob\"]\n",
    "Ps = res_EM_EKS[\"cov_smooth\"].detach()\n",
    "\n",
    "length_plot = params[\"temporal_window_size\"]  # -200\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(xs[0, :length_plot])\n",
    "plt.fill_between(\n",
    "    np.arange(length_plot),\n",
    "    xs[0, :length_plot] - torch.sqrt(Ps[0, 0, :length_plot]),\n",
    "    xs[0, :length_plot] + torch.sqrt(Ps[0, 0, :length_plot]),\n",
    "    label=\"Smoothed PDF\",\n",
    ")\n",
    "plt.plot(params[\"observations\"].T, \"o\", label=\"observations\")\n",
    "plt.plot(params[\"true_state\"].T, label=\"True state\")\n",
    "plt.title(\"cov prob : \" + str(cov_prob))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dyn_sys = (model_E2E_EM_KS.A - model_E2E_EM_KS.A.T) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.eig(Dyn_sys)[0].imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp_concat = torch.cat((X_Train_Dyn_batched[0,:,:], model_E2E_EM_KS.y_aug[0,:,:]), dim=1)\n",
    "y_pred2 = np.zeros((2000, dim_aug))\n",
    "tmp = res_EM_EKS[\"smoothed_states\"][:, 100:101].T\n",
    "tmp[:, :] = tmp[:, :]\n",
    "y_pred2[0, :] = tmp.cpu().data.numpy()\n",
    "for i in range(1, 2000):\n",
    "    F = model_E2E_EM_KS.Koopman_Propagator()  # [0,:,:]\n",
    "    y_pred2[i, :] = torch.mv(F, torch.from_numpy(y_pred2[i - 1, :]).float()).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred2[:200, 0])\n",
    "plt.plot(res_EM_EKS[\"smoothed_states\"][0, 100:].detach())\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 3\n",
    "params[\"observations\"] = torch.from_numpy(X_Test.T).float()\n",
    "params[\"nb_EM_iterations\"] = 1\n",
    "params[\"temporal_window_size\"] = X_Test.shape[0]\n",
    "params[\"true_state\"] = torch.from_numpy(Y_Test.T).float()\n",
    "res_EM_EKS = model_E2E_EM_KS(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = res_EM_EKS[\"smoothed_states\"].detach()\n",
    "# ll=res_EM_EKS['loglikelihood_crit']\n",
    "cov_prob = res_EM_EKS[\"cov_prob\"]\n",
    "Ps = res_EM_EKS[\"cov_smooth\"].detach()\n",
    "\n",
    "length_plot = params[\"temporal_window_size\"]  # -200\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(params[\"true_state\"].T, label=\"True state\", lw=3, alpha=0.3)\n",
    "plt.plot(params[\"observations\"].T, \"o\", label=\"observations\", markersize=16)\n",
    "plt.plot(xs[0, :length_plot], color=\"r\", label=\"PDF mean\")\n",
    "plt.fill_between(\n",
    "    np.arange(length_plot),\n",
    "    xs[0, :length_plot] - torch.sqrt(Ps[0, 0, :length_plot]),\n",
    "    xs[0, :length_plot] + torch.sqrt(Ps[0, 0, :length_plot]),\n",
    "    color=\"tab:green\",\n",
    "    lw=6,\n",
    "    label=\"PDF Standard deviation\",\n",
    ")\n",
    "# plt.title('cov prob : '+ str(cov_prob))\n",
    "plt.legend(loc=9, bbox_to_anchor=(0.7, 1.2))\n",
    "plt.xlabel(\"Time step (adimensional)\")\n",
    "plt.ylabel(\"SST Â°C\")\n",
    "plt.savefig(\"smoothing_sst_pdf.png\")\n",
    "plt.savefig(\"smoothing_sst_pdf.pdf\")\n",
    "plt.savefig(\"smoothing_sst_pdf.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "mu = 0\n",
    "sigma_comp = np.mean(\n",
    "    np.sqrt(\n",
    "        res_EM_EKS[\"sigAll\"][\n",
    "            np.where(~np.isnan(res_EM_EKS[\"innoAll\"]))[0],\n",
    "            np.where(~np.isnan(res_EM_EKS[\"innoAll\"]))[1],\n",
    "            np.where(~np.isnan(res_EM_EKS[\"innoAll\"]))[1],\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "x_comp = np.linspace(mu - 3 * sigma_comp, mu + 3 * sigma_comp, 100)\n",
    "\n",
    "sigma_samp = np.std(res_EM_EKS[\"innoAll\"][np.where(~np.isnan(res_EM_EKS[\"innoAll\"]))])\n",
    "x_samp = np.linspace(mu - 3 * sigma_samp, mu + 3 * sigma_samp, 100)\n",
    "\n",
    "\n",
    "plt.plot(x_comp, stats.norm.pdf(x_comp, mu, sigma_comp), label=\"formula Q/R\")\n",
    "plt.plot(x_samp, stats.norm.pdf(x_samp, mu, sigma_samp), label=\"sample STD\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 3\n",
    "params[\"observations\"] = torch.from_numpy(X_Test.T).float()\n",
    "params[\"observations\"][:, 200:] = np.nan\n",
    "params[\"nb_EM_iterations\"] = 1\n",
    "params[\"temporal_window_size\"] = X_Test.shape[0]\n",
    "params[\"true_state\"] = torch.from_numpy(Y_Test.T).float()\n",
    "res_EM_EKS = model_E2E_EM_KS(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = res_EM_EKS[\"smoothed_states\"].detach()\n",
    "# ll=res_EM_EKS['loglikelihood_crit']\n",
    "cov_prob = res_EM_EKS[\"cov_prob\"]\n",
    "Ps = res_EM_EKS[\"cov_smooth\"].detach()\n",
    "\n",
    "length_plot = params[\"temporal_window_size\"]  # -200\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(xs[0, :length_plot])\n",
    "plt.fill_between(\n",
    "    np.arange(length_plot),\n",
    "    xs[0, :length_plot] - torch.sqrt(Ps[0, 0, :length_plot]),\n",
    "    xs[0, :length_plot] + torch.sqrt(Ps[0, 0, :length_plot]),\n",
    "    label=\"Standard deviation\",\n",
    ")\n",
    "plt.plot(params[\"observations\"].T, \"o\", label=\"observations\")\n",
    "plt.plot(params[\"true_state\"].T, label=\"True state\")\n",
    "# plt.title('cov prob : '+ str(cov_prob))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "mu = 0\n",
    "sigma_comp = np.mean(\n",
    "    np.sqrt(\n",
    "        res_EM_EKS[\"sigAll\"][\n",
    "            np.where(~np.isnan(res_EM_EKS[\"innoAll\"]))[0],\n",
    "            np.where(~np.isnan(res_EM_EKS[\"innoAll\"]))[1],\n",
    "            np.where(~np.isnan(res_EM_EKS[\"innoAll\"]))[1],\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "x_comp = np.linspace(mu - 3 * sigma_comp, mu + 3 * sigma_comp, 100)\n",
    "\n",
    "sigma_samp = np.std(res_EM_EKS[\"innoAll\"][np.where(~np.isnan(res_EM_EKS[\"innoAll\"]))])\n",
    "x_samp = np.linspace(mu - 3 * sigma_samp, mu + 3 * sigma_samp, 100)\n",
    "\n",
    "\n",
    "sigma_comp_init = np.mean(\n",
    "    np.sqrt(\n",
    "        res_EM_EKS_init[\"sigAll\"][\n",
    "            np.where(~np.isnan(res_EM_EKS_init[\"innoAll\"]))[0],\n",
    "            np.where(~np.isnan(res_EM_EKS_init[\"innoAll\"]))[1],\n",
    "            np.where(~np.isnan(res_EM_EKS_init[\"innoAll\"]))[1],\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "x_comp_init = np.linspace(mu - 3 * sigma_comp_init, mu + 3 * sigma_comp_init, 100)\n",
    "\n",
    "\n",
    "res_EM_EKS_init\n",
    "\n",
    "plt.plot(\n",
    "    x_comp, stats.norm.pdf(x_comp_init, mu, sigma_comp_init), label=\"formula Q/R init\"\n",
    ")\n",
    "plt.plot(x_comp, stats.norm.pdf(x_comp, mu, sigma_comp), label=\"formula Q/R\")\n",
    "plt.plot(x_samp, stats.norm.pdf(x_samp, mu, sigma_samp), label=\"sample STD\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\": model_E2E_EM_KS.A},\n",
    "        {\"params\": model_E2E_EM_KS.weight_R, \"lr\": 1e-2},\n",
    "        {\"params\": model_E2E_EM_KS.Q, \"lr\": 1e-2},\n",
    "    ],\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t,b,loss)\n",
    "    if 400>t>300:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.01\n",
    "    if t>400:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-eo_torch_mgp]",
   "language": "python",
   "name": "conda-env-miniconda3-eo_torch_mgp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
