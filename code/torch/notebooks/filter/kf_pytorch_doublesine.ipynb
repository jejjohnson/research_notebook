{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24483926-c46b-4d19-995f-f0122816e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pyprojroot import here\n",
    "\n",
    "\n",
    "# spyder up to find the root\n",
    "root = here(project_files=[\".local\"])\n",
    "local = here(project_files=[\".local\"])\n",
    "\n",
    "# append to path\n",
    "sys.path.append(str(root))\n",
    "sys.path.append(str(local))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437d485-97e7-4825-8b09-bba69672c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "from einops import repeat, rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c1cbdb-7541-4559-8058-52a2a1937304",
   "metadata": {},
   "source": [
    "## Torch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb95086-79d0-4aa4-ac90-67a9f0e1e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 8\n",
    "obs_dim = 4\n",
    "n_batch = 1\n",
    "\n",
    "fn = lambda x: repeat(x, \"... -> B ...\", B=n_batch)\n",
    "\n",
    "# matrices\n",
    "F = torch.randn(state_dim, state_dim)\n",
    "R_noise = 0.5**2 * torch.ones(obs_dim)\n",
    "R_cov = torch.diag(R_noise)\n",
    "H = torch.randn(size=(obs_dim, state_dim))\n",
    "Q = 0.5**2 * torch.eye(obs_dim, obs_dim)\n",
    "\n",
    "\n",
    "# states\n",
    "x = torch.randn(size=(state_dim,))\n",
    "x_batch = fn(x)\n",
    "P = 0.01 * torch.eye(state_dim)\n",
    "P_batch = fn(P)\n",
    "\n",
    "# pred obs\n",
    "mu_pred = torch.randn(size=(obs_dim,))\n",
    "mu_pred_batch = fn(mu_pred)\n",
    "\n",
    "Sigma_pred = 10 * torch.eye(obs_dim)\n",
    "Sigma_pred_batch = fn(Sigma_pred)\n",
    "\n",
    "# observations\n",
    "mask = torch.randint(low=0, high=2, size=(obs_dim,))\n",
    "mask_batch = torch.randint(\n",
    "    low=0,\n",
    "    high=2,\n",
    "    size=(\n",
    "        n_batch,\n",
    "        obs_dim,\n",
    "    ),\n",
    ")\n",
    "obs = torch.randn(size=(obs_dim,))\n",
    "obs_batch = fn(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f80425-e0de-408c-973d-ffd7b3c482c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_noise.shape, R_cov.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a8e9f-7da5-4c2e-8c8f-a36154495546",
   "metadata": {},
   "source": [
    "## Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1219a47-f865-4657-ad1a-cbf443b15095",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_noise_masked = fn(R_noise)\n",
    "assert R_noise_masked.shape == (n_batch, obs_dim)\n",
    "\n",
    "identities = torch.eye(obs_dim, obs_dim).repeat(n_batch, 1, 1)\n",
    "assert identities.shape == (n_batch, obs_dim, obs_dim)\n",
    "\n",
    "H_masked = H.repeat(n_batch, 1, 1)\n",
    "assert H_masked.shape == (n_batch, obs_dim, state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3572f-c337-48fc-812c-2f52d5f5bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskv = mask.unsqueeze(0)\n",
    "# cov_mask = (maskv.to(torch.bool) + maskv.to(torch.bool).T)\n",
    "# cov_mask = (maskv + maskv.T)\n",
    "# maskv.shape, cov_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db505dc9-4b2e-4a4e-b4a7-8ed0b9951a61",
   "metadata": {},
   "source": [
    "### Masked Parameter Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45926a23-4e2f-4b0d-b455-3aa481b8c3d3",
   "metadata": {},
   "source": [
    "#### Mask Observation Operator\n",
    "\n",
    "$$\n",
    "\\mathbf{H}_t = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{H}_t^{\\text{obs}} \\\\\n",
    "\\mathbf{H}_t^{\\text{missing}}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{H}_t^{\\text{obs}} \\\\\n",
    "\\mathbf{0}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9a13b-3ea2-4e3f-a89a-d530a513fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkf.kf import mask_observation_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b0f34-0169-40ce-bf39-67f0bf3dde90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_masked = mask_observation_operator(H, mask)\n",
    "# assert H_masked.shape == H.shape\n",
    "\n",
    "# H_masked_batched = mask_observation_operator(fn(H), fn(mask))\n",
    "# assert H_masked_batched.shape == fn(H).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba3acf-450b-43df-acea-67685fcff057",
   "metadata": {},
   "source": [
    "#### Mask Noise Operator\n",
    "\n",
    "$$\n",
    "\\mathbf{R}_t = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf{R}_{11t}^{\\text{obs}} & \\mathbf{R}_{12t}^{\\text{cross}}\\\\\n",
    "\\mathbf{R}_{21t}^{\\text{cross}} & \\mathbf{R}_{22t}^{\\text{missing}}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{R}_{11t}^{\\text{obs}} & \\mathbf{0}\\\\\n",
    "\\mathbf{0} & \\mathbf{I}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0401b-fd3f-4229-9094-79af05f08072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_batched(x, P, obs, H, R, mask=None):\n",
    "\n",
    "    # create masks\n",
    "    if mask is not None:\n",
    "        H = mask_observation_operator(H, mask)\n",
    "        R = mask_observation_noise_diag(R, mask)\n",
    "\n",
    "    # emission update\n",
    "    pred_sigma = H @ P @ H.transpose(0, 1) + R\n",
    "\n",
    "    # UPDATES\n",
    "    K = stable_kalman_gain(H, P, pred_sigma)\n",
    "\n",
    "    v = obs - torch.einsum(\"so,bo->bs\", H, x)\n",
    "    x = x + torch.einsum(\"bso,bo->bs\", K, v)\n",
    "\n",
    "    identity = torch.eye(*P.shape[1:], device=P.device)\n",
    "\n",
    "    # joseph form for numerical stability\n",
    "    P = (identity - K @ H) @ P @ (identity - K @ H).transpose(\n",
    "        1, 2\n",
    "    ) + K @ R @ K.transpose(1, 2)\n",
    "\n",
    "    return x, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f59d61-78f8-43fd-b1f4-566aa23130c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch.shape, P_batch.shape, obs_batch.shape, H.shape, R_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270d63a-393b-4476-a300-b88f0f033c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_new, P_new = update_batched(x_batch, P_batch, obs_batch, H, R_noise, mask=mask, diag=True)\n",
    "\n",
    "# assert x_new.shape == x_batch.shape\n",
    "# assert P_new.shape == P_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c24050-825a-4b57-885d-3bb0551abcf3",
   "metadata": {},
   "source": [
    "### Masked Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f64dd6-3714-457e-a8a0-bb8c87a4fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "INV2PI = (2 * math.pi) ** -1\n",
    "\n",
    "\n",
    "def masked_multivariate_likelihood(x, mean, cov, mask=None):\n",
    "    \"\"\"Masked Likelihood for full covariance matrices\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor, sha\"\"\"\n",
    "\n",
    "    if mask is not None:\n",
    "        maskv = mask.unsqueeze(0)\n",
    "        # fill x values with zeros\n",
    "        x = x.masked_fill(mask == 0, 0)\n",
    "\n",
    "        # fill mean values with zeros\n",
    "        mean = mean.masked_fill(mask == 0, 0)\n",
    "\n",
    "        # ensure masked entries are independent\n",
    "        cov_masked = cov.masked_fill(maskv + maskv.T == 0, 0)\n",
    "\n",
    "        # ensure masked entries return log likelihood of 0\n",
    "        cov = cov_masked.masked_fill(torch.diag(mask) == 0, INV2PI)\n",
    "\n",
    "    return dist.MultivariateNormal(mean, cov).log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9273a38-7a35-4681-9632-2bc6400c2624",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 1\n",
    "obs_dim = 3\n",
    "torch.manual_seed(234)\n",
    "x = torch.randn(n_batches, obs_dim)\n",
    "mask = torch.randint(0, 2, size=(obs_dim,))\n",
    "mean = torch.randn(n_batches, obs_dim)\n",
    "cov = torch.randn(n_batches, obs_dim, obs_dim)\n",
    "cov = cov @ cov.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277067b-536d-42d2-ac22-e6814d4f03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102b4b3-b199-4a79-a73a-2c218189c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "-masked_multivariate_likelihood(x, mean, cov, mask=mask).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a1a59-6e36-462f-948f-42cc4ca9d20f",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fca8e3-c235-43c0-9e1c-7a6899324de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29f098-c757-4b27-a6d3-cb203bf1c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt = 100\n",
    "t = np.linspace(0, 10, Nt)\n",
    "y = np.stack([np.sin(t), np.cos(t)]).T + np.random.normal(0, 0.1, (Nt, 2))\n",
    "mask = torch.randint(low=0, high=1, size=(Nt, 2))\n",
    "plt.scatter(t, y[:, 0])\n",
    "\n",
    "plt.scatter(t, y[:, 1])\n",
    "plt.plot(t, np.sin(t))\n",
    "plt.plot(t, np.cos(t))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca44d30-bcc8-4a5f-ab30-983efb121107",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.Tensor(y).unsqueeze(0)\n",
    "y_train_batches = repeat(y_train, \"1 ... -> B ...\", B=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fb34e-6f99-431e-a42c-decb26e229f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f26806b-aed7-4940-86de-e9c3465b2971",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52f834-b7e7-4121-ac6d-6c522282fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkf.kf import transition_predict, emission_predict, update_step, predict_step\n",
    "from torchkf.kf import (\n",
    "    mask_observation_noise_diag,\n",
    "    masked_multivariate_likelihood,\n",
    "    mask_observation_operator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f57d4-860e-4e07-aa18-c51bc0eecb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteKalmanFilter(nn.Module):\n",
    "    def __init__(self, obs_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # State update: x_{k} = A @ x_{k-1} + q\n",
    "        self.F = nn.Parameter(torch.randn(latent_dim, latent_dim) / latent_dim)\n",
    "        self.pre_Q = nn.Parameter(torch.ones(latent_dim))\n",
    "\n",
    "        # Emission: y_{k} = H @ x_{k} + r\n",
    "        self.H = nn.Parameter(torch.randn(obs_dim, latent_dim) / latent_dim)\n",
    "        self.pre_R = nn.Parameter(torch.ones(obs_dim))\n",
    "\n",
    "        # Priors\n",
    "        self.x0 = torch.zeros(latent_dim)\n",
    "        self.P0 = torch.eye(latent_dim, latent_dim)\n",
    "\n",
    "    @property\n",
    "    def obs_noise(self):\n",
    "        \"\"\"\n",
    "        Calculate observation noise covariance\n",
    "        \"\"\"\n",
    "        # pre_R_norm = torch.sigmoid(self.pre_R) * (self.noise_upper - self.noise_lower) + self.noise_lower\n",
    "        return torch.eye(self.obs_dim, self.obs_dim) * self.pre_R**2\n",
    "\n",
    "    @property\n",
    "    def trans_noise(self):\n",
    "        \"\"\"\n",
    "        Calculate process noise covariance\n",
    "        \"\"\"\n",
    "        return torch.eye(self.latent_dim, self.latent_dim) * self.pre_Q**2\n",
    "\n",
    "    def forward(self, z, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the Kalman Filter\n",
    "\n",
    "        Keyword arguments:\n",
    "        z -- observed values (torch.Tensor)\n",
    "        Returns:\n",
    "        loss - NLL of observed sequence in predicted probability dist (torch.Tensor)\n",
    "        \"\"\"\n",
    "        assert isinstance(z, torch.Tensor)\n",
    "        n_batches, n_time = z.shape[:2]\n",
    "\n",
    "        pred_means, pred_sigmas, x, P, log_probs = self.filter_forward(z, mask=mask)\n",
    "        # print(z.shape, pred_means.shape)\n",
    "        # print(pred_means.shape, pred_sigmas.shape, x.shape, P.shape, z.shape)\n",
    "        # evaluate observed sequence in predicted distribution\n",
    "        # dist = torch.distributions.MultivariateNormal(pred_means, pred_sigmas)\n",
    "        log_probs = log_probs.sum(dim=1) / n_time\n",
    "        loss = -log_probs.mean()\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x, P):\n",
    "        \"\"\"\n",
    "        Update state mean and covariance p(x_{k} | x_{k-1}) and calculate mean and\n",
    "        covariance in the observation space in the case of discrete time steps\n",
    "        \"\"\"\n",
    "        n_batch = x.shape[0]\n",
    "\n",
    "        x, P, y_pred, y_sigma = predict_step(\n",
    "            x, P, self.F, self.trans_noise, self.H, self.Q\n",
    "        )\n",
    "\n",
    "        assert x.shape == (n_batch, self.latent_dim)\n",
    "        assert P.shape == (n_batch, self.latent_dim, self.latent_dim)\n",
    "        return x, P, pred_mean, pred_sigma\n",
    "\n",
    "    #     def emission(self, x, P):\n",
    "    #         \"\"\"\n",
    "    #         emission from state space m & P to observed space mean & sigma\n",
    "    #         \"\"\"\n",
    "    #         # create masks\n",
    "    #         H = self.H\n",
    "    #         R = self.trans_noise\n",
    "\n",
    "    #         # print(H.shape, R.shape)\n",
    "    #         pred_mean = torch.einsum(\"ij,kj->ki\", H, x)\n",
    "\n",
    "    #         pred_sigma = torch.einsum(\"ij,kjl,ml->kim\", H, P, H) + R\n",
    "    #         assert pred_sigma.ndim == 3\n",
    "    #         assert pred_sigma.shape[0] == P.shape[0]\n",
    "    #         assert pred_sigma.shape[1] == self.obs_dim\n",
    "    #         assert pred_mean.shape[0] == x.shape[0]\n",
    "    #         assert pred_mean.shape[1] == self.obs_dim\n",
    "    #         return pred_mean, pred_sigma\n",
    "\n",
    "    def update(self, x, P, z, pred_sigma, mask=None):\n",
    "        \"\"\"\n",
    "        Update state x and P after the observation,\n",
    "        outputs filtered state and covariance\n",
    "        \"\"\"\n",
    "        assert x.ndim == 2\n",
    "        assert P.ndim == 3\n",
    "        assert z.ndim == 2\n",
    "\n",
    "        n_batch = x.shape[0]\n",
    "\n",
    "        H = self.H\n",
    "        R = self.trans_noise\n",
    "\n",
    "        # create masks\n",
    "        if mask is not None:\n",
    "            H = mask_observation_operator(H, mask)\n",
    "            R = mask_observation_noise_diag(R, mask)\n",
    "\n",
    "        # print(H.shape, R.shape)\n",
    "\n",
    "        # Update state mean and covariance p(x | y), Joseph Form\n",
    "        # Kalman gain, a more stable implementation than naive P @ H^T @ y_sigma^{-1}\n",
    "        L = torch.linalg.cholesky(\n",
    "            pred_sigma\n",
    "            + 1e-6 * torch.eye(pred_sigma.shape[-1], device=pred_sigma.device)\n",
    "        )\n",
    "        K = torch.triangular_solve(H @ P.transpose(1, 2), L, upper=False)[0]\n",
    "        K = torch.triangular_solve(K, L.transpose(1, 2))[0].transpose(1, 2)\n",
    "\n",
    "        # v = z - self.H @ x\n",
    "        v = z - torch.einsum(\"ij,kj->ki\", H, x)\n",
    "        # x = x + K @ v\n",
    "        x = x + torch.einsum(\"bso,bo->bs\", K, v)\n",
    "        # P = (torch.eye(*P.shape[1:]) - K @ self.H) @ P @ (torch.eye(*P.shape[1:]) - K @ self.H).T + K @ self.trans_noise @ K.T\n",
    "        identity = torch.eye(*P.shape[1:], device=P.device)\n",
    "\n",
    "        # joseph form for numerical stability\n",
    "\n",
    "        t1 = identity - K @ H\n",
    "        P = t1 @ P @ t1.transpose(1, 2) + K @ self.trans_noise @ K.transpose(1, 2)\n",
    "\n",
    "        assert x.shape == (n_batch, self.latent_dim)\n",
    "        assert P.shape == (n_batch, self.latent_dim, self.latent_dim)\n",
    "        return x, P\n",
    "\n",
    "    #         n_batch, n_time, _ = obs.shape\n",
    "    #         # Initialization\n",
    "    #         x, P = self.x0, self.P0\n",
    "    #         x = repeat(x, \"... -> B ...\", B=n_batch)\n",
    "    #         P = repeat(P, \"... -> B ...\", B=n_batch)\n",
    "    #         pred_means, pred_sigmas, log_probs = [], [], []\n",
    "    #         xs, Ps = [], []\n",
    "\n",
    "    #         # do prior transition\n",
    "    #         # x, P = transition_predict(self.x0, self.P, self.F, self.Q)\n",
    "\n",
    "    #         # Iterate through sequence performing predict-update steps\n",
    "    #         for i in range(n_time):\n",
    "    #             # print(x.shape, P.shape)\n",
    "    #             x, P, pred_means, pred_sigmas =\n",
    "    #             x_prio, P_prio, pred_mean, pred_sigma = self.predict(x, P)\n",
    "    #             # print(x_prio.shape, P_prio.shape, pred_mean.shape, pred_sigma.shape)\n",
    "    #             x, P = self.update(x_prio, P_prio, obs[:, i, :], pred_sigma)\n",
    "\n",
    "    #             # save predictive observations\n",
    "    #             pred_means.append(pred_mean)\n",
    "    #             pred_sigmas.append(pred_sigma)\n",
    "    #             xs.append(x)\n",
    "    #             Ps.append(P)\n",
    "\n",
    "    #             # calculate log prob\n",
    "    #             dist = torch.distributions.MultivariateNormal(pred_mean, pred_sigma)\n",
    "\n",
    "    #             p = masked_multivariate_likelihood(\n",
    "    #                 obs[:, i, :],\n",
    "    #                 pred_mean,\n",
    "    #                 pred_sigma,\n",
    "    #                 mask[i, :]  if mask is not None else None\n",
    "    #             )\n",
    "    #             # if mask is not None:\n",
    "    #             #     print(pred_mean.shape, pred_sigma.shape, obs[:, i, :].shape)\n",
    "    #             #     p = dist.log_prob(obs[:, i, :] * (1 - mask[:, i, :]))\n",
    "    #             #     print(p.shape)\n",
    "    #             # else:\n",
    "    #             #     p = dist.log_prob(obs[:, i, :])\n",
    "    #             # # print(obs[:, i, :].shape, p.shape, .shape)\n",
    "    #             log_probs.append(p)\n",
    "\n",
    "    def filter_forward(self, obs, mask=None):\n",
    "        \"\"\"\n",
    "        Iterate input data in case of discrete time steps\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : torch.Tensor, shape=(Batch,Time,ObsDim)\n",
    "            the observations\n",
    "        mask : torch.Tensor, shape=(Batch, Time, ObsDim)\n",
    "            the mask for the observations\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred_means : torch.Tensor, shape=(Batch,Time,ObsDim)\n",
    "            the filtered observation means\n",
    "        pred_sigmas : torch.Tensor, shape=(Batch,Time,Dimension,ObsDim)\n",
    "            the filtered observation covariances\n",
    "        xs : torch.Tensor, shape=(Batch,Time,StateDim)\n",
    "            the filtered state dimensions\n",
    "        Ps : torch.Tensor, shape=(Batch,Time,StateDim)\n",
    "            the filtered state covariances\n",
    "        log_probs : torch.Tensor, shape=(Batch,Time,ObsDim)\n",
    "            the log probabilities of the innovations\n",
    "        \"\"\"\n",
    "        n_batch, n_time, _ = obs.shape\n",
    "\n",
    "        pred_means, pred_sigmas, log_probs = [], [], []\n",
    "        xs, Ps = [], []\n",
    "\n",
    "        # do prior\n",
    "        x, P = self.x0, self.P0\n",
    "        x = repeat(x, \"... -> B ...\", B=n_batch)\n",
    "        P = repeat(P, \"... -> B ...\", B=n_batch)\n",
    "        # print(x.shape, P.shape)\n",
    "\n",
    "        for i in range(n_time):\n",
    "\n",
    "            # predict step\n",
    "            x, P, pred_mean, pred_sigma = predict_step(\n",
    "                x, P, self.F, self.trans_noise, self.H, self.obs_noise\n",
    "            )\n",
    "\n",
    "            # log likelihood on innovations\n",
    "            lprob = masked_multivariate_likelihood(\n",
    "                obs[:, i, :],\n",
    "                pred_mean,\n",
    "                pred_sigma,\n",
    "                mask[i, :] if mask is not None else None,\n",
    "            )\n",
    "\n",
    "            # update step\n",
    "            x, P = update_step(\n",
    "                obs[:, i, :],\n",
    "                x,\n",
    "                P,\n",
    "                self.H,\n",
    "                self.obs_noise,\n",
    "                pred_sigma,\n",
    "                mask[i, :] if mask is not None else None,\n",
    "            )\n",
    "\n",
    "            # save predictive observations\n",
    "            pred_means.append(pred_mean)\n",
    "            pred_sigmas.append(pred_sigma)\n",
    "            xs.append(x)\n",
    "            Ps.append(P)\n",
    "            log_probs.append(lprob)\n",
    "\n",
    "        # collapse all variables together (along time dimension)\n",
    "        pred_means = torch.stack(pred_means, dim=1)\n",
    "        pred_sigmas = torch.stack(pred_sigmas, dim=1)\n",
    "        log_probs = torch.stack(log_probs, dim=1)\n",
    "        xs = torch.stack(xs, dim=1)\n",
    "        Ps = torch.stack(Ps, dim=1)\n",
    "\n",
    "        return pred_means, pred_sigmas, xs, Ps, log_probs\n",
    "\n",
    "    def forecasting(self, T, x, P):\n",
    "        \"\"\"\n",
    "        forecast means and sigmas over given time period\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int,\n",
    "            the time steps after final states to forecast\n",
    "        x : torch.Tensor, shape=(Batch,Time,ObsDim)\n",
    "            the final state mean before forecasting window\n",
    "        P : torch.Tensor, shape=(Batch,Time,ObsDim,ObsDim)\n",
    "            the final state cov before the forecasting window\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pred_means : torch.Tensor,  shape=(Batch,T,ObsDim)\n",
    "            the predicted mean observations\n",
    "        pred_sigmas : torch.Tensor, shape=(Batch,T,ObsDim)\n",
    "            the predicted cov observations\n",
    "        \"\"\"\n",
    "        pred_means = torch.Tensor([])\n",
    "        pred_sigmas = torch.Tensor([])\n",
    "        assert isinstance(T, int)\n",
    "        assert T > 0\n",
    "        pred_means, pred_sigmas, log_probs = [], [], []\n",
    "\n",
    "        for i in range(T):\n",
    "            x, P, pred_mean, pred_sigma = predict_step(\n",
    "                x, P, self.F, self.trans_noise, self.H, self.obs_noise\n",
    "            )\n",
    "            pred_means.append(pred_mean)\n",
    "            pred_sigmas.append(pred_sigma)\n",
    "\n",
    "        pred_means = torch.stack(pred_means, dim=1)\n",
    "        pred_sigmas = torch.stack(pred_sigmas, dim=1)\n",
    "        return pred_means, pred_sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcc03c-cfeb-46d9-9b55-f7730ecc866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn((10, 5))\n",
    "# P = torch.randn((10, 5,5))\n",
    "# H = torch.randn((2, 5))\n",
    "# # y = torch.matmul(H.unsqueeze(0), x.unsqueeze(1))\n",
    "# Px = H.matmul(P).matmul(H.t())\n",
    "# Px_ = torch.einsum(\"ij,kjl,ml->kim\", H, P, H)\n",
    "# torch.testing.assert_equal(Px, Px_)\n",
    "# Px.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb20f5-0fc5-4ea2-8273-cf36dd5679a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mask_batches = repeat(y_mask, \"... -> B ...\", B=y_train_batches.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e819cf-402b-4019-98a9-2c7528c6f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiscreteKalmanFilter(obs_dim=2, latent_dim=20)\n",
    "\n",
    "# outs = model.filter_forward(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6296a-63c5-4ac8-89c2-4101759595d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trans_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017af5d-f62f-4e72-9268-56a845d272ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model.filter_forward(y_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd550986-fee9-447a-862c-8c7059d0611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518f5e3-0994-4531-8edf-8c39654faead",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.forward(y_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7e436-e2de-4a33-8913-4f9a4519ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c83d2-102f-4391-a09b-b2339bdb2d2e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91709e-0aaf-4bdf-a5c1-884e4501febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiscreteKalmanFilter(obs_dim=2, latent_dim=20)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b556caa-deac-43bb-bfd0-a6ec3fe36c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e22ebf-3e82-4429-ab95-99793c7b3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1_000\n",
    "\n",
    "losses = []\n",
    "\n",
    "with trange(n_iterations) as pbar:\n",
    "\n",
    "    for i in pbar:\n",
    "        optim.zero_grad()\n",
    "        loss = model(y_train)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pbar.set_description(f\"Iter {i}, loss: {loss:.4f}\")\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49852229-beb5-44f9-888c-ea1d12804eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc1bb2b-f1b8-4f06-b1b4-738b641a5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    pred_mu, pred_sigma, x, P, _ = model.filter_forward(y_train)\n",
    "    pred_mu = pred_mu.numpy()\n",
    "    pred_sigma = pred_sigma.numpy()\n",
    "\n",
    "\n",
    "i_batch = 0\n",
    "\n",
    "plt.scatter(t, y_train[0, :, 0], c=\"C0\")\n",
    "plt.scatter(t, y_train[0, :, 1], c=\"C1\")\n",
    "\n",
    "plt.plot(np.linspace(0, 10, Nt), pred_mu[i_batch, :, 0], c=\"C0\")\n",
    "plt.plot(np.linspace(0, 10, Nt), pred_mu[i_batch, :, 1], c=\"C1\")\n",
    "\n",
    "plt.fill_between(\n",
    "    np.linspace(0, 10, Nt),\n",
    "    pred_mu[i_batch, :, 0] - 1.96 * pred_sigma[i_batch, :, 0, 0],\n",
    "    pred_mu[i_batch, :, 0] + 1.96 * pred_sigma[i_batch, :, 0, 0],\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.fill_between(\n",
    "    np.linspace(0, 10, Nt),\n",
    "    pred_mu[i_batch, :, 1] - 1.96 * pred_sigma[i_batch, :, 1, 1],\n",
    "    pred_mu[i_batch, :, 1] + 1.96 * pred_sigma[i_batch, :, 1, 1],\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# plt.ylim(-1.1, 1.1)\n",
    "\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1575797b-545e-48eb-9099-8d593b2fe4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    pred_mu, pred_sigma, x, P, _ = model.filter_forward(y_train)\n",
    "\n",
    "    # forecasting\n",
    "    Nt_fore = 100\n",
    "    pred_mu_fore, pred_sigma_fore = model.forecasting(Nt_fore, x[:, -1, :], P[:, -1, :])\n",
    "\n",
    "    pred_mu = torch.cat([pred_mu, pred_mu_fore], dim=1)\n",
    "    pred_sigma = torch.cat([pred_sigma, pred_sigma_fore], dim=1)\n",
    "\n",
    "    pred_mu = pred_mu.numpy()\n",
    "    pred_sigma = pred_sigma.numpy()\n",
    "\n",
    "\n",
    "i_batch = 0\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(t, y_masked[:, 0], c=\"C0\")\n",
    "plt.scatter(t, y_masked[:, 1], c=\"C1\")\n",
    "\n",
    "plt.plot(np.linspace(0, 20, Nt + Nt_fore), pred_mu[i_batch, :, 0], c=\"C0\")\n",
    "plt.plot(np.linspace(0, 20, Nt + Nt_fore), pred_mu[i_batch, :, 1], c=\"C1\")\n",
    "\n",
    "plt.fill_between(\n",
    "    np.linspace(0, 20, Nt + Nt_fore),\n",
    "    pred_mu[i_batch, :, 0] - 1.96 * pred_sigma[i_batch, :, 0, 0],\n",
    "    pred_mu[i_batch, :, 0] + 1.96 * pred_sigma[i_batch, :, 0, 0],\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.fill_between(\n",
    "    np.linspace(0, 20, Nt + Nt_fore),\n",
    "    pred_mu[i_batch, :, 1] - 1.96 * pred_sigma[i_batch, :, 1, 1],\n",
    "    pred_mu[i_batch, :, 1] + 1.96 * pred_sigma[i_batch, :, 1, 1],\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# plt.ylim(-1.1, 1.1)\n",
    "\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f9fc2-0bd1-4871-a5ab-4f8233f0677e",
   "metadata": {},
   "source": [
    "## Masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf1e42-c7cd-4d29-90ba-0d1d868601ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ac41b-4234-4763-9262-818b3cf3fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, 10, Nt)\n",
    "y = np.stack([np.sin(t), np.cos(t)]).T + np.random.normal(0, 0.1, (Nt, 2))\n",
    "idx = np.random.choice(np.arange(0, t.shape[0]), size=(Nt - 20))\n",
    "y_masked = y.copy()\n",
    "y_masked[idx, 0] = np.nan\n",
    "y_masked[:, 1] = np.nan\n",
    "mask = np.isnan(y_masked).astype(np.float32)\n",
    "\n",
    "plt.show()\n",
    "plt.scatter(t, y_masked[:, 0])\n",
    "plt.scatter(t, y_masked[:, 1])\n",
    "plt.plot(t, np.sin(t))\n",
    "plt.plot(t, np.cos(t))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd11a4e-46d0-4b95-8299-7c69cbfc8381",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.Tensor(y_masked).unsqueeze(0)\n",
    "y_train = torch.nan_to_num(y_train)\n",
    "y_train_batches = repeat(y_train, \"1 ... -> B ...\", B=10)\n",
    "y_mask = torch.Tensor(mask)\n",
    "y_train.shape, y_mask.shape, y_train_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf6c99-5f0e-496b-99f1-ece663dcfffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871997b0-fe7b-4009-b6a6-ec16026b7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67291273-d999-4385-abe4-0460b8583ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiscreteKalmanFilter(obs_dim=2, latent_dim=8)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336a058-f01f-41dd-8c33-a305fd19bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1_000\n",
    "\n",
    "losses = []\n",
    "\n",
    "with trange(n_iterations) as pbar:\n",
    "\n",
    "    for i in pbar:\n",
    "        optim.zero_grad()\n",
    "        loss = model(y_train, y_mask)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pbar.set_description(f\"Iter {i}, loss: {loss:.4f}\")\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731fb04-1965-49ef-87a2-1a8dbd8bb8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee37a1-7bc9-4f44-a455-96d1eae62c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    pred_mu, pred_sigma, x, P, _ = model.filter_forward(y_train, y_mask)\n",
    "    pred_mu = pred_mu.numpy()\n",
    "    pred_sigma = pred_sigma.numpy()\n",
    "\n",
    "\n",
    "i_batch = 0\n",
    "\n",
    "plt.scatter(t, y_masked[:, 0], c=\"C0\")\n",
    "plt.scatter(t, y_masked[:, 1], c=\"C1\")\n",
    "\n",
    "plt.plot(np.linspace(0, 10, Nt), pred_mu[i_batch, :, 0], c=\"C0\")\n",
    "plt.plot(np.linspace(0, 10, Nt), pred_mu[i_batch, :, 1], c=\"C1\")\n",
    "\n",
    "plt.fill_between(\n",
    "    np.linspace(0, 10, Nt),\n",
    "    pred_mu[i_batch, :, 0] - 1.96 * pred_sigma[i_batch, :, 0, 0],\n",
    "    pred_mu[i_batch, :, 0] + 1.96 * pred_sigma[i_batch, :, 0, 0],\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.fill_between(\n",
    "    np.linspace(0, 10, Nt),\n",
    "    pred_mu[i_batch, :, 1] - 1.96 * pred_sigma[i_batch, :, 1, 1],\n",
    "    pred_mu[i_batch, :, 1] + 1.96 * pred_sigma[i_batch, :, 1, 1],\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# plt.ylim(-1.1, 1.1)\n",
    "\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07507dc-4b5b-4817-b640-a44133bc3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    pred_mu, pred_sigma, x, P, _ = model.filter_forward(y_train, y_mask)\n",
    "\n",
    "    # forecasting\n",
    "    Nt_fore = 100\n",
    "    pred_mu_fore, pred_sigma_fore = model.forecasting(Nt_fore, x[:, -1, :], P[:, -1, :])\n",
    "\n",
    "    pred_mu = torch.cat([pred_mu, pred_mu_fore], dim=1)\n",
    "    pred_sigma = torch.cat([pred_sigma, pred_sigma_fore], dim=1)\n",
    "\n",
    "    pred_mu = pred_mu.numpy()\n",
    "    pred_sigma = pred_sigma.numpy()\n",
    "\n",
    "\n",
    "i_batch = 0\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(t, y_masked[:, 0], c=\"C0\")\n",
    "plt.scatter(t, y_masked[:, 1], c=\"C1\")\n",
    "\n",
    "plt.plot(np.linspace(0, 20, Nt + Nt_fore), pred_mu[i_batch, :, 0], c=\"C0\")\n",
    "plt.plot(np.linspace(0, 20, Nt + Nt_fore), pred_mu[i_batch, :, 1], c=\"C1\")\n",
    "\n",
    "plt.fill_between(\n",
    "    np.linspace(0, 20, Nt + Nt_fore),\n",
    "    pred_mu[i_batch, :, 0] - 1.96 * pred_sigma[i_batch, :, 0, 0],\n",
    "    pred_mu[i_batch, :, 0] + 1.96 * pred_sigma[i_batch, :, 0, 0],\n",
    "    alpha=0.2,\n",
    ")\n",
    "plt.fill_between(\n",
    "    np.linspace(0, 20, Nt + Nt_fore),\n",
    "    pred_mu[i_batch, :, 1] - 1.96 * pred_sigma[i_batch, :, 1, 1],\n",
    "    pred_mu[i_batch, :, 1] + 1.96 * pred_sigma[i_batch, :, 1, 1],\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "# plt.ylim(-1.1, 1.1)\n",
    "\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ed1b0-3ac4-4057-b06a-afddd0994f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-eo_torch_mgp]",
   "language": "python",
   "name": "conda-env-miniconda3-eo_torch_mgp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
