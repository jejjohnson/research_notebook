
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Variational Inference &#8212; Research Notebook</title>
    
  <link rel="stylesheet" href="../../../../_static/css/index.d431a4ee1c1efae0e38bdfebc22debff.css">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../../../_static/sphinx-book-theme.bfb7730f9caf2ec0b46a44615585038c.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.30270b6e4c972e43c488.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/language_data.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Sleeper Concepts" href="../../concepts/overview.html" />
    <link rel="prev" title="Solving Hard Integral Problems" href="inference.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../../index.html">
  
  <img src="../../../../_static/book.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Research Notebook</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../intro.html">
   Overview
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../resources/python/overview.html">
   Python
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../tutorials/jax_journey/overview.html">
   My JAX Journey
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../tutorials/remote/overview.html">
   Remote Computing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="reference internal" href="../overview.html">
   Bayesian
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../intro.html">
     Bayesian: Language of Uncertainty
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gaussian.html">
     Gaussian Distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../gaussian.html#gaussian-distribution">
     Gaussian Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../regression.html">
     Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="inference.html">
     Solving Hard Integral Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="inference.html#inference">
     Inference
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Variational Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../concepts/overview.html">
   Sleeper Concepts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../kernels/overview.html">
   Kernel Methods
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../../_sources/content/notes/bayesian/inference/variational_inference.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivations">
   Motivations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elbo-derivation">
   ELBO - Derivation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comments-on-q-x">
     Comments on
     <span class="math notranslate nohighlight">
      \(q(x)\)
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pros-and-cons">
   Pros and Cons
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-variational-inference">
     Why Variational Inference?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-not-variational-inference">
     Why Not Variational Inference?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#resources">
   Resources
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#from-scratch">
     From Scratch
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Variational Inference
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="variational-inference">
<h1>Variational Inference<a class="headerlink" href="#variational-inference" title="Permalink to this headline">¶</a></h1>
<hr class="docutils" />
<div class="section" id="motivations">
<h2>Motivations<a class="headerlink" href="#motivations" title="Permalink to this headline">¶</a></h2>
<p>Variational inference is the most scalable inference method the machine learning community has (as of 2019).</p>
<p><strong>Tutorials</strong></p>
<ul class="simple">
<li><p>https://www.ritchievink.com/blog/2019/06/10/bayesian-inference-how-we-are-able-to-chase-the-posterior/</p></li>
<li><p>https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/</p></li>
<li></li>
<li><ul>
<li></li>
</ul>
</li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="elbo-derivation">
<h2>ELBO - Derivation<a class="headerlink" href="#elbo-derivation" title="Permalink to this headline">¶</a></h2>
<p>Let’s start with the marginal likelihood function.</p>
<div class="math notranslate nohighlight">
\[\mathcal{P}(y| \theta)=\int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot \mathcal{P}(\mathbf x) \cdot d\mathbf{x}\]</div>
<p>where we have effectively marginalized out the <span class="math notranslate nohighlight">\(f\)</span>’s. We already know that it’s difficult to propagate the <span class="math notranslate nohighlight">\(\mathbf x\)</span>’s through the nonlinear functions <span class="math notranslate nohighlight">\(\mathbf K^{-1}\)</span> and <span class="math notranslate nohighlight">\(|\)</span>det <span class="math notranslate nohighlight">\(\mathbf K|\)</span> (see previous doc for examples). So using the VI strategy, we introduce a new variational distribution <span class="math notranslate nohighlight">\(q(\mathbf x)\)</span> to approximate the posterior distribution <span class="math notranslate nohighlight">\(\mathcal{P}(\mathbf x| y)\)</span>. The distribution is normally chosen to be Gaussian:</p>
<div class="math notranslate nohighlight">
\[q(\mathbf x) = \prod_{i=1}^{N}\mathcal{N}(\mathbf x|\mathbf \mu_z, \mathbf \Sigma_z)\]</div>
<p>So at this point, we aree interested in trying to find a way to measure the difference between the approximate distribution <span class="math notranslate nohighlight">\(q(\mathbf x)\)</span> and the true posterior distribution <span class="math notranslate nohighlight">\(\mathcal{P} (\mathbf x)\)</span>. Using some algebra, let’s take the log of the marginal likelihood (evidence):</p>
<div class="math notranslate nohighlight">
\[\log \mathcal{P}(y|\theta) = \log \int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot \mathcal{P}(\mathbf x) \cdot d\mathbf x\]</div>
<p>So now we are going to use the some tricks that you see within almost every derivation of the VI framework. The first one consists of using the Identity trick. This allows us to change the expectation to incorporate the new variational distribution <span class="math notranslate nohighlight">\(q(\mathbf x)\)</span>. We get the following equation:</p>
<div class="math notranslate nohighlight">
\[\log \mathcal{P}(y|\theta) = \log \int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot \mathcal{P}(\mathbf x) \cdot \frac{q(\mathbf x)}{q(\mathbf x)} \cdot d\mathbf x\]</div>
<p>Now that we have introduced our new variational distribution, we can regroup and reweight our expectation. Because I know what I want, I get the following:</p>
<div class="math notranslate nohighlight">
\[\log \mathcal{P}(y|\theta) = \log \int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot  q(\mathbf x) \cdot \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \cdot d\mathbf x\]</div>
<p>Now with Jensen’s inequality, we have the relationship <span class="math notranslate nohighlight">\(f(\mathbb{E}[x]) \leq \mathbb{E} [f(x)]\)</span>. We would like to put the <span class="math notranslate nohighlight">\(\log\)</span> function inside of the integral. Jensen’s inequality allows us to do this. If we let <span class="math notranslate nohighlight">\(f(\cdot)= \log(\cdot)\)</span> then we get the Jensen’s equality for a concave function, <span class="math notranslate nohighlight">\(f(\mathbb{E}[x]) \geq \mathbb{E} [f(x)]\)</span>. In this case if we match the terms to each component to the inequality, we have</p>
<div class="math notranslate nohighlight">
\[\log \cdot \mathbb{E}_\mathcal{q(\mathbf x)} \left[ \mathcal{P}(y|\mathbf x, \theta) \cdot  \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \right] 
\geq  
\mathbb{E}_\mathcal{q(\mathbf x)} \left[\log  \mathcal{P}(y|\mathbf x, \theta) \cdot \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \right]\]</div>
<p>So now finally we have both terms in the inequality. Summarizing everything we have the following relationship:</p>
<div class="math notranslate nohighlight">
\[log \mathcal{P}(y|\theta) = \log \int_\mathcal{X} \mathcal{P}(y|\mathbf x, \theta) \cdot  q(\mathbf x) \cdot \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \cdot d\mathbf x \]</div>
<div class="math notranslate nohighlight">
\[
\log \mathcal{P}(y|\theta) \geq  \int_\mathcal{X} \left[\log  \mathcal{P}(y|\mathbf x, \theta) \cdot \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \right] q(\mathbf x) \cdot d\mathbf x
\]</div>
<p>I’m going to switch up the terminology just to make it easier aesthetically. I’m going to let <span class="math notranslate nohighlight">\(\mathcal{L}(\theta)\)</span> be <span class="math notranslate nohighlight">\(\log \mathcal{P}(y|\theta)\)</span> and <span class="math notranslate nohighlight">\(\mathcal{F}(q, \theta) \leq \mathcal{L}(\theta)\)</span>. So basically:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta) =\log \mathcal{P}(y|\theta) \geq  \int_\mathcal{X} \left[\log  \mathcal{P}(y|\mathbf x, \theta) \cdot \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)} \right] q(\mathbf x) \cdot d\mathbf x = \mathcal{F}(q, \theta)
\]</div>
<p>With this simple change I can talk about each of the parts individually. Now using log rules we can break apart the likelihood and the quotient. The quotient will be needed for the KL divergence.</p>
<div class="math notranslate nohighlight">
\[
\mathcal{F}(q) =
\underbrace{\int_\mathcal{X} q(\mathbf x) \cdot \log  \mathcal{P}(y|\mathbf x, \theta) \cdot d\mathbf x}_{\mathbb{E}_{q(\mathbf{x})}} +
\underbrace{\int_\mathcal{X} q(\mathbf x) \log  \frac{\mathcal{P}(\mathbf x)}{q(\mathbf x)}   \cdot d\mathbf x}_{\text{KL}}
\]</div>
<p>The punchline of this (after many calculated manipulations), is that we obtain an optimization equation <span class="math notranslate nohighlight">\(\mathcal{F}(\theta)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{F}(q)=\mathbb{E}_{q(\mathbf x)}\left[ \log \mathcal{P}(y|\mathbf x, \theta) \right] - \text{D}_\text{KL}\left[ q(\mathbf x) || \mathcal{P}(\mathbf x) \right]\]</div>
<p>where:</p>
<ul class="simple">
<li><p>Approximate posterior distribution: <span class="math notranslate nohighlight">\(q(x)\)</span></p>
<ul>
<li><p>The best match to the true posterior <span class="math notranslate nohighlight">\(\mathcal{P}(y|\mathbf x, \theta)\)</span>. This is what we want to calculate.</p></li>
</ul>
</li>
<li><p>Reconstruction Cost: <span class="math notranslate nohighlight">\(\mathbb{E}_{q(\mathbf x)}\left[ \log \mathcal{P}(y|\mathbf x, \theta) \right]\)</span></p>
<ul>
<li><p>The expected log-likelihood measure of how well the samples from <span class="math notranslate nohighlight">\(q(x)\)</span> are able to explain the data <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ul>
</li>
<li><p>Penalty: <span class="math notranslate nohighlight">\(\text{D}_\text{KL}\left[ q(\mathbf x) || \mathcal{P}(\mathbf x) \right]\)</span></p>
<ul>
<li><p>Ensures that the explanation of the data <span class="math notranslate nohighlight">\(q(x)\)</span> doesn’t deviate too far from your beliefs <span class="math notranslate nohighlight">\(\mathcal{P}(x)\)</span>. (Okham’s razor constraint)</p></li>
</ul>
</li>
</ul>
<p><strong>Source</strong>: <a class="reference external" href="https://www.shakirm.com/papers/VITutorial.pdf">VI Tutorial</a> - Shakir Mohamed</p>
<p>If we optimize <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> with respect to <span class="math notranslate nohighlight">\(q(\mathbf x)\)</span>, the KL is minimized and we just get the likelihood. As we’ve seen before, the likelihood term is still problematic as it still has the nonlinear portion to propagate the <span class="math notranslate nohighlight">\(\mathbf x\)</span>’s through. So that’s nothing new and we’ve done nothing useful. If we introduce some special structure in <span class="math notranslate nohighlight">\(q(f)\)</span> by introducing sparsity, then we can achieve something useful with this formulation.
But through augmentation of the variable space with <span class="math notranslate nohighlight">\(\mathbf u\)</span> and <span class="math notranslate nohighlight">\(\mathbf Z\)</span> we can bypass this problem. The second term is simple to calculate because they’re both chosen to be Gaussian.</p>
<div class="section" id="comments-on-q-x">
<h3>Comments on <span class="math notranslate nohighlight">\(q(x)\)</span><a class="headerlink" href="#comments-on-q-x" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>We have now transformed our problem from an integration problem to an optimization problem where we optimize for <span class="math notranslate nohighlight">\(q(x)\)</span> directly.</p></li>
<li><p>Many people tend to simplify <span class="math notranslate nohighlight">\(q\)</span> but we could easily write some dependencies on the data for example <span class="math notranslate nohighlight">\(q(x|\mathcal{D})\)</span>.</p></li>
<li><p>We can easily see the convergence as we just have to wait until the loss (free energy) reaches convergence.</p></li>
<li><p>Typically <span class="math notranslate nohighlight">\(q(x)\)</span> is a Gaussian whereby the variational parameters are the mean and the variance. Practically speaking, we could freeze or unfreeze any of these parameters if we have some prior knowledge about our problem.</p></li>
<li><p>Many people say ‘tighten the bound’ but they really just mean optimization: modifying the hyperparameters so that we get as close as possible to the true marginal likelihood.</p></li>
</ul>
</div>
</div>
<div class="section" id="pros-and-cons">
<h2>Pros and Cons<a class="headerlink" href="#pros-and-cons" title="Permalink to this headline">¶</a></h2>
<div class="section" id="why-variational-inference">
<h3>Why Variational Inference?<a class="headerlink" href="#why-variational-inference" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Applicable to all probabilistic models</p></li>
<li><p>Transforms a problem from integration to one of optimization</p></li>
<li><p>Convergence assessment</p></li>
<li><p>Principled and Scalable approach to model selection</p></li>
<li><p>Compact representation of posterior distribution</p></li>
<li><p>Faster to converge</p></li>
<li><p>Numerically stable</p></li>
<li><p>Modern Computing Architectures (GPUs)</p></li>
</ul>
</div>
<div class="section" id="why-not-variational-inference">
<h3>Why Not Variational Inference?<a class="headerlink" href="#why-not-variational-inference" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Approximate posterior only</p></li>
<li><p>Difficulty in optimization due to local minima</p></li>
<li><p>Under-estimates the variance of posterior</p></li>
<li><p>Limited theory and guarantees for variational mehtods</p></li>
</ul>
</div>
</div>
<hr class="docutils" />
<div class="section" id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Tutorial Series - <a class="reference external" href="https://chrisorm.github.io/VI-Why.html">Why?</a> | <a class="reference external" href="https://chrisorm.github.io/VI-ELBO.html">ELBO</a> | <a class="reference external" href="https://chrisorm.github.io/VI-MC.html">MC ELBO</a> | <a class="reference external" href="https://chrisorm.github.io/VI-reparam.html">Reparameterization</a> | <a class="reference external" href="https://chrisorm.github.io/VI-ELBO-MC-approx.html">MC ELBO unBias</a> | <a class="reference external" href="https://chrisorm.github.io/VI-MC-PYT.html">MC ELBO PyTorch</a> | <a class="reference external" href="https://chrisorm.github.io/pydata-2018.html">Talk</a></p></li>
<li><p>Blog Posts: Neural Variational Inference</p>
<ul>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2016-07-01-neural-variational-inference-classical-theory.html">Classical Theory</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2016-07-04-neural-variational-inference-stochastic-variational-inference.html">Scaling Up</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2016-07-05-neural-variational-inference-blackbox.html">BlackBox Mode</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2016-07-11-neural-variational-inference-variational-autoencoders-and-Helmholtz-machines.html">VAEs and Helmholtz Machines</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2016-07-14-neural-variational-importance-weighted-autoencoders.html">Importance Weighted AEs</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html">Neural Samplers and Hierarchical VI</a></p></li>
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2019-05-10-importance-weighted-hierarchical-variational-inference.html">Importance Weighted Hierarchical VI</a> | <a class="reference external" href="https://www.youtube.com/watch?v=pdSu7XfGhHw&amp;feature=youtu.be">Video</a></p></li>
</ul>
</li>
<li><p>Normal Approximation to the Posterior Distribution - <a class="reference external" href="http://bjlkeng.github.io/posts/normal-approximations-to-the-posterior-distribution/">blog</a></p></li>
</ul>
<p><strong>Lower Bound</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf">Understaing the Variational Lower Bound</a></p></li>
<li><p><a class="reference external" href="http://paulrubenstein.co.uk/deriving-the-variational-lower-bound/">Deriving the Variational Lower Bound</a></p></li>
</ul>
<p><strong>Summaries</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1711.05597.pdf">Advances in Variational Inference</a></p></li>
</ul>
<p><strong>Presentations</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.shakirm.com/papers/VITutorial.pdf">VI Shakir</a></p></li>
<li><p>Deisenroth - <a class="reference external" href="https://drive.google.com/file/d/1sAIF0rqgNbVbp7ZbuiS7kh96Yns04k1i/view">VI</a> | <a class="reference external" href="https://drive.google.com/open?id=14WOcbwn011rJbFFsSbeoeuSxY4sMG4KY">IT</a></p></li>
<li><p><a class="reference external" href="https://www.doc.ic.ac.uk/~mpd37/teaching/ml_tutorials/2017-11-22-Ek-BNP-and-priors-over-functions.pdf">Bayesian Non-Parametrics and Priors over functions</a></p></li>
<li><p><a class="reference external" href="https://filebox.ece.vt.edu/~s14ece6504/slides/Moran_I_ECE_6504_VB.pdf">here</a></p></li>
</ul>
<p><strong>Reviews</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="http://krasserm.github.io/2018/04/03/variational-inference/">From EM to SVI</a></p></li>
<li><p><a class="reference external" href="https://ermongroup.github.io/cs228-notes/inference/variational/">Variational Inference</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1601.00670.pdf">VI- Review for Statisticians</a></p></li>
<li><p><a class="reference external" href="http://www.robots.ox.ac.uk/~sjrob/Pubs/vbTutorialFinal.pdf">Tutorial on VI</a></p></li>
<li><p><a class="reference external" href="https://zhiyzuo.github.io/VI/">VI w/ Code</a></p></li>
<li><p><a class="reference external" href="https://blog.evjang.com/2016/08/variational-bayes.html">VI - Mean Field</a></p></li>
<li><p><a class="reference external" href="https://github.com/philschulz/VITutorial">VI Tutorial</a></p></li>
<li><p>GMM</p>
<ul>
<li><p><a class="reference external" href="https://github.com/bertini36/GMM">VI in GMM</a></p></li>
<li><p><a class="reference external" href="https://mattdickenson.com/2018/11/18/gmm-python-pyro/">GMM Pyro</a> | <a class="reference external" href="http://pyro.ai/examples/gmm.html">Pyro</a></p></li>
<li><p><a class="reference external" href="https://github.com/ldeecke/gmm-torch">GMM PyTorch</a> | <a class="reference external" href="https://angusturner.github.io/generative_models/2017/11/03/pytorch-gaussian-mixture-model.html">PyTorch</a> | <a class="reference external" href="https://github.com/RomainSabathe/dagmm/blob/master/gmm.py">PyTorchy</a></p></li>
</ul>
</li>
</ul>
<p><strong>Code</strong></p>
<p><strong>Extensions</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="http://artem.sobolev.name/posts/2019-04-26-neural-samplers-and-hierarchical-variational-inference.html">Neural Samplers and Hierarchical Variational Inference</a></p></li>
</ul>
<div class="section" id="from-scratch">
<h3>From Scratch<a class="headerlink" href="#from-scratch" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Programming a Neural Network from Scratch - Ritchie Vink (2017) - <a class="reference external" href="https://www.ritchievink.com/blog/2017/07/10/programming-a-neural-network-from-scratch/">blog</a></p></li>
<li><p>An Introduction to Probability and Computational Bayesian Statistcs - Eric Ma 0<a class="reference external" href="https://ericmjl.github.io/essays-on-data-science/machine-learning/computational-bayesian-stats/">Blog</a></p></li>
<li><p>Variational Inference from Scratch - Ritchie Vink (2019) - <a class="reference external" href="https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/">blog</a></p></li>
<li><p>Bayesian inference; How we are able to chase the Posterior - Ritchie Vink (2019) - [blog](Bayesian inference; How we are able to chase the Posterior)</p></li>
<li><p>Algorithm Breakdown: Expectation Maximization - <a class="reference external" href="https://www.ritchievink.com/blog/2019/05/24/algorithm-breakdown-expectation-maximization/">blog</a></p></li>
</ul>
</div>
</div>
<div class="section" id="id1">
<h2>Variational Inference<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Variational Bayes and The Mean-Field Approximation - Keng (2017) - <a class="reference external" href="http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/">blog</a></p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/notes/bayesian/inference"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="inference.html" title="previous page">Solving Hard Integral Problems</a>
    <a class='right-next' id="next-link" href="../../concepts/overview.html" title="next page">Sleeper Concepts</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By J. Emmanuel Johnson<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../../../_static/js/index.30270b6e4c972e43c488.js"></script>


    
  </body>
</html>